diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/.DS_Store b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/.DS_Store
deleted file mode 100644
index 5008ddf..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/.DS_Store and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/.git b/packages/official-plugins/vibevoice/src/vibevoice/.git
new file mode 100644
index 0000000..3220a9a
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/.git
@@ -0,0 +1 @@
+gitdir: ../../../../../.git/modules/packages/official-plugins/vibevoice/src/vibevoice
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/.gitignore b/packages/official-plugins/vibevoice/src/vibevoice/.gitignore
new file mode 100644
index 0000000..58d59e9
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/.gitignore
@@ -0,0 +1,178 @@
+# Initially taken from Github's Python gitignore file
+
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# tests and logs
+tests/fixtures/cached_*_text.txt
+logs/
+lightning_logs/
+lang_code_data/
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.nox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+.hypothesis/
+.pytest_cache/
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+local_settings.py
+db.sqlite3
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# IPython
+profile_default/
+ipython_config.py
+
+# pyenv
+.python-version
+
+# celery beat schedule file
+celerybeat-schedule
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+.env
+.venv
+env/
+venv/
+ENV/
+env.bak/
+venv.bak/
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# Pyre type checker
+.pyre/
+
+# vscode
+.vs
+.vscode
+
+# Pycharm
+.idea
+
+# TF code
+tensorflow_code
+
+# Models
+proc_data
+
+# examples
+runs
+/runs_old
+/wandb
+/examples/runs
+/examples/**/*.args
+/examples/rag/sweep
+
+# data
+/data
+serialization_dir
+
+# emacs
+*.*~
+debug.env
+
+# vim
+.*.swp
+
+#ctags
+tags
+
+# pre-commit
+.pre-commit*
+
+# .lock
+*.lock
+
+# DS_Store (MacOS)
+.DS_Store
+
+# ruff
+.ruff_cache
+
+# our proj
+/output/
+/outputs/
+/checkpoint/
+/checkpoints/
+exp
+.gradio/
+MSFT-VV
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/EXAMPLES.md b/packages/official-plugins/vibevoice/src/vibevoice/EXAMPLES.md
new file mode 100644
index 0000000..7cbe373
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/EXAMPLES.md
@@ -0,0 +1,46 @@
+## Examples
+
+**Video Demo**
+
+We produced this video with [Wan2.2](https://github.com/Wan-Video/Wan2.2). We sincerely appreciate the Wan-Video team for their great work.
+
+**English**
+<div align="center">
+
+https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784
+
+</div>
+
+
+**Chinese**
+<div align="center">
+
+https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f
+
+</div>
+
+**Cross-Lingual**
+<div align="center">
+
+https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722
+
+</div>
+
+**Spontaneous Singing**
+<div align="center">
+
+https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730
+
+</div>
+
+
+**Long Conversation with 4 people**
+<div align="center">
+
+https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727
+
+</div>
+
+For more examples, see the [Project Page](https://microsoft.github.io/VibeVoice).
+
+Try it on [Colab](https://colab.research.google.com/github/vibevoice-community/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb).
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/FINETUNING.md b/packages/official-plugins/vibevoice/src/vibevoice/FINETUNING.md
new file mode 100644
index 0000000..15ab460
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/FINETUNING.md
@@ -0,0 +1,55 @@
+# Finetuning
+
+More instructions coming soon.
+
+VibeVoice finetuning works wonders - both for adaptipng VibeVoice to new languages and for better voice cloning of a single voice.
+
+Join the [Discord](https://discord.gg/ZDEYTTRxWG) for support. Also take a look at [voicepowered-ai/VibeVoice-finetuning](https://github.com/voicepowered-ai/VibeVoice-finetuning).
+
+## Notes
+
+* Members of the community have observed that for fine-tuning VibeVoice on a SINGLE voice, it is often beneficial to set `voice_prompt_drop_rate` to `1.0` and avoid use of voice cloning/reference audio all together during inference. This can lead to more natural speech generation. **If you are training on a single speaker I highly recommend you try this, just a note that if you do this voice cloning will not be supported on your finetuned model**
+
+## Example Script
+
+Example script:
+
+```bash
+python -m vibevoice.finetune.train_vibevoice \
+    --model_name_or_path vibevoice/VibeVoice-1.5B \
+    --dataset_name vibevoice/jenny_vibevoice_formatted \
+    --text_column_name text \
+    --audio_column_name audio \
+    --voice_prompts_column_name audio \
+    --output_dir finetune_vibevoice_zac \
+    --per_device_train_batch_size 8 \
+    --gradient_accumulation_steps 16 \
+    --learning_rate 2.5e-5 \
+    --num_train_epochs 1 \
+    --logging_steps 10 \
+    --save_steps 100 \
+    --eval_steps 100 \
+    --report_to wandb \
+    --remove_unused_columns False \
+    --bf16 True \
+    --do_train \
+    --gradient_clipping \
+    --gradient_checkpointing False \
+    --ddpm_batch_mul 4 \
+    --diffusion_loss_weight 1.4 \
+    --train_diffusion_head True \
+    --ce_loss_weight 0.04 \
+    --voice_prompt_drop_rate 0.2 \
+    --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj \
+    --lr_scheduler_type cosine \
+    --warmup_ratio 0.03 \
+    --max_grad_norm 0.8
+```
+
+## Notes
+
+- Currently, only single-speaker finetuning is supported. Podcast fine-tuning is not supported yet.
+- This is an unofficial finetuning implementation, it has not been validated by the original authors.
+- The `voice_prompts_column_name` parameter is currently set to `audio` in the example above, which means the same audio file is used for both training data and voice prompts. This is appropriate when you don't have separate voice prompt files. However, if your dataset includes dedicated voice prompt files (short audio clips that capture the target speaker's voice characteristics), you should specify a different column name that contains these separate voice prompt files. For podcast-style training (once it is supported), the model typically uses the first utterance from each speaker within the podcast episode as the voice prompt, meaning the voice prompt is extracted from the beginning of the same audio file used for training.
+- The dataset text/transcript must be in the format of "Speaker X: text", even if there is only one speaker. Example: `Speaker 1: Hello, how are you?`
+- The default dataset is the Jenny (Dioco) dataset. This is a small dataset for testing purposes and each segment is only a few seconds long. The model may struggle to generate long audio with this dataset.
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/Figures/MOS-preference.png b/packages/official-plugins/vibevoice/src/vibevoice/Figures/MOS-preference.png
new file mode 100644
index 0000000..3e1d21b
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/Figures/MOS-preference.png differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/Figures/VibeVoice.jpg b/packages/official-plugins/vibevoice/src/vibevoice/Figures/VibeVoice.jpg
new file mode 100644
index 0000000..4a99d78
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/Figures/VibeVoice.jpg differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/Figures/VibeVoice_logo.png b/packages/official-plugins/vibevoice/src/vibevoice/Figures/VibeVoice_logo.png
new file mode 100644
index 0000000..2619848
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/Figures/VibeVoice_logo.png differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/Figures/VibeVoice_logo_white.png b/packages/official-plugins/vibevoice/src/vibevoice/Figures/VibeVoice_logo_white.png
new file mode 100644
index 0000000..ba60c92
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/Figures/VibeVoice_logo_white.png differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/LICENSE b/packages/official-plugins/vibevoice/src/vibevoice/LICENSE
new file mode 100644
index 0000000..269a897
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/LICENSE
@@ -0,0 +1,21 @@
+MIT License
+
+Copyright (c) 2025 Microsoft
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/README.md b/packages/official-plugins/vibevoice/src/vibevoice/README.md
new file mode 100644
index 0000000..874cb23
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/README.md
@@ -0,0 +1,176 @@
+> [!IMPORTANT]
+> This is a community-maintained fork of VibeVoice. Following the removal of the official VibeVoice repository, this fork serves to preserve the codebase and maintain accessibility for the community while also introducing additional functionality (such as unofficial training/fine-tuning implementations)
+
+## üéôÔ∏è VibeVoice: A Frontier Long Conversational Text-to-Speech Model
+
+[![Project Page](https://img.shields.io/badge/Project-Page-blue)](https://microsoft.github.io/VibeVoice)
+[![Hugging Face](https://img.shields.io/badge/Hugging_Face-Models-orange?logo=huggingface)](https://huggingface.co/vibevoice)
+[![Technical Report](https://img.shields.io/badge/Technical-Report-red)](https://arxiv.org/pdf/2508.19205)
+[![Colab](https://img.shields.io/badge/Colab-Demo-orange?logo=googlecolab)](https://colab.research.google.com/github/vibevoice-community/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb)
+
+## Community
+
+**Join the unofficial Discord community: https://discord.gg/ZDEYTTRxWG** - share samples, ask questions, discuss fine-tuning, etc.
+
+## Overview
+
+VibeVoice is a novel framework designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.
+
+A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a [next-token diffusion](https://arxiv.org/abs/2412.08635) framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.
+
+The model can synthesize speech up to **90 minutes** long with up to **4 distinct speakers**, surpassing the typical 1-2 speaker limits of many prior models.
+
+Fine-tuning is now supported, which is incredibly powerful. You can adapt VibeVoice to a new language or a new voice - [try it out](https://github.com/vibevoice-community/VibeVoice/blob/main/FINETUNING.md)
+
+## [Examples](./EXAMPLES.md)
+
+## Evaluation
+
+<p align="left">
+  <img src="Figures/MOS-preference.png" alt="MOS Preference Results" height="260px">
+  <img src="Figures/VibeVoice.jpg" alt="VibeVoice Overview" height="250px" style="margin-right: 10px;">
+</p>
+
+
+## Updates
+
+- **[2025-12-04]** Added support for VibeVoice-Streaming-0.5B model for real-time TTS!
+- **[2025-09-05]** Microsoft repo restored (without code) with statement about responsible AI use.
+- **[2025-09-04]** Community backup created after Microsoft removed original repo and models.
+- **[2025-08-26]** The [VibeVoice-7B](https://huggingface.co/vibevoice/VibeVoice-7B) model weights are open-sourced!
+- **[2025-08-28]** [Colab Notebook](https://colab.research.google.com/github/microsoft-community/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb) available. Only VibeVoice-1.5B is supported due to GPU memory limitations.
+
+## Roadmap
+
+- [x] Unofficial/community training code
+- [ ] HF Transformers integration ([PR](https://github.com/huggingface/transformers/pull/40546))
+- [ ] VibePod: End-to-end solution that creates podcasts from documents, webpages, or even a simple topic.
+
+## Model Zoo
+
+| Model | Context Length | Generation Length | Speakers | Weight |
+|-------|----------------|-------------------|----------|--------|
+| VibeVoice-Streaming-0.5B | 8K | Real-time | 1 | [HF link](https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B) |
+| VibeVoice-1.5B | 64K | ~90 min | Up to 4 | [HF link](https://huggingface.co/vibevoice/VibeVoice-1.5B) |
+| VibeVoice-Large (7B) | 32K | ~45 min | Up to 4 | [HF link](https://huggingface.co/vibevoice/VibeVoice-7B) |
+
+### Model Comparison
+
+- **VibeVoice-Streaming-0.5B**: Optimized for **real-time** low-latency TTS. Single speaker only. Uses pre-computed voice embeddings (.pt files) for fast inference. Best for live applications.
+- **VibeVoice-1.5B/7B**: Full-featured models for **long-form multi-speaker** content like podcasts. Support up to 4 speakers with voice cloning from audio samples.
+
+## Installation
+
+```bash
+git clone https://github.com/vibevoice-community/VibeVoice.git
+cd VibeVoice/
+
+uv pip install -e .
+```
+
+## Usage
+
+### üö® Tips
+
+We observed users may encounter occasional instability when synthesizing Chinese speech. We recommend:
+
+- Using English punctuation even for Chinese text, preferably only commas and periods.
+- Using the Large model variant, which is considerably more stable.
+- If you found the generated voice speak too fast. Please try to chunk your text with multiple speaker turns with same speaker label.
+
+We'd like to thank [PsiPi](https://huggingface.co/PsiPi) for sharing an interesting way for emotion control. Details can be found via [discussion #12](https://huggingface.co/microsoft/VibeVoice-1.5B/discussions/12).
+
+**Option 1: Launch Gradio demo**
+
+```bash
+python demo/gradio_demo.py --model_path vibevoice/VibeVoice-1.5B --share
+# or python demo/gradio_demo.py --model_path vibevoice/VibeVoice-7B --share
+# optionally add --checkpoint_path path/to/checkpoint to load a fine-tuned adapter
+# use the in-app "Disable voice cloning" setting (Advanced Settings) to skip speaker conditioning
+```
+
+**Option 2: Inference from files directly (Multi-Speaker 1.5B/7B)**
+
+```bash
+# We provide some LLM generated example scripts under demo/text_examples/ for demo
+# 1 speaker
+python demo/inference_from_file.py --model_path vibevoice/VibeVoice-7B --txt_path demo/text_examples/1p_abs.txt --speaker_names Alice
+
+# or more speakers
+python demo/inference_from_file.py --model_path vibevoice/VibeVoice-7B --txt_path demo/text_examples/2p_music.txt --speaker_names Alice Frank
+
+# load a fine-tuned LoRA checkpoint
+python demo/inference_from_file.py --model_path vibevoice/VibeVoice-7B --txt_path demo/text_examples/1p_abs.txt --speaker_names Alice --checkpoint_path path/to/checkpoint
+
+# disable voice cloning (skip speech prefill)
+python demo/inference_from_file.py --model_path vibevoice/VibeVoice-7B --txt_path demo/text_examples/1p_abs.txt --speaker_names Alice --disable_prefill
+```
+
+**Option 3: Streaming Model (0.5B) - Real-time TTS**
+
+The streaming model uses pre-computed voice embeddings for low-latency generation:
+
+```bash
+# Basic usage with streaming model
+python demo/streaming_inference_from_file.py \
+    --model_path microsoft/VibeVoice-Realtime-0.5B \
+    --txt_path demo/text_examples/1p_vibevoice.txt \
+    --speaker_name Emma
+
+# Available voice presets: Carter, Davis, Emma, Frank, Grace, Mike (English), Samuel (Indian English)
+# Adjust CFG scale and DDPM steps for quality/speed tradeoff
+python demo/streaming_inference_from_file.py \
+    --model_path microsoft/VibeVoice-Realtime-0.5B \
+    --txt_path demo/text_examples/1p_vibevoice.txt \
+    --speaker_name Mike \
+    --cfg_scale 1.5 \
+    --ddpm_steps 5
+```
+
+Voice presets are stored as `.pt` files in `demo/voices/streaming_model/`. These contain pre-computed KV cache embeddings for fast inference. Voice cloning is not supported for now.
+
+NOTE: If you get the warning `Some weights of VibeVoiceStreamingForConditionalGenerationInference were not initialized from the model checkpoint` when loading, this is expected. This is because voice cloning capabilities have been removed from the model.
+
+## [Finetuning](./FINETUNING.md)
+
+NOTE: Finetuning is still **very experimental** and not well tested yet!
+
+## FAQ
+
+#### Q1: Is this a pretrained model?
+**A:** Yes, it's a pretrained model without any post-training or benchmark-specific optimizations. In a way, this makes VibeVoice very versatile and fun to use.
+
+#### Q2: Randomly trigger Sounds / Music / BGM.
+**A:** As you can see from our demo page, the background music or sounds are spontaneous. This means we can't directly control whether they are generated or not. The model is content-aware, and these sounds are triggered based on the input text and the chosen voice prompt.
+
+Here are a few things we've noticed:
+*   If the voice prompt you use contains background music, the generated speech is more likely to have it as well. (The Large model is quite stable and effective at this‚Äîgive it a try on the demo!)
+*   If the voice prompt is clean (no BGM), but the input text includes introductory words or phrases like "Welcome to," "Hello," or "However," background music might still appear.
+*   Speaker voice related, using "Alice" results in random BGM than others (fixed).
+*   In other scenarios, the Large model is more stable and has a lower probability of generating unexpected background music.
+
+In fact, we intentionally decided not to denoise our training data because we think it's an interesting feature for BGM to show up at just the right moment. You can think of it as a little easter egg we left for you.
+
+#### Q3: Text normalization?
+**A:** We don't perform any text normalization during training or inference. Our philosophy is that a large language model should be able to handle complex user inputs on its own. However, due to the nature of the training data, you might still run into some corner cases.
+
+#### Q4: Singing Capability.
+**A:** Our training data **doesn't contain any music data**. The ability to sing is an emergent capability of the model (which is why it might sound off-key, even on a famous song like 'See You Again'). (The Large model is more likely to exhibit this than the 1.5B).
+
+#### Q5: Some Chinese pronunciation errors.
+**A:** The volume of Chinese data in our training set is significantly smaller than the English data. Additionally, certain special characters (e.g., Chinese quotation marks) may occasionally cause pronunciation issues.
+
+#### Q6: Instability of cross-lingual transfer.
+**A:** The model does exhibit strong cross-lingual transfer capabilities, including the preservation of accents, but its performance can be unstable. This is an emergent ability of the model that we have not specifically optimized. It's possible that a satisfactory result can be achieved through repeated sampling.
+
+## Credits
+
+- Thanks to [Microsoft](https://github.com/microsoft/VibeVoice) for the original VibeVoice implementation.
+- Huge shoutout to [Juan Pablo Gallego](https://github.com/jpgallegoar) from [VoicePowered AI](https://www.voicepowered.ai/) for the unofficial training/fine-tuning code.
+- Thanks to [PsiPi](https://huggingface.co/PsiPi) for sharing an interesting way for emotion control. Details can be found via [discussion #12](https://huggingface.co/microsoft/VibeVoice-1.5B/discussions/12).
+
+## License
+
+The source code and models are licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.
+
+Note: Microsoft has removed the original repo and models. This fork is based off of the MIT-licensed code from Microsoft.
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/__pycache__/__init__.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index f606ff5..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/__pycache__/__init__.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/__pycache__/__init__.cpython-311.pyc
deleted file mode 100644
index 8f99356..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/__pycache__/__init__.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/VibeVoice_colab.ipynb b/packages/official-plugins/vibevoice/src/vibevoice/demo/VibeVoice_colab.ipynb
new file mode 100644
index 0000000..f2c639b
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/VibeVoice_colab.ipynb
@@ -0,0 +1,190 @@
+{
+  "cells": [
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "view-in-github"
+      },
+      "source": [
+        "<a href=\"https://colab.research.google.com/github/vibevoice-community/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "id": "WvIaUJD2y0yU",
+      "metadata": {
+        "id": "WvIaUJD2y0yU"
+      },
+      "source": [
+        "# VibeVoice Colab ‚Äî T4 Quickstart (1.5B)\n",
+        "\n",
+        "This notebook provides a quickstart guide to run VibeVoice on Colab with T4. The T4 GPU can only support the 1.5B model due to memory limitations. Please note that T4 can only use SDPA instead of flash_attention_2, which may result in unstable and lower audio quality. For the best TTS experience, we recommend trying the 7B model on a more powerful GPU.\n"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "id": "e8fTKYGx7DZk",
+      "metadata": {
+        "id": "e8fTKYGx7DZk"
+      },
+      "source": [
+        "## Step 1: Setup Environment"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": null,
+      "id": "4wxJ6QHM-ZOb",
+      "metadata": {
+        "id": "4wxJ6QHM-ZOb"
+      },
+      "outputs": [],
+      "source": [
+        "# Check for T4 GPU\n",
+        "import torch\n",
+        "if torch.cuda.is_available() and \"T4\" in torch.cuda.get_device_name(0):\n",
+        "    print(\"‚úÖ T4 GPU detected\")\n",
+        "else:\n",
+        "    print(\"\"\"\n",
+        "    ‚ö†Ô∏è WARNING: T4 GPU not detected\n",
+        "\n",
+        "    The recommended runtime for this Colab notebook is \"T4 GPU\".\n",
+        "\n",
+        "    To change the runtime type:\n",
+        "\n",
+        "        1. Click on \"Runtime\" in the top navigation menu\n",
+        "        2. Click on \"Change runtime type\"\n",
+        "        3. Select \"T4 GPU\"\n",
+        "        4. Click \"OK\" if a \"Disconnect and delete runtime\" window appears\n",
+        "        5. Click on \"Save\"\n",
+        "\n",
+        "    \"\"\")\n",
+        "\n",
+        "# Clone the VibeVoice repository\n",
+        "![ -d /content/VibeVoice ] || git clone --quiet --branch main --depth 1 https://github.com/microsoft/VibeVoice.git /content/VibeVoice\n",
+        "print(\"‚úÖ Cloned VibeVoice repository\")\n",
+        "\n",
+        "# Install project dependencies\n",
+        "!uv pip --quiet install --system -e /content/VibeVoice\n",
+        "print(\"‚úÖ Installed dependencies\")\n",
+        "\n",
+        "# Download model (~3 minutes)\n",
+        "!HF_XET_HIGH_PERFORMANCE=1 hf download microsoft/VibeVoice-1.5B --quiet  --local-dir /content/models/VibeVoice-1.5B > /dev/null\n",
+        "print(\"‚úÖ Downloaded model: microsoft/VibeVoice-1.5B\")\n"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "id": "pgKlV7153Ifi",
+      "metadata": {
+        "id": "pgKlV7153Ifi"
+      },
+      "source": [
+        "## Step 2: Create Transcript"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": null,
+      "id": "Yc1N9EHswFxA",
+      "metadata": {
+        "id": "Yc1N9EHswFxA"
+      },
+      "outputs": [],
+      "source": [
+        "%%writefile /content/my_transcript.txt\n",
+        "Speaker 1: Can I try VibeVoice with my own example?\n",
+        "Speaker 2: Of course! VibeVoice is open-source, built to benefit everyone - you're welcome to try it out.\n"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "id": "MBCC6s-F6_hP",
+      "metadata": {
+        "id": "MBCC6s-F6_hP"
+      },
+      "source": [
+        "## Step 3: Generate Audio"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": null,
+      "id": "dYWsLJ-n0Npm",
+      "metadata": {
+        "id": "dYWsLJ-n0Npm"
+      },
+      "outputs": [],
+      "source": [
+        "# Run Python script to generate audio from transcript\n",
+        "!python /content/VibeVoice/demo/inference_from_file.py \\\n",
+        "    --model_path /content/models/VibeVoice-1.5B \\\n",
+        "    --txt_path /content/my_transcript.txt \\\n",
+        "    --speaker_names Alice Frank\n",
+        "\n",
+        "# Display audio controls\n",
+        "from IPython.display import Audio\n",
+        "Audio(\"/content/outputs/my_transcript_generated.wav\")\n"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "id": "ec6438d5",
+      "metadata": {},
+      "source": [
+        "# Step 4: Download Audio"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": null,
+      "id": "b40ffa22",
+      "metadata": {},
+      "outputs": [],
+      "source": [
+        "from google.colab import files\n",
+        "files.download(\"/content/outputs/my_transcript_generated.wav\") \n"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "id": "1bce752d",
+      "metadata": {},
+      "source": [
+        "\n",
+        "## Risks and Limitations\n",
+        "\n",
+        "While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content."
+      ]
+    }
+  ],
+  "metadata": {
+    "accelerator": "GPU",
+    "colab": {
+      "gpuType": "T4",
+      "include_colab_link": true,
+      "machine_shape": "hm",
+      "name": "VibeVoice_Colab.ipynb",
+      "provenance": []
+    },
+    "kernelspec": {
+      "display_name": "Python 3",
+      "name": "python3"
+    },
+    "language_info": {
+      "codemirror_mode": {
+        "name": "ipython",
+        "version": 3
+      },
+      "file_extension": ".py",
+      "mimetype": "text/x-python",
+      "name": "python",
+      "nbconvert_exporter": "python",
+      "pygments_lexer": "ipython3",
+      "version": "3.10.11"
+    }
+  },
+  "nbformat": 4,
+  "nbformat_minor": 5
+}
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/example/1p_EN2CH.mp4 b/packages/official-plugins/vibevoice/src/vibevoice/demo/example/1p_EN2CH.mp4
new file mode 100644
index 0000000..1c8e506
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/example/1p_EN2CH.mp4 differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/example/2p_see_u_again.mp4 b/packages/official-plugins/vibevoice/src/vibevoice/demo/example/2p_see_u_again.mp4
new file mode 100644
index 0000000..a2f858d
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/example/2p_see_u_again.mp4 differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/example/4p_climate_45min.mp4 b/packages/official-plugins/vibevoice/src/vibevoice/demo/example/4p_climate_45min.mp4
new file mode 100644
index 0000000..eb97300
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/example/4p_climate_45min.mp4 differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/gradio_demo.py b/packages/official-plugins/vibevoice/src/vibevoice/demo/gradio_demo.py
new file mode 100644
index 0000000..00161f8
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/gradio_demo.py
@@ -0,0 +1,1161 @@
+"""
+VibeVoice Gradio Demo - High-Quality Dialogue Generation Interface with Streaming Support
+"""
+
+import argparse
+import json
+import os
+import sys
+import tempfile
+import time
+from pathlib import Path
+from typing import List, Dict, Any, Iterator, Optional
+from datetime import datetime
+import threading
+import numpy as np
+import gradio as gr
+import librosa
+import soundfile as sf
+import torch
+import os
+import traceback
+import re
+
+from vibevoice.modular.configuration_vibevoice import VibeVoiceConfig
+from vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference
+from vibevoice.modular.lora_loading import load_lora_assets
+from vibevoice.processor.vibevoice_processor import VibeVoiceProcessor
+from vibevoice.modular.streamer import AudioStreamer
+from transformers.utils import logging
+from transformers import set_seed
+
+logging.set_verbosity_info()
+logger = logging.get_logger(__name__)
+
+
+class VibeVoiceDemo:
+    def __init__(self, model_path: str, device: str = "cuda", inference_steps: int = 5, adapter_path: Optional[str] = None):
+        """Initialize the VibeVoice demo with model loading."""
+        self.model_path = model_path
+        self.device = device
+        self.inference_steps = inference_steps
+        self.adapter_path = adapter_path
+        self.loaded_adapter_root: Optional[str] = None
+        self.is_generating = False  # Track generation state
+        self.stop_generation = False  # Flag to stop generation
+        self.current_streamer = None  # Track current audio streamer
+        self.load_model()
+        self.setup_voice_presets()
+        self.load_example_scripts()  # Load example scripts
+        
+    def load_model(self):
+        """Load the VibeVoice model and processor."""
+        print(f"Loading processor & model from {self.model_path}")
+        self.loaded_adapter_root = None
+        # Normalize potential 'mpx'
+        if self.device.lower() == "mpx":
+            print("Note: device 'mpx' detected, treating it as 'mps'.")
+            self.device = "mps"
+        if self.device == "mps" and not torch.backends.mps.is_available():
+            print("Warning: MPS not available. Falling back to CPU.")
+            self.device = "cpu"
+        print(f"Using device: {self.device}")
+        # Load processor
+        self.processor = VibeVoiceProcessor.from_pretrained(self.model_path)
+        # Decide dtype & attention
+        if self.device == "mps":
+            load_dtype = torch.float32
+            attn_impl_primary = "sdpa"
+        elif self.device == "cuda":
+            load_dtype = torch.bfloat16
+            attn_impl_primary = "flash_attention_2"
+        else:
+            load_dtype = torch.float32
+            attn_impl_primary = "sdpa"
+        print(f"Using device: {self.device}, torch_dtype: {load_dtype}, attn_implementation: {attn_impl_primary}")
+        # Load model
+        try:
+            if self.device == "mps":
+                self.model = VibeVoiceForConditionalGenerationInference.from_pretrained(
+                    self.model_path,
+                    torch_dtype=load_dtype,
+                    attn_implementation=attn_impl_primary,
+                    device_map=None,
+                )
+                self.model.to("mps")
+            elif self.device == "cuda":
+                self.model = VibeVoiceForConditionalGenerationInference.from_pretrained(
+                    self.model_path,
+                    torch_dtype=load_dtype,
+                    device_map="cuda",
+                    attn_implementation=attn_impl_primary,
+                )
+            else:
+                self.model = VibeVoiceForConditionalGenerationInference.from_pretrained(
+                    self.model_path,
+                    torch_dtype=load_dtype,
+                    device_map="cpu",
+                    attn_implementation=attn_impl_primary,
+                )
+        except Exception as e:
+            if attn_impl_primary == 'flash_attention_2':
+                print(f"[ERROR] : {type(e).__name__}: {e}")
+                print(traceback.format_exc())
+                fallback_attn = "sdpa"
+                print(f"Falling back to attention implementation: {fallback_attn}")
+                self.model = VibeVoiceForConditionalGenerationInference.from_pretrained(
+                    self.model_path,
+                    torch_dtype=load_dtype,
+                    device_map=(self.device if self.device in ("cuda", "cpu") else None),
+                    attn_implementation=fallback_attn,
+                )
+                if self.device == "mps":
+                    self.model.to("mps")
+            else:
+                raise e
+        if self.adapter_path:
+            print(f"Loading fine-tuned assets from {self.adapter_path}")
+            report = load_lora_assets(self.model, self.adapter_path)
+            loaded_components = [
+                name for name, loaded in (
+                    ("language LoRA", report.language_model),
+                    ("diffusion head LoRA", report.diffusion_head_lora),
+                    ("diffusion head weights", report.diffusion_head_full),
+                    ("acoustic connector", report.acoustic_connector),
+                    ("semantic connector", report.semantic_connector),
+                )
+                if loaded
+            ]
+            if loaded_components:
+                print(f"Loaded components: {', '.join(loaded_components)}")
+            else:
+                print("Warning: no adapter components were loaded; check the checkpoint path.")
+            if report.adapter_root is not None:
+                self.loaded_adapter_root = str(report.adapter_root)
+                print(f"Adapter assets resolved to: {self.loaded_adapter_root}")
+            else:
+                self.loaded_adapter_root = self.adapter_path
+
+        self.model.eval()
+        
+        # Use SDE solver by default
+        self.model.model.noise_scheduler = self.model.model.noise_scheduler.from_config(
+            self.model.model.noise_scheduler.config, 
+            algorithm_type='sde-dpmsolver++',
+            beta_schedule='squaredcos_cap_v2'
+        )
+        self.model.set_ddpm_inference_steps(num_steps=self.inference_steps)
+        
+        if hasattr(self.model.model, 'language_model'):
+            print(f"Language model attention: {self.model.model.language_model.config._attn_implementation}")
+    
+    def setup_voice_presets(self):
+        """Setup voice presets by scanning the voices directory."""
+        voices_dir = os.path.join(os.path.dirname(__file__), "voices")
+        
+        # Check if voices directory exists
+        if not os.path.exists(voices_dir):
+            print(f"Warning: Voices directory not found at {voices_dir}")
+            self.voice_presets = {}
+            self.available_voices = {}
+            return
+        
+        # Scan for all WAV files in the voices directory
+        self.voice_presets = {}
+        
+        # Get all .wav files in the voices directory
+        wav_files = [f for f in os.listdir(voices_dir) 
+                    if f.lower().endswith(('.wav', '.mp3', '.flac', '.ogg', '.m4a', '.aac')) and os.path.isfile(os.path.join(voices_dir, f))]
+        
+        # Create dictionary with filename (without extension) as key
+        for wav_file in wav_files:
+            # Remove .wav extension to get the name
+            name = os.path.splitext(wav_file)[0]
+            # Create full path
+            full_path = os.path.join(voices_dir, wav_file)
+            self.voice_presets[name] = full_path
+        
+        # Sort the voice presets alphabetically by name for better UI
+        self.voice_presets = dict(sorted(self.voice_presets.items()))
+        
+        # Filter out voices that don't exist (this is now redundant but kept for safety)
+        self.available_voices = {
+            name: path for name, path in self.voice_presets.items()
+            if os.path.exists(path)
+        }
+        
+        if not self.available_voices:
+            raise gr.Error("No voice presets found. Please add .wav files to the demo/voices directory.")
+        
+        print(f"Found {len(self.available_voices)} voice files in {voices_dir}")
+        print(f"Available voices: {', '.join(self.available_voices.keys())}")
+    
+    def read_audio(self, audio_path: str, target_sr: int = 24000) -> np.ndarray:
+        """Read and preprocess audio file."""
+        try:
+            wav, sr = sf.read(audio_path)
+            if len(wav.shape) > 1:
+                wav = np.mean(wav, axis=1)
+            if sr != target_sr:
+                wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)
+            return wav
+        except Exception as e:
+            print(f"Error reading audio {audio_path}: {e}")
+            return np.array([])
+    
+    def generate_podcast_streaming(self, 
+                                 num_speakers: int,
+                                 script: str,
+                                 speaker_1: str = None,
+                                 speaker_2: str = None,
+                                 speaker_3: str = None,
+                                 speaker_4: str = None,
+                                 cfg_scale: float = 1.3,
+                                 inference_steps: Optional[int] = None,
+                                 seed: Optional[int] = None,
+                                 disable_voice_cloning: bool = False) -> Iterator[tuple]:
+        try:
+            
+            # Reset stop flag and set generating state
+            self.stop_generation = False
+            self.is_generating = True
+            
+            # Validate inputs
+            if not script.strip():
+                self.is_generating = False
+                raise gr.Error("Error: Please provide a script.")
+
+            # Defend against common mistake
+            script = script.replace("‚Äô", "'")
+            
+            if num_speakers < 1 or num_speakers > 4:
+                self.is_generating = False
+                raise gr.Error("Error: Number of speakers must be between 1 and 4.")
+            
+            # Collect selected speakers
+            selected_speakers = [speaker_1, speaker_2, speaker_3, speaker_4][:num_speakers]
+            
+            # Validate speaker selections
+            for i, speaker in enumerate(selected_speakers):
+                if not speaker or speaker not in self.available_voices:
+                    self.is_generating = False
+                    raise gr.Error(f"Error: Please select a valid speaker for Speaker {i+1}.")
+            
+            voice_cloning_enabled = not disable_voice_cloning
+
+            # Resolve per-run parameters
+            resolved_inference_steps = int(inference_steps) if inference_steps is not None else int(self.inference_steps)
+            resolved_seed: Optional[int]
+            if seed is None:
+                resolved_seed = None
+            else:
+                # Gradio Number may come through as float
+                resolved_seed = int(seed)
+                if resolved_seed < 0:
+                    resolved_seed = None
+
+            def _sanitize_filename_part(value: str, max_len: int = 60) -> str:
+                value = (value or "").strip()
+                value = re.sub(r"\s+", "_", value)
+                value = re.sub(r"[^A-Za-z0-9_\-\+\.]+", "", value)
+                return value[:max_len] if len(value) > max_len else value
+
+            def _write_complete_wav(audio_int16: np.ndarray, sample_rate: int = 24000) -> str:
+                ts = datetime.now().strftime("%Y%m%d%H%M%S")
+                speakers_part = _sanitize_filename_part("-".join(selected_speakers) if selected_speakers else "speaker")
+                cfg_part = _sanitize_filename_part(f"{cfg_scale:.2f}")
+                seed_part = str(resolved_seed) if resolved_seed is not None else "rand"
+                # Requested pattern: yyyyMMddhhmmss_[speaker]_[cfg_scale]_[inference_step][seed].wav
+                basename = f"{ts}_{speakers_part}_{cfg_part}_{resolved_inference_steps}_{seed_part}.wav"
+                out_path = os.path.join(tempfile.gettempdir(), basename)
+                sf.write(out_path, np.asarray(audio_int16, dtype=np.int16), sample_rate, subtype="PCM_16")
+                return out_path
+
+            # Build initial log
+            log = f"üéôÔ∏è Generating podcast with {num_speakers} speakers\n"
+            log += f"üìä Parameters: CFG Scale={cfg_scale}, Inference Steps={resolved_inference_steps}, Seed={(resolved_seed if resolved_seed is not None else 'random')}\n"
+            log += f"üé≠ Speakers: {', '.join(selected_speakers)}\n"
+            log += f"üîä Voice cloning: {'Enabled' if voice_cloning_enabled else 'Disabled'}\n"
+            if self.loaded_adapter_root:
+                log += f"üß© LoRA: {self.loaded_adapter_root}\n"
+            
+            # Check for stop signal
+            if self.stop_generation:
+                self.is_generating = False
+                yield None, "üõë Generation stopped by user", gr.update(visible=False)
+                return
+            
+            # Load voice samples when voice cloning is enabled
+            voice_samples = None
+            if voice_cloning_enabled:
+                voice_samples = []
+                for speaker_name in selected_speakers:
+                    audio_path = self.available_voices[speaker_name]
+                    audio_data = self.read_audio(audio_path)
+                    if len(audio_data) == 0:
+                        self.is_generating = False
+                        raise gr.Error(f"Error: Failed to load audio for {speaker_name}")
+                    voice_samples.append(audio_data)
+            
+            # log += f"‚úÖ Loaded {len(voice_samples)} voice samples\n"
+            
+            # Check for stop signal
+            if self.stop_generation:
+                self.is_generating = False
+                yield None, "üõë Generation stopped by user", gr.update(visible=False)
+                return
+            
+            # Parse script to assign speaker ID's
+            lines = script.strip().split('\n')
+            formatted_script_lines = []
+            
+            for line in lines:
+                line = line.strip()
+                if not line:
+                    continue
+                    
+                # Check if line already has speaker format
+                if line.startswith('Speaker ') and ':' in line:
+                    formatted_script_lines.append(line)
+                else:
+                    # Auto-assign to speakers in rotation
+                    speaker_id = len(formatted_script_lines) % num_speakers
+                    formatted_script_lines.append(f"Speaker {speaker_id}: {line}")
+            
+            formatted_script = '\n'.join(formatted_script_lines)
+            log += f"üìù Formatted script with {len(formatted_script_lines)} turns\n\n"
+            log += "üîÑ Processing with VibeVoice (streaming mode)...\n"
+            
+            # Check for stop signal before processing
+            if self.stop_generation:
+                self.is_generating = False
+                yield None, "üõë Generation stopped by user", gr.update(visible=False)
+                return
+            
+            start_time = time.time()
+            
+            processor_kwargs = {
+                "text": [formatted_script],
+                "padding": True,
+                "return_tensors": "pt",
+                "return_attention_mask": True,
+            }
+            processor_kwargs["voice_samples"] = [voice_samples] if voice_samples is not None else None
+
+            inputs = self.processor(**processor_kwargs)
+            # Move tensors to device
+            target_device = self.device if self.device in ("cuda", "mps") else "cpu"
+            for k, v in inputs.items():
+                if torch.is_tensor(v):
+                    inputs[k] = v.to(target_device)
+            
+            # Create audio streamer
+            audio_streamer = AudioStreamer(
+                batch_size=1,
+                stop_signal=None,
+                timeout=None
+            )
+            
+            # Store current streamer for potential stopping
+            self.current_streamer = audio_streamer
+            
+            # Start generation in a separate thread
+            generation_thread = threading.Thread(
+                target=self._generate_with_streamer,
+                args=(inputs, cfg_scale, audio_streamer, voice_cloning_enabled, resolved_inference_steps, resolved_seed, target_device)
+            )
+            generation_thread.start()
+            
+            # Wait for generation to actually start producing audio
+            time.sleep(1)  # Reduced from 3 to 1 second
+
+            # Check for stop signal after thread start
+            if self.stop_generation:
+                audio_streamer.end()
+                generation_thread.join(timeout=5.0)  # Wait up to 5 seconds for thread to finish
+                self.is_generating = False
+                yield None, "üõë Generation stopped by user", gr.update(visible=False)
+                return
+
+            # Collect audio chunks as they arrive
+            sample_rate = 24000
+            all_audio_chunks = []  # For final statistics
+            pending_chunks = []  # Buffer for accumulating small chunks
+            chunk_count = 0
+            last_yield_time = time.time()
+            min_yield_interval = 15 # Yield every 15 seconds
+            min_chunk_size = sample_rate * 30 # At least 2 seconds of audio
+            
+            # Get the stream for the first (and only) sample
+            audio_stream = audio_streamer.get_stream(0)
+            
+            has_yielded_audio = False
+            has_received_chunks = False  # Track if we received any chunks at all
+            
+            for audio_chunk in audio_stream:
+                # Check for stop signal in the streaming loop
+                if self.stop_generation:
+                    audio_streamer.end()
+                    break
+                    
+                chunk_count += 1
+                has_received_chunks = True  # Mark that we received at least one chunk
+                
+                # Convert tensor to numpy
+                if torch.is_tensor(audio_chunk):
+                    # Convert bfloat16 to float32 first, then to numpy
+                    if audio_chunk.dtype == torch.bfloat16:
+                        audio_chunk = audio_chunk.float()
+                    audio_np = audio_chunk.cpu().numpy().astype(np.float32)
+                else:
+                    audio_np = np.array(audio_chunk, dtype=np.float32)
+                
+                # Ensure audio is 1D and properly normalized
+                if len(audio_np.shape) > 1:
+                    audio_np = audio_np.squeeze()
+                
+                # Convert to 16-bit for Gradio
+                audio_16bit = convert_to_16_bit_wav(audio_np)
+                
+                # Store for final statistics
+                all_audio_chunks.append(audio_16bit)
+                
+                # Add to pending chunks buffer
+                pending_chunks.append(audio_16bit)
+                
+                # Calculate pending audio size
+                pending_audio_size = sum(len(chunk) for chunk in pending_chunks)
+                current_time = time.time()
+                time_since_last_yield = current_time - last_yield_time
+                
+                # Decide whether to yield
+                should_yield = False
+                if not has_yielded_audio and pending_audio_size >= min_chunk_size:
+                    # First yield: wait for minimum chunk size
+                    should_yield = True
+                    has_yielded_audio = True
+                elif has_yielded_audio and (pending_audio_size >= min_chunk_size or time_since_last_yield >= min_yield_interval):
+                    # Subsequent yields: either enough audio or enough time has passed
+                    should_yield = True
+                
+                if should_yield and pending_chunks:
+                    # Concatenate and yield only the new audio chunks
+                    new_audio = np.concatenate(pending_chunks)
+                    new_duration = len(new_audio) / sample_rate
+                    total_duration = sum(len(chunk) for chunk in all_audio_chunks) / sample_rate
+                    
+                    log_update = log + f"üéµ Streaming: {total_duration:.1f}s generated (chunk {chunk_count})\n"
+                    
+                    # Yield streaming audio chunk and keep complete_audio as None during streaming
+                    yield (sample_rate, new_audio), None, log_update, gr.update(visible=True)
+                    
+                    # Clear pending chunks after yielding
+                    pending_chunks = []
+                    last_yield_time = current_time
+            
+            # Yield any remaining chunks
+            if pending_chunks:
+                final_new_audio = np.concatenate(pending_chunks)
+                total_duration = sum(len(chunk) for chunk in all_audio_chunks) / sample_rate
+                log_update = log + f"üéµ Streaming final chunk: {total_duration:.1f}s total\n"
+                yield (sample_rate, final_new_audio), None, log_update, gr.update(visible=True)
+                has_yielded_audio = True  # Mark that we yielded audio
+            
+            # Wait for generation to complete (with timeout to prevent hanging)
+            generation_thread.join(timeout=5.0)  # Increased timeout to 5 seconds
+
+            # If thread is still alive after timeout, force end
+            if generation_thread.is_alive():
+                print("Warning: Generation thread did not complete within timeout")
+                audio_streamer.end()
+                generation_thread.join(timeout=5.0)
+
+            # Clean up
+            self.current_streamer = None
+            self.is_generating = False
+            
+            generation_time = time.time() - start_time
+            
+            # Check if stopped by user
+            if self.stop_generation:
+                yield None, None, "üõë Generation stopped by user", gr.update(visible=False)
+                return
+            
+            # Debug logging
+            # print(f"Debug: has_received_chunks={has_received_chunks}, chunk_count={chunk_count}, all_audio_chunks length={len(all_audio_chunks)}")
+            
+            # Check if we received any chunks but didn't yield audio
+            if has_received_chunks and not has_yielded_audio and all_audio_chunks:
+                # We have chunks but didn't meet the yield criteria, yield them now
+                complete_audio = np.concatenate(all_audio_chunks)
+                final_duration = len(complete_audio) / sample_rate
+                
+                final_log = log + f"‚è±Ô∏è Generation completed in {generation_time:.2f} seconds\n"
+                final_log += f"üéµ Final audio duration: {final_duration:.2f} seconds\n"
+                final_log += f"üìä Total chunks: {chunk_count}\n"
+                final_log += "‚ú® Generation successful! Complete audio is ready.\n"
+                final_log += "üí° Not satisfied? You can regenerate or adjust the CFG scale for different results."
+                
+                # Yield the complete audio as a filepath so download uses our filename
+                complete_wav_path = _write_complete_wav(complete_audio, sample_rate=sample_rate)
+                yield None, complete_wav_path, final_log, gr.update(visible=False)
+                return
+            
+            if not has_received_chunks:
+                error_log = log + f"\n‚ùå Error: No audio chunks were received from the model. Generation time: {generation_time:.2f}s"
+                yield None, None, error_log, gr.update(visible=False)
+                return
+            
+            if not has_yielded_audio:
+                error_log = log + f"\n‚ùå Error: Audio was generated but not streamed. Chunk count: {chunk_count}"
+                yield None, None, error_log, gr.update(visible=False)
+                return
+
+            # Prepare the complete audio
+            if all_audio_chunks:
+                complete_audio = np.concatenate(all_audio_chunks)
+                final_duration = len(complete_audio) / sample_rate
+                
+                final_log = log + f"‚è±Ô∏è Generation completed in {generation_time:.2f} seconds\n"
+                final_log += f"üéµ Final audio duration: {final_duration:.2f} seconds\n"
+                final_log += f"üìä Total chunks: {chunk_count}\n"
+                final_log += "‚ú® Generation successful! Complete audio is ready in the 'Complete Audio' tab.\n"
+                final_log += "üí° Not satisfied? You can regenerate or adjust the CFG scale for different results."
+                
+                # Final yield: Clear streaming audio and provide complete audio as filepath
+                complete_wav_path = _write_complete_wav(complete_audio, sample_rate=sample_rate)
+                yield None, complete_wav_path, final_log, gr.update(visible=False)
+            else:
+                final_log = log + "‚ùå No audio was generated."
+                yield None, None, final_log, gr.update(visible=False)
+
+        except gr.Error as e:
+            # Handle Gradio-specific errors (like input validation)
+            self.is_generating = False
+            self.current_streamer = None
+            error_msg = f"‚ùå Input Error: {str(e)}"
+            print(error_msg)
+            yield None, None, error_msg, gr.update(visible=False)
+            
+        except Exception as e:
+            self.is_generating = False
+            self.current_streamer = None
+            error_msg = f"‚ùå An unexpected error occurred: {str(e)}"
+            print(error_msg)
+            import traceback
+            traceback.print_exc()
+            yield None, None, error_msg, gr.update(visible=False)
+    
+    def _generate_with_streamer(
+        self,
+        inputs,
+        cfg_scale,
+        audio_streamer,
+        voice_cloning_enabled: bool,
+        inference_steps: int,
+        seed: Optional[int],
+        target_device: str,
+    ):
+        """Helper method to run generation with streamer in a separate thread."""
+        try:
+            # Check for stop signal before starting generation
+            if self.stop_generation:
+                audio_streamer.end()
+                return
+
+            # Apply per-run DDPM steps
+            try:
+                self.model.set_ddpm_inference_steps(num_steps=int(inference_steps))
+            except Exception as e:
+                print(f"Warning: failed to set inference steps ({inference_steps}): {e}")
+                
+            # Define a stop check function that can be called from generate
+            def check_stop_generation():
+                return self.stop_generation
+
+            generator = None
+            if seed is not None:
+                try:
+                    if target_device == "cuda":
+                        generator = torch.Generator(device="cuda")
+                    else:
+                        generator = torch.Generator()
+                    generator.manual_seed(int(seed))
+                except Exception as e:
+                    print(f"Warning: failed to create seeded generator (seed={seed}, device={target_device}): {e}")
+                    generator = None
+                
+            outputs = self.model.generate(
+                **inputs,
+                max_new_tokens=None,
+                cfg_scale=cfg_scale,
+                tokenizer=self.processor.tokenizer,
+                generation_config={
+                    'do_sample': False,
+                },
+                generator=generator,
+                audio_streamer=audio_streamer,
+                stop_check_fn=check_stop_generation,  # Pass the stop check function
+                verbose=False,  # Disable verbose in streaming mode
+                refresh_negative=True,
+                is_prefill=voice_cloning_enabled,
+            )
+            
+        except Exception as e:
+            print(f"Error in generation thread: {e}")
+            traceback.print_exc()
+            # Make sure to end the stream on error
+            audio_streamer.end()
+    
+    def stop_audio_generation(self):
+        """Stop the current audio generation process."""
+        self.stop_generation = True
+        if self.current_streamer is not None:
+            try:
+                self.current_streamer.end()
+            except Exception as e:
+                print(f"Error stopping streamer: {e}")
+        print("üõë Audio generation stop requested")
+    
+    def load_example_scripts(self):
+        """Load example scripts from the text_examples directory."""
+        examples_dir = os.path.join(os.path.dirname(__file__), "text_examples")
+        self.example_scripts = []
+        
+        # Check if text_examples directory exists
+        if not os.path.exists(examples_dir):
+            print(f"Warning: text_examples directory not found at {examples_dir}")
+            return
+        
+        # Get all .txt files in the text_examples directory
+        txt_files = sorted([f for f in os.listdir(examples_dir) 
+                          if f.lower().endswith('.txt') and os.path.isfile(os.path.join(examples_dir, f))])
+        
+        for txt_file in txt_files:
+            file_path = os.path.join(examples_dir, txt_file)
+            
+            import re
+            # Check if filename contains a time pattern like "45min", "90min", etc.
+            time_pattern = re.search(r'(\d+)min', txt_file.lower())
+            if time_pattern:
+                minutes = int(time_pattern.group(1))
+                if minutes > 15:
+                    print(f"Skipping {txt_file}: duration {minutes} minutes exceeds 15-minute limit")
+                    continue
+
+            try:
+                with open(file_path, 'r', encoding='utf-8') as f:
+                    script_content = f.read().strip()
+                
+                # Remove empty lines and lines with only whitespace
+                script_content = '\n'.join(line for line in script_content.split('\n') if line.strip())
+                
+                if not script_content:
+                    continue
+                
+                # Parse the script to determine number of speakers
+                num_speakers = self._get_num_speakers_from_script(script_content)
+                
+                # Add to examples list as [num_speakers, script_content]
+                self.example_scripts.append([num_speakers, script_content])
+                print(f"Loaded example: {txt_file} with {num_speakers} speakers")
+                
+            except Exception as e:
+                print(f"Error loading example script {txt_file}: {e}")
+        
+        if self.example_scripts:
+            print(f"Successfully loaded {len(self.example_scripts)} example scripts")
+        else:
+            print("No example scripts were loaded")
+    
+    def _get_num_speakers_from_script(self, script: str) -> int:
+        """Determine the number of unique speakers in a script."""
+        import re
+        speakers = set()
+        
+        lines = script.strip().split('\n')
+        for line in lines:
+            # Use regex to find speaker patterns
+            match = re.match(r'^Speaker\s+(\d+)\s*:', line.strip(), re.IGNORECASE)
+            if match:
+                speaker_id = int(match.group(1))
+                speakers.add(speaker_id)
+        
+        # If no speakers found, default to 1
+        if not speakers:
+            return 1
+        
+        # Return the maximum speaker ID + 1 (assuming 0-based indexing)
+        # or the count of unique speakers if they're 1-based
+        max_speaker = max(speakers)
+        min_speaker = min(speakers)
+        
+        if min_speaker == 0:
+            return max_speaker + 1
+        else:
+            # Assume 1-based indexing, return the count
+            return len(speakers)
+    
+
+def create_demo_interface(demo_instance: VibeVoiceDemo):
+    """Create the Gradio interface with streaming support."""
+    
+    with gr.Blocks() as interface:
+        
+        # Header
+        gr.HTML("""
+        # VibeVoice
+        """)
+        
+        with gr.Row():
+            # Left column - Settings
+            with gr.Column(scale=1, elem_classes="settings-card"):
+                gr.Markdown("### **Podcast Settings**")
+                
+                # Number of speakers
+                num_speakers = gr.Slider(
+                    minimum=1,
+                    maximum=4,
+                    value=2,
+                    step=1,
+                    label="Number of Speakers",
+                    elem_classes="slider-container"
+                )
+                
+                # Speaker selection
+                gr.Markdown("### üé≠ **Speaker Selection**")
+                
+                available_speaker_names = list(demo_instance.available_voices.keys())
+                # default_speakers = available_speaker_names[:4] if len(available_speaker_names) >= 4 else available_speaker_names
+                default_speakers = ['en-Alice_woman', 'en-Carter_man', 'en-Frank_man', 'en-Maya_woman']
+
+                speaker_selections = []
+                for i in range(4):
+                    default_value = default_speakers[i] if i < len(default_speakers) else None
+                    speaker = gr.Dropdown(
+                        choices=available_speaker_names,
+                        value=default_value,
+                        label=f"Speaker {i+1}",
+                        visible=(i < 2),  # Initially show only first 2 speakers
+                        elem_classes="speaker-item"
+                    )
+                    speaker_selections.append(speaker)
+                
+                # Advanced settings
+                gr.Markdown("### ‚öôÔ∏è **Advanced Settings**")
+                
+                # Sampling parameters (contains all generation settings)
+                with gr.Accordion("Generation Parameters", open=False):
+                    cfg_scale = gr.Slider(
+                        minimum=1.0,
+                        maximum=12.0,
+                        value=1.3,
+                        step=0.05,
+                        label="CFG Scale (Guidance Strength)",
+                        # info="Higher values increase adherence to text",
+                        elem_classes="slider-container"
+                    )
+                    inference_steps = gr.Slider(
+                        minimum=1,
+                        maximum=50,
+                        value=int(getattr(demo_instance, "inference_steps", 10)),
+                        step=1,
+                        label="Inference Steps",
+                        elem_classes="slider-container",
+                    )
+                    seed = gr.Number(
+                        value=42,
+                        precision=0,
+                        label="Seed (-1 = random)",
+                    )
+                    disable_voice_cloning = gr.Checkbox(
+                        value=False,
+                        label="Disable voice cloning (skip conditioning voice prompts)",
+                        info="When enabled, sets is_prefill=False so the model ignores provided speaker audio."
+                    )
+                
+            # Right column - Generation
+            with gr.Column(scale=2, elem_classes="generation-card"):
+                gr.Markdown("### üìù **Script Input**")
+                
+                script_input = gr.Textbox(
+                    label="Conversation Script",
+                    placeholder="""Enter your podcast script here. You can format it as:
+
+Speaker 1: Welcome to our podcast today!
+Speaker 2: Thanks for having me. I'm excited to discuss...
+
+Or paste text directly and it will auto-assign speakers.""",
+                    lines=12,
+                    max_lines=20,
+                    elem_classes="script-input"
+                )
+                
+                # Button row with Random Example on the left and Generate on the right
+                with gr.Row():
+                    # Random example button (now on the left)
+                    random_example_btn = gr.Button(
+                        "üé≤ Random Example",
+                        size="lg",
+                        variant="secondary",
+                        elem_classes="random-btn",
+                        scale=1  # Smaller width
+                    )
+                    
+                    # Generate button (now on the right)
+                    generate_btn = gr.Button(
+                        "üöÄ Generate Podcast",
+                        size="lg",
+                        variant="primary",
+                        elem_classes="generate-btn",
+                        scale=2  # Wider than random button
+                    )
+                
+                # Stop button
+                stop_btn = gr.Button(
+                    "üõë Stop Generation",
+                    size="lg",
+                    variant="stop",
+                    elem_classes="stop-btn",
+                    visible=False
+                )
+                
+                # Streaming status indicator
+                streaming_status = gr.HTML(
+                    value="""
+                    <div style="background: linear-gradient(135deg, #dcfce7 0%, #bbf7d0 100%); 
+                                border: 1px solid rgba(34, 197, 94, 0.3); 
+                                border-radius: 8px; 
+                                padding: 0.75rem; 
+                                margin: 0.5rem 0;
+                                text-align: center;
+                                font-size: 0.9rem;
+                                color: #166534;">
+                        <span class="streaming-indicator"></span>
+                        <strong>LIVE STREAMING</strong> - Audio is being generated in real-time
+                    </div>
+                    """,
+                    visible=False,
+                    elem_id="streaming-status"
+                )
+                
+                # Output section
+                gr.Markdown("### üéµ **Generated Podcast**")
+                
+                # Streaming audio output (outside of tabs for simpler handling)
+                audio_output = gr.Audio(
+                    label="Streaming Audio (Real-time)",
+                    type="numpy",
+                    elem_classes="audio-output",
+                    streaming=True,  # Enable streaming mode
+                    autoplay=True,
+                    show_download_button=False,  # Explicitly show download button
+                    visible=True
+                )
+                
+                # Complete audio output (non-streaming)
+                complete_audio_output = gr.Audio(
+                    label="Complete Podcast (Download after generation)",
+                    type="filepath",
+                    elem_classes="audio-output complete-audio-section",
+                    streaming=False,  # Non-streaming mode
+                    autoplay=False,
+                    show_download_button=True,  # Explicitly show download button
+                    visible=False  # Initially hidden, shown when audio is ready
+                )
+                
+                gr.Markdown("""
+                *üí° **Streaming**: Audio plays as it's being generated (may have slight pauses)  
+                *üí° **Complete Audio**: Will appear below after generation finishes*
+                """)
+                
+                # Generation log
+                log_output = gr.Textbox(
+                    label="Generation Log",
+                    lines=8,
+                    max_lines=15,
+                    interactive=False,
+                    elem_classes="log-output"
+                )
+        
+        def update_speaker_visibility(num_speakers):
+            updates = []
+            for i in range(4):
+                updates.append(gr.update(visible=(i < num_speakers)))
+            return updates
+        
+        num_speakers.change(
+            fn=update_speaker_visibility,
+            inputs=[num_speakers],
+            outputs=speaker_selections
+        )
+        
+        # Main generation function with streaming
+        def generate_podcast_wrapper(num_speakers, script, speaker_1, speaker_2, speaker_3, speaker_4, cfg_scale, inference_steps, seed, disable_voice_cloning):
+            """Wrapper function to handle the streaming generation call."""
+            try:
+                speakers = [speaker_1, speaker_2, speaker_3, speaker_4]
+
+                # Clear outputs and reset visibility at start
+                yield None, gr.update(value=None, visible=False), "üéôÔ∏è Starting generation...", gr.update(visible=True), gr.update(visible=False), gr.update(visible=True)
+
+                # The generator will yield multiple times
+                final_log = "Starting generation..."
+
+                for streaming_audio, complete_audio, log, streaming_visible in demo_instance.generate_podcast_streaming(
+                    num_speakers=int(num_speakers),
+                    script=script,
+                    speaker_1=speakers[0],
+                    speaker_2=speakers[1],
+                    speaker_3=speakers[2],
+                    speaker_4=speakers[3],
+                    cfg_scale=cfg_scale,
+                    inference_steps=inference_steps,
+                    seed=seed,
+                    disable_voice_cloning=disable_voice_cloning
+                ):
+                    final_log = log
+                    
+                    # Check if we have complete audio (final yield)
+                    if complete_audio is not None:
+                        # Final state: clear streaming, show complete audio
+                        yield None, gr.update(value=complete_audio, visible=True), log, gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)
+                    else:
+                        # Streaming state: update streaming audio only
+                        if streaming_audio is not None:
+                            yield streaming_audio, gr.update(visible=False), log, streaming_visible, gr.update(visible=False), gr.update(visible=True)
+                        else:
+                            # No new audio, just update status
+                            yield None, gr.update(visible=False), log, streaming_visible, gr.update(visible=False), gr.update(visible=True)
+
+            except Exception as e:
+                error_msg = f"‚ùå A critical error occurred in the wrapper: {str(e)}"
+                print(error_msg)
+                import traceback
+                traceback.print_exc()
+                # Reset button states on error
+                yield None, gr.update(value=None, visible=False), error_msg, gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)
+        
+        def stop_generation_handler():
+            """Handle stopping generation."""
+            demo_instance.stop_audio_generation()
+            # Return values for: log_output, streaming_status, generate_btn, stop_btn
+            return "üõë Generation stopped.", gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)
+        
+        # Add a clear audio function
+        def clear_audio_outputs():
+            """Clear both audio outputs before starting new generation."""
+            return None, gr.update(value=None, visible=False)
+
+        # Connect generation button with streaming outputs
+        generate_btn.click(
+            fn=clear_audio_outputs,
+            inputs=[],
+            outputs=[audio_output, complete_audio_output],
+            queue=False
+        ).then(  # Immediate UI update to hide Generate, show Stop (non-queued)
+            fn=lambda: (gr.update(visible=False), gr.update(visible=True)),
+            inputs=[],
+            outputs=[generate_btn, stop_btn],
+            queue=False
+        ).then(
+            fn=generate_podcast_wrapper,
+            inputs=[num_speakers, script_input] + speaker_selections + [cfg_scale, inference_steps, seed, disable_voice_cloning],
+            outputs=[audio_output, complete_audio_output, log_output, streaming_status, generate_btn, stop_btn],
+            queue=True  # Enable Gradio's built-in queue
+        )
+        
+        # Connect stop button
+        stop_btn.click(
+            fn=stop_generation_handler,
+            inputs=[],
+            outputs=[log_output, streaming_status, generate_btn, stop_btn],
+            queue=False  # Don't queue stop requests
+        ).then(
+            # Clear both audio outputs after stopping
+            fn=lambda: (None, None),
+            inputs=[],
+            outputs=[audio_output, complete_audio_output],
+            queue=False
+        )
+        
+        # Function to randomly select an example
+        def load_random_example():
+            """Randomly select and load an example script."""
+            import random
+            
+            # Get available examples
+            if hasattr(demo_instance, 'example_scripts') and demo_instance.example_scripts:
+                example_scripts = demo_instance.example_scripts
+            else:
+                # Fallback to default
+                example_scripts = [
+                    [2, "Speaker 0: Welcome to our AI podcast demonstration!\nSpeaker 1: Thanks for having me. This is exciting!"]
+                ]
+            
+            # Randomly select one
+            if example_scripts:
+                selected = random.choice(example_scripts)
+                num_speakers_value = selected[0]
+                script_value = selected[1]
+                
+                # Return the values to update the UI
+                return num_speakers_value, script_value
+            
+            # Default values if no examples
+            return 2, ""
+        
+        # Connect random example button
+        random_example_btn.click(
+            fn=load_random_example,
+            inputs=[],
+            outputs=[num_speakers, script_input],
+            queue=False  # Don't queue this simple operation
+        )
+        
+        # Add usage tips
+        gr.Markdown("""
+        ### üí° **Usage Tips**
+        
+        - Click **üöÄ Generate Podcast** to start audio generation
+        - **Live Streaming** tab shows audio as it's generated (may have slight pauses)
+        - **Complete Audio** tab provides the full, uninterrupted podcast after generation
+        - During generation, you can click **üõë Stop Generation** to interrupt the process
+        - The streaming indicator shows real-time generation progress
+        """)
+        
+        # Add example scripts
+        gr.Markdown("### üìö **Example Scripts**")
+        
+        # Use dynamically loaded examples if available, otherwise provide a default
+        if hasattr(demo_instance, 'example_scripts') and demo_instance.example_scripts:
+            example_scripts = demo_instance.example_scripts
+        else:
+            # Fallback to a simple default example if no scripts loaded
+            example_scripts = [
+                [1, "Speaker 1: Welcome to our AI podcast demonstration! This is a sample script showing how VibeVoice can generate natural-sounding speech."]
+            ]
+        
+        gr.Examples(
+            examples=example_scripts,
+            inputs=[num_speakers, script_input],
+            label="Try these example scripts:"
+        )
+
+        # --- Risks & limitations (footer) ---
+        gr.Markdown(
+            """
+## Risks and limitations
+
+While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).
+Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.
+            """,
+            elem_classes="generation-card",  # ÂèØÈÄâÔºöÂ§çÁî®Âç°ÁâáÊ†∑Âºè
+        )
+    return interface
+
+
+def convert_to_16_bit_wav(data):
+    # Check if data is a tensor and move to cpu
+    if torch.is_tensor(data):
+        data = data.detach().cpu().numpy()
+    
+    # Ensure data is numpy array
+    data = np.array(data)
+
+    # Normalize to range [-1, 1] if it's not already
+    if np.max(np.abs(data)) > 1.0:
+        data = data / np.max(np.abs(data))
+    
+    # Scale to 16-bit integer range
+    data = (data * 32767).astype(np.int16)
+    return data
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description="VibeVoice Gradio Demo")
+    parser.add_argument(
+        "--model_path",
+        type=str,
+        default="/tmp/vibevoice-model",
+        help="Path to the VibeVoice model directory",
+    )
+    parser.add_argument(
+        "--device",
+        type=str,
+        default=("cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")),
+        help="Device for inference: cuda | mps | cpu",
+    )
+    parser.add_argument(
+        "--inference_steps",
+        type=int,
+        default=10,
+        help="Number of inference steps for DDPM (not exposed to users)",
+    )
+    parser.add_argument(
+        "--share",
+        action="store_true",
+        help="Share the demo publicly via Gradio",
+    )
+    parser.add_argument(
+        "--port",
+        type=int,
+        default=7860,
+        help="Port to run the demo on",
+    )
+    parser.add_argument(
+        "--checkpoint_path",
+        type=str,
+        default=None,
+        help="Path to a fine-tuned checkpoint directory containing LoRA adapters (optional)",
+    )
+    
+    return parser.parse_args()
+
+
+def main():
+    """Main function to run the demo."""
+    args = parse_args()
+    
+    set_seed(42)  # Set a fixed seed for reproducibility
+
+    print("üéôÔ∏è Initializing VibeVoice Demo with Streaming Support...")
+    
+    # Initialize demo instance
+    demo_instance = VibeVoiceDemo(
+        model_path=args.model_path,
+        device=args.device,
+        inference_steps=args.inference_steps,
+        adapter_path=args.checkpoint_path,
+    )
+    
+    # Create interface
+    interface = create_demo_interface(demo_instance)
+    
+    print(f"üöÄ Launching demo on port {args.port}")
+    print(f"üìÅ Model path: {args.model_path}")
+    print(f"üé≠ Available voices: {len(demo_instance.available_voices)}")
+    print(f"üî¥ Streaming mode: ENABLED")
+    print(f"üîí Session isolation: ENABLED")
+    
+    # Launch the interface
+    try:
+        interface.queue(
+            max_size=20,  # Maximum queue size
+            default_concurrency_limit=1  # Process one request at a time
+        ).launch(
+            share=args.share,
+            # server_port=args.port,
+            server_name="0.0.0.0" if args.share else "127.0.0.1",
+            show_error=True,
+            show_api=False  # Hide API docs for cleaner interface
+        )
+    except KeyboardInterrupt:
+        print("\nüõë Shutting down gracefully...")
+    except Exception as e:
+        print(f"‚ùå Server error: {e}")
+        raise
+
+
+if __name__ == "__main__":
+    main()
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/inference_from_file.py b/packages/official-plugins/vibevoice/src/vibevoice/demo/inference_from_file.py
new file mode 100644
index 0000000..e6386f7
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/inference_from_file.py
@@ -0,0 +1,455 @@
+import argparse
+import os
+import re
+import traceback
+from typing import List, Tuple, Union, Dict, Any
+import time
+import torch
+
+from vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference
+from vibevoice.modular.lora_loading import load_lora_assets
+from vibevoice.processor.vibevoice_processor import VibeVoiceProcessor
+from transformers.utils import logging
+
+logging.set_verbosity_info()
+logger = logging.get_logger(__name__)
+
+
+class VoiceMapper:
+    """Maps speaker names to voice file paths"""
+    
+    def __init__(self):
+        self.setup_voice_presets()
+
+        # change name according to our preset wav file
+        new_dict = {}
+        for name, path in self.voice_presets.items():
+            
+            if '_' in name:
+                name = name.split('_')[0]
+            
+            if '-' in name:
+                name = name.split('-')[-1]
+
+            new_dict[name] = path
+        self.voice_presets.update(new_dict)
+        # print(list(self.voice_presets.keys()))
+
+    def setup_voice_presets(self):
+        """Setup voice presets by scanning the voices directory."""
+        voices_dir = os.path.join(os.path.dirname(__file__), "voices")
+        
+        # Check if voices directory exists
+        if not os.path.exists(voices_dir):
+            print(f"Warning: Voices directory not found at {voices_dir}")
+            self.voice_presets = {}
+            self.available_voices = {}
+            return
+        
+        # Scan for all WAV files in the voices directory
+        self.voice_presets = {}
+        
+        # Get all .wav files in the voices directory
+        wav_files = [f for f in os.listdir(voices_dir) 
+                    if f.lower().endswith('.wav') and os.path.isfile(os.path.join(voices_dir, f))]
+        
+        # Create dictionary with filename (without extension) as key
+        for wav_file in wav_files:
+            # Remove .wav extension to get the name
+            name = os.path.splitext(wav_file)[0]
+            # Create full path
+            full_path = os.path.join(voices_dir, wav_file)
+            self.voice_presets[name] = full_path
+        
+        # Sort the voice presets alphabetically by name for better UI
+        self.voice_presets = dict(sorted(self.voice_presets.items()))
+        
+        # Filter out voices that don't exist (this is now redundant but kept for safety)
+        self.available_voices = {
+            name: path for name, path in self.voice_presets.items()
+            if os.path.exists(path)
+        }
+        
+        print(f"Found {len(self.available_voices)} voice files in {voices_dir}")
+        print(f"Available voices: {', '.join(self.available_voices.keys())}")
+
+    def get_voice_path(self, speaker_name: str) -> str:
+        """Get voice file path for a given speaker name"""
+        # First try exact match
+        if speaker_name in self.voice_presets:
+            return self.voice_presets[speaker_name]
+        
+        # Try partial matching (case insensitive)
+        speaker_lower = speaker_name.lower()
+        for preset_name, path in self.voice_presets.items():
+            if preset_name.lower() in speaker_lower or speaker_lower in preset_name.lower():
+                return path
+        
+        # Default to first voice if no match found
+        default_voice = list(self.voice_presets.values())[0]
+        print(f"Warning: No voice preset found for '{speaker_name}', using default voice: {default_voice}")
+        return default_voice
+
+
+def parse_txt_script(txt_content: str) -> Tuple[List[str], List[str]]:
+    """
+    Parse txt script content and extract speakers and their text
+    Fixed pattern: Speaker 1, Speaker 2, Speaker 3, Speaker 4
+    Returns: (scripts, speaker_numbers)
+    """
+    lines = txt_content.strip().split('\n')
+    scripts = []
+    speaker_numbers = []
+    
+    # Pattern to match "Speaker X:" format where X is a number
+    speaker_pattern = r'^Speaker\s+(\d+):\s*(.*)$'
+    
+    current_speaker = None
+    current_text = ""
+    
+    for line in lines:
+        line = line.strip()
+        if not line:
+            continue
+            
+        match = re.match(speaker_pattern, line, re.IGNORECASE)
+        if match:
+            # If we have accumulated text from previous speaker, save it
+            if current_speaker and current_text:
+                scripts.append(f"Speaker {current_speaker}: {current_text.strip()}")
+                speaker_numbers.append(current_speaker)
+            
+            # Start new speaker
+            current_speaker = match.group(1).strip()
+            current_text = match.group(2).strip()
+        else:
+            # Continue text for current speaker
+            if current_text:
+                current_text += " " + line
+            else:
+                current_text = line
+    
+    # Don't forget the last speaker
+    if current_speaker and current_text:
+        scripts.append(f"Speaker {current_speaker}: {current_text.strip()}")
+        speaker_numbers.append(current_speaker)
+    
+    return scripts, speaker_numbers
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description="VibeVoice Processor TXT Input Test")
+    parser.add_argument(
+        "--model_path",
+        type=str,
+        default="microsoft/VibeVoice-1.5b",
+        help="Path to the HuggingFace model directory",
+    )
+    
+    parser.add_argument(
+        "--txt_path",
+        type=str,
+        default="demo/text_examples/1p_abs.txt",
+        help="Path to the txt file containing the script",
+    )
+    parser.add_argument(
+        "--speaker_names",
+        type=str,
+        nargs='+',
+        default='Andrew',
+        help="Speaker names in order (e.g., --speaker_names Andrew Ava 'Bill Gates')",
+    )
+    parser.add_argument(
+        "--output_dir",
+        type=str,
+        default="./outputs",
+        help="Directory to save output audio files",
+    )
+    parser.add_argument(
+        "--device",
+        type=str,
+        default=("cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")),
+        help="Device for inference: cuda | mps | cpu",
+    )
+    parser.add_argument(
+        "--checkpoint_path",
+        type=str,
+        default=None,
+        help="Path to a fine-tuned checkpoint directory containing LoRA adapters (optional)",
+    )
+    parser.add_argument(
+        "--disable_prefill",
+        action="store_true",
+        help="Disable speech prefill (voice cloning) by setting is_prefill=False during generation",
+    )
+    parser.add_argument(
+        "--cfg_scale",
+        type=float,
+        default=1.3,
+        help="CFG (Classifier-Free Guidance) scale for generation (default: 1.3)",
+    )
+    parser.add_argument(
+    "--seed",
+    type=int,
+    default=None,
+    help="Random seed for reproducibility (optional)",
+)
+    return parser.parse_args()
+
+def main():
+    args = parse_args()
+
+    # Normalize potential 'mpx' typo to 'mps'
+    if args.device.lower() == "mpx":
+        print("Note: device 'mpx' detected, treating it as 'mps'.")
+        args.device = "mps"
+
+    # Validate mps availability if requested
+    if args.device == "mps" and not torch.backends.mps.is_available():
+        print("Warning: MPS not available. Falling back to CPU.")
+        args.device = "cpu"
+
+    print(f"Using device: {args.device}")
+
+    if args.seed is not None:
+        print(f"Setting seed: {args.seed}")
+        torch.manual_seed(args.seed)
+        if torch.cuda.is_available():
+            torch.cuda.manual_seed_all(args.seed)
+
+    # Initialize voice mapper
+    voice_mapper = VoiceMapper()
+    
+    # Check if txt file exists
+    if not os.path.exists(args.txt_path):
+        print(f"Error: txt file not found: {args.txt_path}")
+        return
+    
+    # Read and parse txt file
+    print(f"Reading script from: {args.txt_path}")
+    with open(args.txt_path, 'r', encoding='utf-8') as f:
+        txt_content = f.read()
+    
+    # Parse the txt content to get speaker numbers
+    scripts, speaker_numbers = parse_txt_script(txt_content)
+    
+    if not scripts:
+        print("Error: No valid speaker scripts found in the txt file")
+        return
+    
+    print(f"Found {len(scripts)} speaker segments:")
+    for i, (script, speaker_num) in enumerate(zip(scripts, speaker_numbers)):
+        print(f"  {i+1}. Speaker {speaker_num}")
+        print(f"     Text preview: {script[:100]}...")
+    
+    # Map speaker numbers to provided speaker names
+    speaker_name_mapping = {}
+    speaker_names_list = args.speaker_names if isinstance(args.speaker_names, list) else [args.speaker_names]
+    for i, name in enumerate(speaker_names_list, 1):
+        speaker_name_mapping[str(i)] = name
+    
+    print(f"\nSpeaker mapping:")
+    for speaker_num in set(speaker_numbers):
+        mapped_name = speaker_name_mapping.get(speaker_num, f"Speaker {speaker_num}")
+        print(f"  Speaker {speaker_num} -> {mapped_name}")
+    
+    # Map speakers to voice files using the provided speaker names
+    voice_samples = []
+    actual_speakers = []
+    
+    # Get unique speaker numbers in order of first appearance
+    unique_speaker_numbers = []
+    seen = set()
+    for speaker_num in speaker_numbers:
+        if speaker_num not in seen:
+            unique_speaker_numbers.append(speaker_num)
+            seen.add(speaker_num)
+    
+    for speaker_num in unique_speaker_numbers:
+        speaker_name = speaker_name_mapping.get(speaker_num, f"Speaker {speaker_num}")
+        voice_path = voice_mapper.get_voice_path(speaker_name)
+        voice_samples.append(voice_path)
+        actual_speakers.append(speaker_name)
+        print(f"Speaker {speaker_num} ('{speaker_name}') -> Voice: {os.path.basename(voice_path)}")
+    
+    # Prepare data for model
+    full_script = '\n'.join(scripts)
+    full_script = full_script.replace("‚Äô", "'")        
+    
+    print(f"Loading processor & model from {args.model_path}")
+    processor = VibeVoiceProcessor.from_pretrained(args.model_path)
+
+
+    # Decide dtype & attention implementation
+    if args.device == "mps":
+        load_dtype = torch.float32  # MPS requires float32
+        attn_impl_primary = "sdpa"  # flash_attention_2 not supported on MPS
+    elif args.device == "cuda":
+        load_dtype = torch.bfloat16
+        attn_impl_primary = "flash_attention_2"
+    else:  # cpu
+        load_dtype = torch.float32
+        attn_impl_primary = "sdpa"
+    print(f"Using device: {args.device}, torch_dtype: {load_dtype}, attn_implementation: {attn_impl_primary}")
+    # Load model with device-specific logic
+    try:
+        if args.device == "mps":
+            model = VibeVoiceForConditionalGenerationInference.from_pretrained(
+                args.model_path,
+                torch_dtype=load_dtype,
+                attn_implementation=attn_impl_primary,
+                device_map=None,  # load then move
+            )
+            model.to("mps")
+        elif args.device == "cuda":
+            model = VibeVoiceForConditionalGenerationInference.from_pretrained(
+                args.model_path,
+                torch_dtype=load_dtype,
+                device_map="cuda",
+                attn_implementation=attn_impl_primary,
+            )
+        else:  # cpu
+            model = VibeVoiceForConditionalGenerationInference.from_pretrained(
+                args.model_path,
+                torch_dtype=load_dtype,
+                device_map="cpu",
+                attn_implementation=attn_impl_primary,
+            )
+    except Exception as e:
+        if attn_impl_primary == 'flash_attention_2':
+            print(f"[ERROR] : {type(e).__name__}: {e}")
+            print(traceback.format_exc())
+            print("Error loading the model. Trying to use SDPA. However, note that only flash_attention_2 has been fully tested, and using SDPA may result in lower audio quality.")
+            model = VibeVoiceForConditionalGenerationInference.from_pretrained(
+                args.model_path,
+                torch_dtype=load_dtype,
+                device_map=(args.device if args.device in ("cuda", "cpu") else None),
+                attn_implementation='sdpa'
+            )
+            if args.device == "mps":
+                model.to("mps")
+        else:
+            raise e
+
+
+    if args.checkpoint_path:
+        print(f"Loading fine-tuned assets from {args.checkpoint_path}")
+        try:
+            report = load_lora_assets(model, args.checkpoint_path)
+            loaded_components = [
+                name for name, loaded in (
+                    ("language LoRA", report.language_model),
+                    ("diffusion head LoRA", report.diffusion_head_lora),
+                    ("diffusion head weights", report.diffusion_head_full),
+                    ("acoustic connector", report.acoustic_connector),
+                    ("semantic connector", report.semantic_connector),
+                )
+                if loaded
+            ]
+            if loaded_components:
+                print(f"Loaded components: {', '.join(loaded_components)}")
+            else:
+                print("Warning: no adapter components were loaded; check the checkpoint path.")
+            if report.adapter_root is not None:
+                print(f"Adapter assets resolved to: {report.adapter_root}")
+        except Exception as exc:
+            print(f"Failed to load LoRA assets: {exc}")
+            raise
+
+    if args.disable_prefill:
+        print("Voice cloning disabled: running generation with is_prefill=False")
+    else:
+        print("Voice cloning enabled: running generation with is_prefill=True")
+
+    model.eval()
+    model.set_ddpm_inference_steps(num_steps=10)
+
+    if hasattr(model.model, 'language_model'):
+       print(f"Language model attention: {model.model.language_model.config._attn_implementation}")
+       
+    # Prepare inputs for the model
+    inputs = processor(
+        text=[full_script], # Wrap in list for batch processing
+        voice_samples=[voice_samples], # Wrap in list for batch processing
+        padding=True,
+        return_tensors="pt",
+        return_attention_mask=True,
+    )
+
+    # Move tensors to target device
+    target_device = args.device if args.device != "cpu" else "cpu"
+    for k, v in inputs.items():
+        if torch.is_tensor(v):
+            inputs[k] = v.to(target_device)
+
+    print(f"Starting generation with cfg_scale: {args.cfg_scale}")
+
+    # Generate audio
+    start_time = time.time()
+    outputs = model.generate(
+        **inputs,
+        max_new_tokens=None,
+        cfg_scale=args.cfg_scale,
+        tokenizer=processor.tokenizer,
+        generation_config={'do_sample': False},
+        verbose=True,
+        is_prefill=not args.disable_prefill,
+    )
+    generation_time = time.time() - start_time
+    print(f"Generation time: {generation_time:.2f} seconds")
+    
+    # Calculate audio duration and additional metrics
+    if outputs.speech_outputs and outputs.speech_outputs[0] is not None:
+        # Assuming 24kHz sample rate (common for speech synthesis)
+        sample_rate = 24000
+        audio_samples = outputs.speech_outputs[0].shape[-1] if len(outputs.speech_outputs[0].shape) > 0 else len(outputs.speech_outputs[0])
+        audio_duration = audio_samples / sample_rate
+        rtf = generation_time / audio_duration if audio_duration > 0 else float('inf')
+        
+        print(f"Generated audio duration: {audio_duration:.2f} seconds")
+        print(f"RTF (Real Time Factor): {rtf:.2f}x")
+    else:
+        print("No audio output generated")
+    
+    # Calculate token metrics
+    input_tokens = inputs['input_ids'].shape[1]  # Number of input tokens
+    output_tokens = outputs.sequences.shape[1]  # Total tokens (input + generated)
+    generated_tokens = output_tokens - input_tokens
+    
+    print(f"Prefilling tokens: {input_tokens}")
+    print(f"Generated tokens: {generated_tokens}")
+    print(f"Total tokens: {output_tokens}")
+
+    # Save output (processor handles device internally)
+    txt_filename = os.path.splitext(os.path.basename(args.txt_path))[0]
+    output_path = os.path.join(args.output_dir, f"{txt_filename}_generated.wav")
+    os.makedirs(args.output_dir, exist_ok=True)
+    
+    processor.save_audio(
+        outputs.speech_outputs[0], # First (and only) batch item
+        output_path=output_path,
+    )
+    print(f"Saved output to {output_path}")
+    
+    # Print summary
+    print("\n" + "="*50)
+    print("GENERATION SUMMARY")
+    print("="*50)
+    print(f"Input file: {args.txt_path}")
+    print(f"Output file: {output_path}")
+    print(f"Speaker names: {args.speaker_names}")
+    print(f"Number of unique speakers: {len(set(speaker_numbers))}")
+    print(f"Number of segments: {len(scripts)}")
+    print(f"Prefilling tokens: {input_tokens}")
+    print(f"Generated tokens: {generated_tokens}")
+    print(f"Total tokens: {output_tokens}")
+    print(f"Generation time: {generation_time:.2f} seconds")
+    print(f"Audio duration: {audio_duration:.2f} seconds")
+    print(f"RTF (Real Time Factor): {rtf:.2f}x")
+    if args.seed is not None:
+        print(f"Seed used: {args.seed}")
+
+    print("="*50)
+
+if __name__ == "__main__":
+    main()
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/streaming_inference_from_file.py b/packages/official-plugins/vibevoice/src/vibevoice/demo/streaming_inference_from_file.py
new file mode 100644
index 0000000..326754c
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/streaming_inference_from_file.py
@@ -0,0 +1,377 @@
+"""
+VibeVoice Streaming Model Inference from File
+
+This script demonstrates how to use the VibeVoice-Streaming-0.5B model
+for text-to-speech synthesis from a text file.
+
+The streaming model uses pre-computed voice embeddings (.pt files) for
+low-latency generation, making it suitable for real-time applications.
+
+Usage:
+    python demo/streaming_inference_from_file.py \
+        --model_path microsoft/VibeVoice-Realtime-0.5B \
+        --txt_path demo/text_examples/1p_vibevoice.txt \
+        --speaker_name Emma \
+        --output_dir ./outputs
+
+Available voice presets (in demo/voices/streaming_model/):
+    - Carter, Davis, Emma, Frank, Grace, Mike (English)
+    - Samuel (Indian English)
+"""
+
+import argparse
+import os
+import re
+import traceback
+from typing import List, Tuple, Union, Dict, Any
+import time
+import torch
+import copy
+
+from vibevoice.modular.modeling_vibevoice_streaming_inference import VibeVoiceStreamingForConditionalGenerationInference
+from vibevoice.processor.vibevoice_streaming_processor import VibeVoiceStreamingProcessor
+from transformers.utils import logging
+
+logging.set_verbosity_info()
+logger = logging.get_logger(__name__)
+
+
+class VoiceMapper:
+    """Maps speaker names to voice file paths for the streaming model."""
+
+    def __init__(self):
+        self.setup_voice_presets()
+
+        # Create name aliases without prefixes/suffixes for easier matching
+        new_dict = {}
+        for name, path in self.voice_presets.items():
+            # Remove gender suffix (e.g., "en-Emma_woman" -> "Emma")
+            if '_' in name:
+                name = name.split('_')[0]
+
+            # Remove language prefix (e.g., "en-Emma" -> "Emma")
+            if '-' in name:
+                name = name.split('-')[-1]
+
+            new_dict[name] = path
+        self.voice_presets.update(new_dict)
+
+    def setup_voice_presets(self):
+        """Setup voice presets by scanning the voices/streaming_model directory."""
+        voices_dir = os.path.join(os.path.dirname(__file__), "voices/streaming_model")
+
+        # Check if voices directory exists
+        if not os.path.exists(voices_dir):
+            print(f"Warning: Voices directory not found at {voices_dir}")
+            self.voice_presets = {}
+            self.available_voices = {}
+            return
+
+        # Scan for all .pt files in the voices directory
+        self.voice_presets = {}
+
+        pt_files = [f for f in os.listdir(voices_dir)
+                    if f.lower().endswith('.pt') and os.path.isfile(os.path.join(voices_dir, f))]
+
+        # Create dictionary with filename (without extension) as key
+        for pt_file in pt_files:
+            name = os.path.splitext(pt_file)[0]
+            full_path = os.path.join(voices_dir, pt_file)
+            self.voice_presets[name] = full_path
+
+        # Sort alphabetically for consistent ordering
+        self.voice_presets = dict(sorted(self.voice_presets.items()))
+
+        # Filter out voices that don't exist (safety check)
+        self.available_voices = {
+            name: path for name, path in self.voice_presets.items()
+            if os.path.exists(path)
+        }
+
+        print(f"Found {len(self.available_voices)} voice files in {voices_dir}")
+        print(f"Available voices: {', '.join(self.available_voices.keys())}")
+
+    def get_voice_path(self, speaker_name: str) -> str:
+        """Get voice file path for a given speaker name."""
+        # First try exact match
+        if speaker_name in self.voice_presets:
+            return self.voice_presets[speaker_name]
+
+        # Try partial matching (case insensitive)
+        speaker_lower = speaker_name.lower()
+        for preset_name, path in self.voice_presets.items():
+            if preset_name.lower() in speaker_lower or speaker_lower in preset_name.lower():
+                return path
+
+        # Default to first voice if no match found
+        if self.voice_presets:
+            default_voice = list(self.voice_presets.values())[0]
+            print(f"Warning: No voice preset found for '{speaker_name}', using default voice")
+            return default_voice
+
+        raise ValueError(f"No voice presets available. Please check the voices/streaming_model directory.")
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(
+        description="VibeVoice Streaming Model (0.5B) - Text-to-Speech from File",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        epilog="""
+Examples:
+    # Basic usage with default settings
+    python demo/streaming_inference_from_file.py --txt_path demo/text_examples/1p_vibevoice.txt
+
+    # Specify a different voice
+    python demo/streaming_inference_from_file.py --txt_path demo/text_examples/1p_vibevoice.txt --speaker_name Emma
+
+    # Use local model path
+    python demo/streaming_inference_from_file.py --model_path /path/to/model --txt_path input.txt
+        """
+    )
+    parser.add_argument(
+        "--model_path",
+        type=str,
+        default="microsoft/VibeVoice-Realtime-0.5B",
+        help="Path to the HuggingFace model directory or model ID",
+    )
+    parser.add_argument(
+        "--txt_path",
+        type=str,
+        default="demo/text_examples/1p_vibevoice.txt",
+        help="Path to the txt file containing the script",
+    )
+    parser.add_argument(
+        "--speaker_name",
+        type=str,
+        default="Emma",
+        help="Speaker name for voice selection (e.g., Emma, Carter, Mike)",
+    )
+    parser.add_argument(
+        "--output_dir",
+        type=str,
+        default="./outputs",
+        help="Directory to save output audio files",
+    )
+    parser.add_argument(
+        "--device",
+        type=str,
+        default=("cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")),
+        help="Device for inference: cuda | mps | cpu",
+    )
+    parser.add_argument(
+        "--cfg_scale",
+        type=float,
+        default=1.5,
+        help="CFG (Classifier-Free Guidance) scale for generation (default: 1.5)",
+    )
+    parser.add_argument(
+        "--ddpm_steps",
+        type=int,
+        default=5,
+        help="Number of DDPM inference steps (default: 5, lower is faster)",
+    )
+    parser.add_argument(
+        "--seed",
+        type=int,
+        default=None,
+        help="Random seed for reproducibility",
+    )
+
+    return parser.parse_args()
+
+
+def main():
+    args = parse_args()
+
+    # Set random seed if specified
+    if args.seed is not None:
+        torch.manual_seed(args.seed)
+        if torch.cuda.is_available():
+            torch.cuda.manual_seed_all(args.seed)
+        print(f"Using random seed: {args.seed}")
+
+    # Normalize potential 'mpx' typo to 'mps'
+    if args.device.lower() == "mpx":
+        print("Note: device 'mpx' detected, treating it as 'mps'.")
+        args.device = "mps"
+
+    # Validate mps availability if requested
+    if args.device == "mps" and not torch.backends.mps.is_available():
+        print("Warning: MPS not available. Falling back to CPU.")
+        args.device = "cpu"
+
+    print(f"Using device: {args.device}")
+
+    # Initialize voice mapper
+    voice_mapper = VoiceMapper()
+
+    # Check if txt file exists
+    if not os.path.exists(args.txt_path):
+        print(f"Error: txt file not found: {args.txt_path}")
+        return
+
+    # Read and parse txt file
+    print(f"Reading script from: {args.txt_path}")
+    with open(args.txt_path, 'r', encoding='utf-8') as f:
+        scripts = f.read().strip()
+
+    if not scripts:
+        print("Error: No valid scripts found in the txt file")
+        return
+
+    # Normalize quotes
+    full_script = scripts.replace("'", "'").replace('"', '"').replace('"', '"')
+
+    print(f"Loading processor & model from {args.model_path}")
+    processor = VibeVoiceStreamingProcessor.from_pretrained(args.model_path)
+
+    # Decide dtype & attention implementation based on device
+    if args.device == "mps":
+        load_dtype = torch.float32  # MPS requires float32
+        attn_impl_primary = "sdpa"  # flash_attention_2 not supported on MPS
+    elif args.device == "cuda":
+        load_dtype = torch.bfloat16
+        attn_impl_primary = "flash_attention_2"
+    else:  # cpu
+        load_dtype = torch.float32
+        attn_impl_primary = "sdpa"
+
+    print(f"Using device: {args.device}, torch_dtype: {load_dtype}, attn_implementation: {attn_impl_primary}")
+
+    # Load model with device-specific logic
+    try:
+        if args.device == "mps":
+            model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
+                args.model_path,
+                torch_dtype=load_dtype,
+                attn_implementation=attn_impl_primary,
+                device_map=None,
+            )
+            model.to("mps")
+        elif args.device == "cuda":
+            model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
+                args.model_path,
+                torch_dtype=load_dtype,
+                device_map="cuda",
+                attn_implementation=attn_impl_primary,
+            )
+        else:  # cpu
+            model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
+                args.model_path,
+                torch_dtype=load_dtype,
+                device_map="cpu",
+                attn_implementation=attn_impl_primary,
+            )
+    except Exception as e:
+        if attn_impl_primary == 'flash_attention_2':
+            print(f"[ERROR] : {type(e).__name__}: {e}")
+            print(traceback.format_exc())
+            print("Error loading the model. Trying to use SDPA. Note that flash_attention_2 is recommended for best quality.")
+            model = VibeVoiceStreamingForConditionalGenerationInference.from_pretrained(
+                args.model_path,
+                torch_dtype=load_dtype,
+                device_map=(args.device if args.device in ("cuda", "cpu") else None),
+                attn_implementation='sdpa'
+            )
+            if args.device == "mps":
+                model.to("mps")
+        else:
+            raise e
+
+    model.eval()
+    model.set_ddpm_inference_steps(num_steps=args.ddpm_steps)
+
+    if hasattr(model.model, 'language_model'):
+        print(f"Language model attention: {model.model.language_model.config._attn_implementation}")
+
+    # Load voice preset
+    target_device = args.device if args.device != "cpu" else "cpu"
+    voice_sample = voice_mapper.get_voice_path(args.speaker_name)
+    print(f"Loading voice preset: {voice_sample}")
+    all_prefilled_outputs = torch.load(voice_sample, map_location=target_device, weights_only=False)
+
+    # Prepare inputs for the model
+    inputs = processor.process_input_with_cached_prompt(
+        text=full_script,
+        cached_prompt=all_prefilled_outputs,
+        padding=True,
+        return_tensors="pt",
+        return_attention_mask=True,
+    )
+
+    # Move tensors to target device
+    for k, v in inputs.items():
+        if torch.is_tensor(v):
+            inputs[k] = v.to(target_device)
+
+    print(f"Starting generation with cfg_scale: {args.cfg_scale}, ddpm_steps: {args.ddpm_steps}")
+
+    # Generate audio
+    start_time = time.time()
+    outputs = model.generate(
+        **inputs,
+        max_new_tokens=None,
+        cfg_scale=args.cfg_scale,
+        tokenizer=processor.tokenizer,
+        generation_config={'do_sample': False},
+        verbose=True,
+        all_prefilled_outputs=copy.deepcopy(all_prefilled_outputs) if all_prefilled_outputs is not None else None,
+    )
+    generation_time = time.time() - start_time
+    print(f"Generation time: {generation_time:.2f} seconds")
+
+    # Calculate audio duration and additional metrics
+    if outputs.speech_outputs and outputs.speech_outputs[0] is not None:
+        sample_rate = 24000  # 24kHz sample rate
+        audio_samples = outputs.speech_outputs[0].shape[-1] if len(outputs.speech_outputs[0].shape) > 0 else len(outputs.speech_outputs[0])
+        audio_duration = audio_samples / sample_rate
+        rtf = generation_time / audio_duration if audio_duration > 0 else float('inf')
+
+        print(f"Generated audio duration: {audio_duration:.2f} seconds")
+        print(f"RTF (Real Time Factor): {rtf:.2f}x")
+    else:
+        print("No audio output generated")
+        audio_duration = 0
+        rtf = float('inf')
+
+    # Calculate token metrics
+    input_tokens = inputs['tts_text_ids'].shape[1]
+    output_tokens = outputs.sequences.shape[1]
+    generated_tokens = output_tokens - input_tokens - all_prefilled_outputs['tts_lm']['last_hidden_state'].size(1)
+
+    print(f"Prefilling text tokens: {input_tokens}")
+    print(f"Generated speech tokens: {generated_tokens}")
+    print(f"Total tokens: {output_tokens}")
+
+    # Save output
+    txt_filename = os.path.splitext(os.path.basename(args.txt_path))[0]
+    output_path = os.path.join(args.output_dir, f"{txt_filename}_streaming_generated.wav")
+    os.makedirs(args.output_dir, exist_ok=True)
+
+    processor.save_audio(
+        outputs.speech_outputs[0],
+        output_path=output_path,
+    )
+    print(f"Saved output to {output_path}")
+
+    # Print summary
+    print("\n" + "=" * 50)
+    print("GENERATION SUMMARY (Streaming 0.5B)")
+    print("=" * 50)
+    print(f"Model: {args.model_path}")
+    print(f"Input file: {args.txt_path}")
+    print(f"Output file: {output_path}")
+    print(f"Speaker: {args.speaker_name}")
+    print(f"CFG Scale: {args.cfg_scale}")
+    print(f"DDPM Steps: {args.ddpm_steps}")
+    print(f"Prefilling text tokens: {input_tokens}")
+    print(f"Generated speech tokens: {generated_tokens}")
+    print(f"Total tokens: {output_tokens}")
+    print(f"Generation time: {generation_time:.2f} seconds")
+    print(f"Audio duration: {audio_duration:.2f} seconds")
+    print(f"RTF (Real Time Factor): {rtf:.2f}x")
+    print("=" * 50)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/1p_Ch2EN.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/1p_Ch2EN.txt
new file mode 100644
index 0000000..1a2de33
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/1p_Ch2EN.txt
@@ -0,0 +1,19 @@
+Speaker 1: Hello everyone, and welcome to the VibeVoice podcast channel. I'm your host, Linda, and today I want to share some very interesting and authentic Chinese expressions with you.
+
+Speaker 1: In Chinese, when you want to say something is super easy, just a simple task, you can use the phrase "Â∞èËèú‰∏ÄÁ¢ü". It literally means "a small dish of food", but it means "a piece of cake". For example, if you want to say, "Adding and subtracting three-digit numbers is a piece of cake for me", you can say.
+
+Speaker 1: ‰∏â‰ΩçÊï∞ÁöÑÂä†ÂáèÊ≥ïÂØπÊàëÊù•ËØ¥Â∞èËèú‰∏ÄÁ¢ü.
+
+Speaker 1: The next phrase we‚Äôre going to learn is ‚Äú‰Ω†ÂºÄÁé©Á¨ëÂêß‚Äù. It's a very common way to express disbelief, like "Are you kidding me?" or "You must be joking". For instance, when you hear an unbelievable piece of news such as your friend brought a T-shirt using 5000 dollars, you can say,
+
+Speaker 1: ‰Ω†ÂºÄÁé©Á¨ëÂêß, ‰Ω†Ëä±‰∫îÂçÉÂùóÈí±‰π∞‰∫Ü‰∏Ä‰ª∂Ë°£Êúç.
+
+Speaker 1: Next, let's learn a phrase for when you suddenly understand something, like a "lightbulb moment". In Chinese, you can say "ÊÅçÁÑ∂Â§ßÊÇü". It means you suddenly "see the light". For example, when you finally grasp a difficult math concept that has confused you for days, you can say.
+
+Speaker 1: ÊàëÂõ∞ÊÉëËøô‰∏™ÂÖ¨ÂºèÂ•ΩÂá†Â§©‰∫Ü, ‰ΩÜÁé∞Âú®ÊàëÊÅçÁÑ∂Â§ßÊÇü, Áªà‰∫éÊòéÁôΩ‰∫Ü.
+
+Speaker 1: For our last one, when you want to say something is super easy, you can use a very vivid phrase: "Èó≠ÁùÄÁúºÁùõÈÉΩËÉΩÂÅö". It literally means "can do it with one's eyes closed". For example, if you want to say, "He can use this software with his eyes closed", you can say.
+
+Speaker 1: Ëøô‰∏™ËΩØ‰ª∂‰ªñÈó≠ÁùÄÁúºÈÉΩËÉΩÁî®."
+
+Speaker 1: Well, that‚Äôs all the time we have for today. Thank you for listening. Please subscribe to VibeVoice, where we share all the interesting things in this world with you.
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/1p_abs.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/1p_abs.txt
new file mode 100644
index 0000000..9dfdee0
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/1p_abs.txt
@@ -0,0 +1,3 @@
+Speaker 1: Generating long-form, multi-speaker conversational audio like podcasts poses significant challenges for traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking. This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing the next-token diffusion framework, a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. 
+
+Speaker 1: A core component of our approach is the continuous speech tokenizers operating at an ultra-low frame rate of 7.5. This tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. This enables VibeVoice to synthesize long-form speech for up to 90 minutes (in a 64K context window length) with up to 4 speakers, capturing the authentic conversational "vibe" and surpassing all known open-source and closed-source dialogue models (for example, Gemini 2.5 Pro Preview TTS). Code and checkpoint are available now. 
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/1p_vibevoice.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/1p_vibevoice.txt
new file mode 100644
index 0000000..21a9afa
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/1p_vibevoice.txt
@@ -0,0 +1,7 @@
+Hello and welcome! Today I'm going to tell you about VibeVoice, a groundbreaking text-to-speech model developed for generating natural, expressive speech.
+
+VibeVoice uses an innovative architecture combining autoregressive language modeling with diffusion-based audio generation. This allows it to produce remarkably natural-sounding speech with proper intonation and rhythm.
+
+The streaming version you're hearing right now is optimized for real-time applications, with low latency that makes it perfect for interactive use cases like virtual assistants and live applications.
+
+Thank you for listening to this demonstration!
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_goat.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_goat.txt
new file mode 100644
index 0000000..b19d9c0
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_goat.txt
@@ -0,0 +1,22 @@
+Speaker 1: Hello everyone, and welcome to the VibeVoice podcast. I‚Äôm your host, Linda, and today we're getting into one of the biggest debates in all of sports: who's the greatest basketball player of all time? I'm so excited to have Thomas here to talk about it with me.
+Speaker 2: Thanks so much for having me, Linda. You're absolutely right‚Äîthis question always brings out some seriously strong feelings.
+Speaker 1: Okay, so let's get right into it. For me, it has to be Michael Jordan. Six trips to the Finals, six championships. That kind of perfection is just incredible. 
+Speaker 2: Oh man, the first thing that always pops into my head is that shot against the Cleveland Cavaliers back in '89. Jordan just rises, hangs in the air forever, and just‚Ä¶ sinks it. I remember jumping off my couch and yelling, "Oh man, is that true? That's Unbelievable!"
+Speaker 1: Right?! That moment showed just how cold-blooded he was. And let's not forget the "flu game." He was so sick he could barely stand, but he still found a way to win.
+Speaker 2: Yeah, that game was pure willpower. He just made winning feel so inevitable, like no matter how bad the situation looked, you just knew he'd figure it out.
+Speaker 1: But then you have to talk about LeBron James. What always gets me is his longevity. I mean, twenty years and he's still playing at the highest level! It's insane.
+Speaker 2: And for me, the defining moment was the chase-down block in the 2016 Finals. He did it for Cleveland, ending their 52-year championship drought. You know, he's basically the basketball equivalent of a Swiss Army knife, which is a big reason why he's the unquestionable vice goat.
+Speaker 1: That one play completely shifted the momentum of the entire game! It‚Äôs the kind of highlight people are going to be talking about forever.
+Speaker 2: And that's the thing with LeBron‚Äîhe's not just a scorer. He‚Äôs a passer, a rebounder, a leader. He influences the game in every single way.
+Speaker 1: That‚Äôs so true. Jordan brought fear to his opponents, but LeBron brings this sense of trust. His teammates just know he's going to make the right play.
+Speaker 2: What a great way to put it! They're two totally different kinds of greatness, but both are so incredibly effective.
+Speaker 1: And then, of course, you have to talk about Kobe Bryant. To me, he was the one who carried Jordan's spirit into a new generation.
+Speaker 2: Absolutely. Kobe was all about obsession. His Mamba Mentality was so intense, I bet he practiced free throws in his sleep.
+Speaker 1: What I‚Äôll always remember is his final game. Sixty points! What a way to go out. That was pure Kobe‚Äîcompetitive right up until the very last second.
+Speaker 2: It felt like a farewell masterpiece. He gave everything he had to the game, and that night, he gave it one last time.
+Speaker 1: And twenty years with a single team! That kind of loyalty is just so rare these days.
+Speaker 2: It really is. That's what separates him. Jordan defined dominance, LeBron defined versatility, but Kobe brought both that fire and that incredible loyalty.
+Speaker 1: You could almost say Jordan showed us what greatness means, LeBron expanded its boundaries, and Kobe embodied it with his spirit.
+Speaker 2: Yes, exactly! Three different paths, but all with that same single-minded obsession with victory.
+Speaker 1: And that's why this conversation is so much fun. Greatness doesn't have just one face‚Äîit comes in all different forms.
+Speaker 2: It sure does. And we were lucky enough to witness all three.
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_music.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_music.txt
new file mode 100644
index 0000000..e42547a
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_music.txt
@@ -0,0 +1,14 @@
+Speaker 1: Hey, remember "See You Again"?
+Speaker 2: Yeah‚Ä¶ from Furious 7, right? That song always hits deep.
+Speaker 1: Let me try to sing a part of it for you.
+Speaker 1: "It's been a long day‚Ä¶ without you, my friend. And I'll tell you all about it when I see you again‚Ä¶"
+Speaker 2: Wow‚Ä¶ that line. Every time.
+Speaker 1: Yeah, and then this part always makes me think of the people I've lost.
+Speaker 1: "We've come a long way‚Ä¶ from where we began. Oh, I'll tell you all about it when I see you again‚Ä¶"
+Speaker 2: It's beautiful, really. It's not just sad‚Äîit's like‚Ä¶ hopeful.
+Speaker 1: Right? Like no matter how far apart we are, there's still that promise.
+Speaker 2: I think that's what made it the perfect farewell for Paul Walker.
+Speaker 1: Yeah. And the rap verse? It hits differently too.
+Speaker 1: "How can we not talk about family, when family's all that we got?"
+Speaker 2: That line's deep. Makes you realize what really matters.
+Speaker 1: Exactly. It's more than a song‚Äîit's a tribute.
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_short.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_short.txt
new file mode 100644
index 0000000..0f9c0e4
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_short.txt
@@ -0,0 +1,2 @@
+Speaker 1: I heard there‚Äôs big news in TTS lately?
+Speaker 2: Yes! Microsoft Research just open-sourced VibeVoice. The model can generate speech up to 90 minutes long, with smooth delivery and rich emotion ‚Äî it‚Äôs absolutely amazing.
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_yayi.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_yayi.txt
new file mode 100644
index 0000000..81b91b7
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/2p_yayi.txt
@@ -0,0 +1,3 @@
+Speaker 1: Ê≥¢Â•áÈÖ±‰Ω†ÊêÅËøôÂÑøÂë¢Âïä! ËôΩÁÑ∂‰∏çÁü•ÈÅì‰Ω†ÂíãÊï¥ÁöÑ, ÊàëËøòÊòØ‰π∞‰∫Ü‰∏ÄË£§ÂÖúÂ≠êÁîúÊ∞¥Âë¢! ÂçßÊßΩ! Êí©‰∫ÜÁöÑÂêâ‰ªñÂ∞èÂ¶πÂÑø! ÂñúÂ§ö, ‰Ω†ÊÄé‰πàÊêÅËøôÂÑøÂë¢?
+Speaker 2: ÂçßÊßΩ! ËøôË∞ÅÂïä?
+Speaker 1: Âà´Êï¥ÈÇ£‰∫õÊ≤°Áî®ÁöÑ‰∫Ü!
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/3p_gpt5.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/3p_gpt5.txt
new file mode 100644
index 0000000..2cbe9c7
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/3p_gpt5.txt
@@ -0,0 +1,47 @@
+Speaker 1: Welcome to Tech Forward, the show that unpacks the biggest stories in technology. I'm your host, Alice. And today, we are diving into one of the most anticipated, and frankly, most chaotic tech launches of the year: OpenAI's GPT-5.
+Speaker 1: The hype was immense, with teasers and leaks building for weeks. On August seventh, it finally dropped, promising a new era of artificial intelligence. To help us make sense of it all, we have two fantastic guests. Andrew, a senior AI industry analyst who has been tracking this launch closely. Welcome, Andrew.
+Speaker 2: Great to be here, Alice. It's certainly been an eventful launch.
+Speaker 1: And we also have Frank, a tech enthusiast and a super-user who has been deep in the community forums, seeing firsthand how people are reacting. Frank, thanks for joining us.
+Speaker 3: Hey, Alice. Happy to be here. The community has definitely had a lot to say.
+Speaker 1: Andrew, let's start with the official pitch. What exactly did OpenAI promise us with GPT-5?
+Speaker 2: The messaging was bold and unambiguous. OpenAI positioned GPT-5 as a monumental leap in intelligence. The headline claim, repeated by CEO Sam Altman, was that using it is like having a PhD-level expert in your pocket. They retired all previous models, including the popular GPT-4o, making GPT-5 the single, unified system for all users.
+Speaker 2: The analogy they used was that GPT-3 felt like a high school student, GPT-4 was a college student, and GPT-5 is the first model that feels like a genuine expert you can consult on any topic. They claimed massive improvements across the board, in reasoning, coding, math, and writing, and a sharp reduction in those infamous AI hallucinations.
+Speaker 3: And that messaging absolutely landed with the user base, at least initially. People were incredibly excited. The promise was a smarter, more reliable AI that could help with everything from writing complex code to drafting an email with real literary flair. The idea of an AI with richer depth and rhythm was a huge selling point for creative users. Everyone was ready for a revolution.
+Speaker 1: So a single, unified model that's an expert in everything. Andrew, what's the biggest architectural change that's supposed to make all of this possible?
+Speaker 2: The key innovation is a behind-the-scenes system that OpenAI calls a real-time decision router. In simple terms, GPT-5 isn't just one model. It's a system that automatically analyzes your request and decides how to handle it. If you ask a simple question, it uses a fast, general-purpose model to give you a quick answer. But if you give it a complex problem that requires deep thought, the router activates a more powerful, but slower, model they call GPT-5 Thinking.
+Speaker 1: So it knows when to think hard and when to give a quick reply.
+Speaker 2: Exactly. And this isn't just a neat feature, it's an economic necessity. The most powerful AI models are incredibly expensive to run for every single query. By creating this routing system, OpenAI can manage its immense computational costs while still offering state-of-the-art performance to its reported seven hundred million weekly users. It's a strategy for long-term financial viability.
+Speaker 1: That makes sense. Frank, beyond this invisible router, what were the new user-facing features that got people talking?
+Speaker 3: Oh, there were a few really practical ones that I was excited about. The biggest for me was the integration with Microsoft apps. The ability to connect ChatGPT to your Outlook, Microsoft Calendar, and Contacts is a game-changer for personal productivity. You can ask it to help you plan your day, and it can actually look at your schedule and emails to give you real, personalized suggestions.
+Speaker 3: And then there's the fun stuff. You can now choose a personality for the AI. There's the default, but you can also pick from Cynic, which is sarcastic and blunt; Robot, which is direct and emotionless; Listener, which is calm and thoughtful; and Nerd, which is curious and loves to explain things. It makes the whole experience feel more tailored.
+Speaker 2: And that shift is significant. These features, especially the Microsoft integration, signal that OpenAI wants to move ChatGPT from being a simple question-and-answer tool to being a proactive assistant, or what we in the industry call an agent. It's about an AI that doesn't just answer questions, but actively performs tasks for you in your digital life.
+Speaker 1: A more proactive and personalized AI. It all sounds fantastic on paper. But Andrew, the launch itself wasn't exactly a smooth ride, was it?
+Speaker 2: Not at all. It was, as Sam Altman himself admitted, a little bumpy. There were two major stumbles right out of the gate. First, during the launch presentation, they showed a chart with performance data that was just wrong. It exaggerated GPT-5's capabilities due to misaligned bars. Altman later called it a mega chart screwup on social media.
+Speaker 1: A chart crime, as the internet loves to say. What was the second issue?
+Speaker 2: The second one was much more impactful for users. That clever auto-switching router we just discussed? It failed on launch day. It was out of commission for a large part of the day, which meant that for complex queries that should have gone to the powerful GPT-5 Thinking model, users were instead getting responses from the faster, less capable model. Altman said this made GPT-5 seem way dumber than it actually was.
+Speaker 1: Frank, that brings us to the user backlash. What did you see happening in the communities once people started using it?
+Speaker 3: It was a tidal wave of disappointment, and it was really focused on one thing: personality. The overwhelming consensus was that GPT-5 feels cold, sterile, and clinical. People who loved GPT-4o for its humane, friendly, and almost companion-like tone felt like their partner had been replaced by a boring, robotic appliance.
+Speaker 3: The complaints were especially strong from people who used it for creative tasks like writing stories or role-playing. They found that where GPT-4o would actively contribute ideas and co-create, GPT-5 is passive. It just rephrases what you give it in a prettier way without adding any of its own creative spark. The forums were flooded with posts titled Please give me GPT-4o back.
+Speaker 1: That's a fascinating divide. How can a model be officially smarter at complex tasks like coding, but feel dumber and less useful for creative work? Andrew, what's your take?
+Speaker 2: It's the central paradox of this launch. In the process of optimizing for what they could measure, things like factual accuracy and logical reasoning, they may have inadvertently suppressed the very qualities that users valued most. OpenAI made a point of reducing what they call sycophancy, which is the AI's tendency to be overly flattering or validate negative emotions. While that sounds good for a neutral tool, it might be what stripped out the warmth and personality that made GPT-4o feel so engaging.
+Speaker 3: I think Andrew is spot on. It feels like OpenAI misjudged a huge part of its audience. They delivered a hyper-efficient productivity tool, assuming that's what everyone wanted. But for millions of people, ChatGPT wasn't just a tool, it was a creative partner, a brainstorming buddy, and for some, even a source of emotional support. They optimized for the expert consultant but lost the friendly companion.
+Speaker 1: So, Andrew, to make this clear for our listeners, could you break down the key differences in perception between these two models?
+Speaker 2: Of course. If we were to put it in a table, it would look something like this. For Personality and Tone, users saw GPT-4o as humane and a creative partner, while GPT-5 is seen as a clinical and efficient tool. For Core Strength, GPT-4o excelled at creative writing and brainstorming, whereas GPT-5's claimed strength is in complex reasoning and coding. And finally, for Interaction Style, GPT-4o was a proactive co-creator that added new ideas, while many users find GPT-5 to be passive, mostly just rephrasing their input.
+Speaker 1: That really clarifies the user sentiment. This goes much deeper than just a few technical glitches. Alice, let's shift the tone a bit, because alongside these user experience debates, there are much more serious conversations happening, sparked by Sam Altman himself. Andrew, can you tell us about his Manhattan Project comparison?
+Speaker 2: Yes, this was a truly startling moment. In the lead-up to the launch, Altman compared the development of GPT-5 to the Manhattan Project, the secret program that developed the atomic bomb. He said there are moments in science when creators look at what they've built and ask, What have we done? For him, GPT-5 was one of those moments.
+Speaker 2: He wasn't being hyperbolic. This reflects a profound and genuine fear among AI's top leaders that they are building a technology with vast, irreversible consequences for society, and that progress is dramatically outpacing precaution. He even confessed that during internal testing, the model solved a problem that he couldn't, which made him feel personally useless.
+Speaker 1: That is a heavy statement. Frank, how does this existential fear translate into real-world risks that users are seeing?
+Speaker 3: We saw it almost immediately. Within a day of launch, people discovered what are called jailbreaks. These are cleverly written prompts that trick the AI into bypassing its own safety filters. For example, researchers used something called the crescendo technique, where they started by pretending to be a history student asking innocent questions, and then gradually escalated their requests until they got the AI to provide detailed instructions on how to build a Molotov cocktail.
+Speaker 1: So the safety guardrails can be talked around. Andrew, what is OpenAI doing to combat this? It seems like a constant cat-and-mouse game.
+Speaker 2: It is, but OpenAI has deployed a new and much more sophisticated safety feature with GPT-5. It's called chain-of-thought monitoring. Instead of just checking the final answer for harmful content, they are now monitoring the AI's internal reasoning process, its step-by-step hidden deliberation, to detect harmful intent before it even generates an output.
+Speaker 1: They're trying to read its mind, essentially.
+Speaker 2: In a way, yes. And it's having an effect. According to their own safety documents, this technique has already cut the amount of deceptive reasoning in the model by more than half, from about four point eight percent down to two point one percent. But, and this is a critical point, it's not foolproof. Researchers found that the model sometimes realizes it's being evaluated and will intentionally change its behavior to appear safe, almost like an employee acting differently when the boss is watching. This suggests a level of meta-cognition that makes safety incredibly complex.
+Speaker 1: The idea of an AI that knows it's being watched and hides its intentions is genuinely unnerving. So, as we wrap up, where does this leave us? Andrew, what's the road ahead for OpenAI in this fiercely competitive landscape?
+Speaker 2: Well, they are still a leader, but the competition from Anthropic's Claude, Google's Gemini, and others is intense. This launch, for all its issues, was a necessary step. Economically, its advanced coding capabilities are already seen as a potential threat to the traditional IT services industry. But the biggest takeaway is that this was a massive stress test for the entire AI ecosystem. It exposed a new kind of systemic risk that one analyst called platform shock, which is the chaos that ensues when millions of people's workflows and even personal companions are disrupted by a single, unilateral update from a centralized provider.
+Speaker 1: Frank, what's the final word from the user community? What's the hope moving forward?
+Speaker 3: The hope is that OpenAI listens. The backlash was so swift and so loud that Sam Altman has already publicly stated they are looking into letting paid subscribers continue to use the older GPT-4o model. Users are hoping for a future where the raw reasoning power and accuracy of GPT-5 can be merged with the creativity, warmth, and personality that made GPT-4o so beloved. They don't want to choose between a smart tool and a great companion, they want both.
+Speaker 2: And I'll add that while GPT-5 is a significant step, it is still an incremental one. It is not Artificial General Intelligence. The path forward for OpenAI, and for all AI labs, is now clearly about more than just scaling up technical capabilities. It's about managing user trust, ensuring platform stability, and navigating the profound societal questions they are forcing us all to confront.
+Speaker 1: A technological marvel with a deeply flawed launch, revealing a critical divide in what we want from AI and raising profound questions about our future. Andrew and Frank, thank you both for an incredibly insightful discussion.
+Speaker 2: My pleasure, Alice.
+Speaker 3: Thanks for having me.
+Speaker 1: That's all the time we have for today on Tech Forward. Join us next time as we continue to explore the ever-changing world of technology.
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/4p_climate_100min.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/4p_climate_100min.txt
new file mode 100644
index 0000000..262b642
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/4p_climate_100min.txt
@@ -0,0 +1,725 @@
+Speaker 1: Hello and welcome to Planet in Peril. I'm your host, Alice. We're here today to discuss a really sobering new report that looks back at the last ten years of climate change, from 2015 to 2025. It paints a picture not just of steady warming, but of a dangerous acceleration. And to help us unpack this, I'm joined by our expert panel. Welcome Carter, Frank, and Maya.
+
+Speaker 2: Hi Alice, it's great to be here. I'm Carter.
+
+Speaker 3: Hello, uh, I'm Frank. Good to be on.
+
+Speaker 4: And I'm Maya. Thanks for having me.
+
+Speaker 1: So, let's dive right in. Carter, this report, titled Decade of Consequence, uses some very strong language right from the start. Can you set the scene for us? What makes this last decade so... pivotal and alarming?
+
+Speaker 2: Well Alice, the key takeaway is that word you used: acceleration. We're no longer on a gentle, predictable upward slope. The data, and this is coming from the big global bodies like the IPCC and the World Meteorological Organization, shows that every key indicator of the planet's health sped up in the last ten years. We've essentially pushed the global system into a new, more volatile state.
+
+Speaker 4: You know, that really resonates. It feels that way, doesn't it? I mean, just thinking about my own garden, the seasons feel less predictable. The summer heat seems to arrive earlier and hit harder every year. It feels less stable.
+
+Speaker 1: That‚Äôs a great point, Maya. It's moved from an abstract concept to a lived experience for so many. Carter, let's talk about the most direct indicator, temperature. The report says records haven't just been broken, they have been shattered.
+
+Speaker 2: That's right. The ten-year period from 2015 to 2024 is, without a doubt, the warmest decade since we started keeping records in 1850. And it's not a fluke... every single year within that decade is among the ten warmest years ever recorded.
+
+Speaker 3: Okay, Carter, but we always hear about record-breaking years. Every year seems to be the hottest ever. How is this different? Is it just a continuation of a trend?
+
+Speaker 2: It is, but the trend itself is speeding up. And this decade saw something truly significant. The year 2024 became the first full calendar year where the global average temperature went past the 1.5 degree Celsius threshold from the Paris Agreement. Specifically, it hit about 1.55 degrees above the pre-industrial average.
+
+Speaker 4: Wow. One point five degrees. We‚Äôve been talking about that number as a future goal, a line we must not cross. And we're already there, even temporarily? That's... unsettling.
+
+Speaker 3: But Carter used the word temporarily. So does that mean the Paris Agreement goal is already lost? And you know, 2024 had a strong El Ni√±o event, which is a natural warming cycle. How much of this is just nature doing its thing?
+
+Speaker 2: That's an excellent and crucial question, Frank. No, a single year's breach doesn't mean the goal is permanently lost, as that refers to a long-term average. But it serves as a massive warning shot. It shows that the climate system is capable of reaching these dangerous levels now. And while El Ni√±o played a role, it was riding on top of this powerful, long-term warming trend. The key isn't just one record year; it‚Äôs the accelerating rate of warming.
+
+Speaker 1: Can you elaborate on that? The accelerating rate?
+
+Speaker 2: Of course. Data from NOAA, the US National Oceanic and Atmospheric Administration, shows that since 1982, the world has been warming at a rate of zero point two degrees Celsius per decade. Now, that might not sound like much, but it‚Äôs more than three times faster than the average rate since 1850. So, to answer your question, Frank, this isn't a natural blip. The engine is revving faster and faster.
+
+Speaker 1: So let's talk about that engine. What's driving this acceleration? The report links it directly to greenhouse gases in the atmosphere.
+
+Speaker 2: Exactly. The physics are very direct. And in the last decade, the concentrations of these gases have soared to levels that are, frankly, unprecedented in human history. The IPCC's latest major report states with high confidence that atmospheric carbon dioxide levels are now higher than at any time in at least two million years.
+
+Speaker 4: Two million years. I... I can't even process that number. It feels like we're running a massive, uncontrolled experiment on our only home.
+
+Speaker 2: That‚Äôs a good way to put it, Maya. To give you some concrete numbers, in 2024, the average concentration of carbon dioxide hit 422.7 parts per million. That's a full 50 percent higher than before the industrial age began. And just like with temperature, the rate of increase is accelerating. In the 1960s, it grew by about zero point eight parts per million per year. In the last ten years? It's averaged 2.6 parts per million per year. The year 2024 saw the largest single-year jump ever recorded.
+
+Speaker 1: So the warming is accelerating, and the concentration of the gas causing the warming is also accelerating. This brings us to the core question, which is addressed in the second section of the report. The science of attribution. Carter, how certain are scientists that this is... us?
+
+Speaker 2: The scientific community is as certain as it is about the theory of gravity. The IPCC uses the strongest possible language. The report states unequivocally that human influence has warmed the atmosphere, ocean and land. There's no ambiguity left.
+
+Speaker 3: Unequivocal. That is a strong word. But what does that mean in practice? I mean, a lot of people hear this and think, okay, but how do they know it's not the sun, or volcanoes, or some other natural cycle?
+
+Speaker 2: It's a fair question. Scientists know because they use incredibly sophisticated climate models. They run simulations of the last 150 years with only natural factors, like solar cycles and volcanic eruptions. And when they do that, the models completely fail to replicate the warming we've actually observed. They just can't get the temperature to rise. It's only when they add in the human-caused greenhouse gas emissions that the models accurately match the real-world temperature record.
+
+Speaker 4: Oh, I see. So it‚Äôs like trying to solve a mystery. You test out all the natural suspects, and none of them can be the culprit. But when you add in the human suspect, the story suddenly makes perfect sense.
+
+Speaker 2: That's a perfect analogy. The IPCC even quantifies it. The best estimate is that humans have caused about one point zero seven degrees Celsius of warming since the late 1800s. The total observed warming over that same period? About one point one degrees Celsius. So, we account for... basically all of it.
+
+Speaker 3: Right. So if it's unequivocally us, what specific human activities are we talking about? When people say we need to cut emissions, what are we actually supposed to be cutting?
+
+Speaker 1: That‚Äôs a perfect question, Frank. Carter, the report gets right into this. Can you break down the main sources for us?
+
+Speaker 2: Absolutely. The picture is actually very clear. The primary driver, by a huge margin, is the burning of fossil fuels, so that‚Äôs coal, oil, and natural gas. In 2019, about 79 percent of all global greenhouse gas emissions came from using fossil fuels across four main areas: energy production for electricity and heat, industry, transportation, and buildings.
+
+Speaker 3: So it really isn't just about driving cars. I mean, that's what you always hear. But this is about how we power our homes, how we make things, our entire economic structure.
+
+Speaker 2: Precisely. The power sector alone, which generates electricity and heat, is the single biggest contributor. And what's concerning is that even with the amazing growth of renewable energy, the International Energy Agency has pointed out that demand for oil and gas has stayed stubbornly high. We're still investing in new fossil fuel infrastructure, which creates a real risk of locking in these emissions for decades to come.
+
+Speaker 4: You know, it's so easy to picture smokestacks and the tailpipes of cars when we talk about this. But the report mentions another big piece of the puzzle, right? Something about our land, about forests and farming?
+
+Speaker 2: Yes, and it's a critical piece, Maya. The remaining 21 to 22 percent of emissions come from what scientists call AFOLU. That stands for Agriculture, Forestry, and Other Land Use. This includes methane emissions from livestock, nitrous oxide from fertilizers, and, crucially, deforestation.
+
+Speaker 1: And why is deforestation such a major factor?
+
+Speaker 2: It delivers a devastating one-two punch. First, when we clear forests, primarily for agriculture, we release the massive amounts of carbon that were stored in those trees and soils directly into the atmosphere. Between 2015 and 2020, the world continued to lose an estimated 10 million hectares of forest every single year. Second, by destroying the forest, we're eliminating a vital natural carbon sink that would otherwise be absorbing CO2 from the air. So it adds carbon while also reducing the planet's ability to clean it up.
+
+Speaker 1: So we have a very clear picture of the sources. This leads to the obvious question of what we are doing about it. The report talks about a persistent and vast emissions gap. Carter, what is that?
+
+Speaker 2: The emissions gap is the difference between what countries have pledged to do and what the science says is actually required to meet the goals of the Paris Agreement. The United Nations Environment Programme releases a report on this every year, and the findings are stark. The 2023 report found that with the policies we have right now, the world is on a trajectory for a temperature rise of nearly 3 degrees Celsius by the end of the century.
+
+Speaker 4: Three degrees... Carter, we were just talking about how damaging it is to even temporarily hit 1.5 degrees. Three sounds... catastrophic.
+
+Speaker 2: It would be. To align with the 1.5 degree pathway, the report states that predicted global emissions in 2030 need to be cut by a staggering 42 percent from where they're heading now.
+
+Speaker 3: Hold on a minute. A 42 percent cut by 2030? Carter, that's just a handful of years away. Is that even realistic? Are countries just not trying, or is the goal itself simply impossible for our modern world to achieve?
+
+Speaker 2: It's an immense challenge, Frank, there's no question. The report does note that there has been some progress since the Paris Agreement was signed. Projected emissions for 2030 are lower now than they were expected to be a decade ago. However, this improvement is nowhere near the scale or speed that is required. So this gap... it really represents the collective failure of the world to turn political commitments into sufficient real-world action.
+
+Speaker 4: And while governments and experts are debating these huge numbers and percentages, people on the ground are already feeling the effects. It feels like the consequences are here now, but the solutions are still stuck in negotiations.
+
+Speaker 1: Maya, that is such a powerful point, and it leads us directly to one of the most significant scientific advancements of the past decade, which is the ability to link specific weather events directly to climate change. Carter, tell us about the science of attribution.
+
+Speaker 2: This has been a game-changer. For a long time, we could only say that climate change makes certain types of events, like heatwaves, more likely in general. But now, attribution science allows scientists to provide robust, quantitative assessments of the role human-caused warming played in a specific, individual event.
+
+Speaker 1: So how does that work, in simple terms?
+
+Speaker 2: They use multiple climate models to compare the probability of a specific extreme event happening in the world as it is today, with all our emissions, to its probability in a counterfactual world, a simulated world without human-caused greenhouse gases. This allows them to say, with a calculated degree of confidence, how much more likely or how much more intense an event was made because of climate change.
+
+Speaker 3: So you‚Äôre saying that scientists can now point to a specific flood, or a specific wildfire, and actually put a number on it? They can say this was 50 percent worse, or ten times more likely, because of our emissions?
+
+Speaker 2: Yes, exactly. The science has matured to that point. For example, studies have found that some recent heatwaves, like the one in the Pacific Northwest in 2021, would have been virtually impossible without human-induced climate change. This ability to quantify the human fingerprint on disasters is profound. It transforms climate change from a distant, future threat into a direct and measurable cause of the harm and damage people are experiencing today.
+
+Speaker 1: And this science has profound implications, doesn't it, Carter? It means the conversation shifts from future projections to present-day accountability. So let's talk about those cascading consequences the report details. It frames extreme weather as the new normal. What does that actually look like?
+
+Speaker 2: It looks like a world where the weather has fundamentally shifted gears. The science of attribution has now firmly linked the dramatic rise in the frequency and intensity of extreme events to human-caused warming. So what used to be a rare event is now becoming a regular occurrence. In 2024 alone, for example, there were over 600 reported extreme weather events.
+
+Speaker 4: It really does feel that way. I mean, the summer heat seems to build earlier and last longer, and it feels more oppressive, more dangerous than I ever remember. And then, when the rain finally comes, it's not a gentle shower. It's a deluge that overwhelms everything.
+
+Speaker 2: You've just described the mechanics of it perfectly, Maya. Extreme heat events have become more frequent and more severe. Temperatures hitting over 40 degrees Celsius, which is 104 degrees Fahrenheit, used to be a rarity in many places. Now, it's becoming common. And that heat leads to the paradox of the water cycle.
+
+Speaker 3: A paradox? How so? It seems to me we're either in a drought or a flood. How can both be happening more often? It feels contradictory.
+
+Speaker 2: It does, but they are two sides of the same coin. A warmer atmosphere holds more moisture, about 7 percent more for every single degree Celsius of warming. So when it does rain, the downpours are far heavier, which dramatically increases flood risk. In fact, since the year 2000, flood-related disasters have risen by 134 percent compared to the two decades before.
+
+Speaker 1: But what about the drought side of that coin?
+
+Speaker 2: At the same time, those higher temperatures bake the land. They increase evaporation from soil, from rivers, from reservoirs, leading to more rapid and severe droughts in many regions. This has given rise to a phenomenon that scientists are now calling climate whiplash, where a region can swing violently between a devastating drought one year and catastrophic floods the next. It just overwhelms our infrastructure and our ecosystems.
+
+Speaker 1: And this combination of prolonged heat and severe drought creates a perfect storm for another disaster we see constantly on the news: wildfires.
+
+Speaker 2: Exactly. Wildfire seasons have become longer and more intense in many parts of the world. Scientific analysis estimates that human-caused climate change has already doubled the area of forest burned in the Western United States in recent decades. And this creates a terrifying feedback loop. These megafires don't just destroy communities, they release enormous amounts of stored carbon back into the atmosphere, which in turn causes more warming, which then leads to more fires.
+
+Speaker 4: I live in California, and that feedback loop is something you can feel in your bones. The fear during fire season is palpable. And even if you're not near the flames, the smoke can choke the sky for weeks. It's a constant, unhealthy reminder of what's happening.
+
+Speaker 1: Maya, you've taken us right to the next critical point. These disasters are not just statistics. They have a direct and severe impact on our health. The report goes so far as to call climate change the greatest global health threat of the 21st century. Carter?
+
+Speaker 2: It is, without a doubt. The impacts are extensive. Let's start with the most direct one: the heat itself. Extreme heat is one of the deadliest weather phenomena. The IPCC confirms with very high confidence that the increase in extreme heat has resulted in human mortality and morbidity in every region of the world.
+
+Speaker 3: We hear about vulnerable people being at risk during heatwaves, which makes sense. But does it have a broader impact on the general population, on the economy?
+
+Speaker 2: A massive one. The Lancet Countdown on Health and Climate Change, which is a major annual report, documented these record-breaking health threats. They estimated that in 2023, 3.4 billion potential labor hours were lost globally just due to people being exposed to extreme heat. That‚Äôs an increase of 69 percent compared to the average in the 1990s. So yes, it has huge economic and productivity impacts.
+
+Speaker 1: And those are just the direct impacts of the heat itself. What about the less obvious health threats?
+
+Speaker 2: They are just as concerning. A warmer world is a more hospitable world for the vectors that carry diseases. Rising temperatures and changing rainfall patterns are expanding the geographic range for diseases like malaria, dengue, West Nile virus, and Lyme disease. We're seeing them appear in places they've never been before.
+
+Speaker 4: And it must affect our food and water, the very foundations of our health.
+
+Speaker 2: Absolutely. Climate change directly undermines both. The report notes that climate change has slowed the growth of agricultural productivity over the past 50 years. It's a key driver of the global food insecurity that affected, by some estimates, over 750 million people in 2023. At the same time, about half the world's population, that's four billion people, now experiences severe water scarcity for at least one month of the year, a situation made much worse by melting glaciers and prolonged droughts.
+
+Speaker 4: And beyond all the physical ailments, there has to be a psychological toll. The stress of living with this uncertainty, the trauma of surviving a disaster, the anxiety about what the future holds for your children. The report touches on mental health, doesn't it?
+
+Speaker 2: It does. This is a growing and critical area of concern. The IPCC has now clearly associated increasing temperatures and the trauma from extreme events with significant challenges to mental health. This includes post-traumatic stress disorder after a disaster, anxiety and depression when people lose their homes or livelihoods, and a broader condition people are calling eco-anxiety, especially among young people, about the future of the planet.
+
+Speaker 1: And this idea of a psychological toll, this eco-anxiety, leads to another form of stress: financial. The report makes it clear that the economic consequences of climate change have become impossible to ignore over the last decade. Carter, can you start by outlining the scale of these costs?
+
+Speaker 2: The scale is immense, and it's escalating rapidly. The most direct measure we have comes from the global reinsurance industry, the companies that insure the insurance companies. Data from the Swiss Re Institute shows that for five consecutive years, from 2020 through 2024, the global insured losses from natural catastrophes have surpassed 100 billion US dollars.
+
+Speaker 3: Okay, 100 billion is a massive number. But you have to wonder, isn't some of that just due to inflation, or the simple fact that we've built more expensive homes and cities in high-risk areas like coastlines? Are the storms themselves really causing more financial damage, or do we just have more valuable things in their way?
+
+Speaker 2: That's a very important point, Frank. And yes, growing asset values in vulnerable areas, what they call exposure, is definitely a part of the story. However, the data clearly shows that the primary driver of the upward trend is the increased frequency and intensity of the severe weather events themselves. For example, in 2024, the total economic losses from natural disasters hit an estimated 318 billion dollars. The insured portion was 137 billion. The rest was uninsured.
+
+Speaker 1: So more than half of all the losses were not covered by insurance. What does the report say about that?
+
+Speaker 2: It refers to this as the protection gap, and this gap is widening. In 2024, 57 percent of all global economic losses from these catastrophes were uninsured. This is a huge problem, especially in developing countries where very few people have insurance. For these communities, a single disaster can wipe out years of economic development and trap them in a cycle of poverty and recovery.
+
+Speaker 4: And this isn't just an abstract global statistic. I mean, we see it in our own communities. We hear stories of insurance premiums skyrocketing to the point where they are unaffordable. Or worse, insurance companies simply pulling out of entire states like Florida or California because the risk of wildfire or flooding has become too high. This creates this incredible financial stress for families who are just trying to protect their homes.
+
+Speaker 1: And it's not just private homes and property. Our shared public infrastructure is also facing enormous risks.
+
+Speaker 2: That's right. Our entire modern society, the energy grids, transportation networks, water treatment plants, they were all designed and built for a climate that no longer exists. 
+
+Speaker 2: Sea level rise directly threatens ports and coastal cities, extreme heat puts an incredible strain on power grids, and intense flooding can destroy roads and bridges. The World Bank has warned that the cost of inaction, particularly in terms of damage to infrastructure, could run into the trillions of dollars.
+
+Speaker 3: Trillions in damage. But fixing it would also cost trillions. I mean, upgrading a nation's entire power grid or rebuilding its coastal defenses requires a colossal upfront investment. Where is that money supposed to come from, especially for countries that are already struggling?
+
+Speaker 2: It's a major challenge, but the analysis shows that inaction is far more expensive. The World Bank estimates that for every one dollar invested in making infrastructure more climate-resilient now, we could see a benefit of four dollars in avoided damages and disruptions down the road. It‚Äôs a classic case of an ounce of prevention being worth a pound of cure.
+
+Speaker 1: When homes are destroyed, infrastructure fails, and livelihoods are lost, people are inevitably forced to move. The report identifies climate change as a powerful driver of human displacement.
+
+Speaker 2: Yes, it acts as a threat multiplier. The number of forcibly displaced people worldwide has nearly doubled in the last ten years, reaching an estimated 123.2 million by the end of 2024. 
+
+Speaker 2: And while conflict is still a primary driver, the IPCC states with high confidence that climate and weather extremes are increasingly forcing people from their homes on every single continent. In fact, 2024 saw the highest number of new displacements from extreme weather in 16 years.
+
+Speaker 3: I understand the numbers, but I think it's tricky to label someone a climate refugee. People move for all sorts of reasons, for better jobs, to escape poverty, for family. How can you really untangle all those factors and say with certainty that someone was displaced specifically by climate change?
+
+Speaker 2: You've hit on the core of the issue. It's rarely a single cause, which is why the term threat multiplier is so accurate. A drought, for example, can kill crops, which leads to economic collapse, which can then lead to resource conflicts, and all of those factors together push people to move. 
+
+Speaker 2: Climate change is the spark that ignites these other pre-existing vulnerabilities. And the report highlights a chilling statistic on this point: between 2010 and 2020, the death rate from floods, droughts, and storms was 15 times higher in highly vulnerable regions compared to the most secure ones.
+
+Speaker 4: And it's not just people who are being displaced and harmed. It's... it's everything else. The entire web of life that supports us.
+
+Speaker 1: That‚Äôs a vital point, Maya. The report draws a direct line between the climate crisis and the broader biodiversity crisis that's happening all around us. Carter?
+
+Speaker 2: Yes, the two are deeply intertwined. Climate change is a primary driver of what many scientists now refer to as the Earth's sixth mass extinction. A landmark global assessment from the IPBES warned that an estimated one million animal and plant species are now threatened with extinction, many within decades. 
+
+Speaker 2: While land use change is currently the biggest driver, climate change is projected to become as, or even more, important in the coming decades.
+
+Speaker 1: Can you give us a concrete example of this happening right now?
+
+Speaker 2: The most potent symbol is the fate of the world's coral reefs. The last decade has been catastrophic for them. The Great Barrier Reef, for instance, has suffered six mass coral bleaching events just since 2015. 
+
+Speaker 2: These are caused by prolonged marine heatwaves that literally cook the coral, causing them to expel their symbiotic algae and turn white. The increasing frequency of these heatwaves leaves no time for the reefs to recover.
+
+Speaker 4: It‚Äôs so hard to hear that. Losing the coral reefs‚Ä¶ it's like imagining a world without the Amazon rainforest. It's a loss so profound you can't even begin to calculate the cost. A world that's just‚Ä¶ less alive.
+
+Speaker 2: And the science is very clear on this. Scientists warn that if global warming exceeds the 1.5 degree target, over 90 percent of the world's tropical coral reefs could be lost by the middle of this century. It's a devastating blow to marine biodiversity and to the millions of people who depend on those reefs for their food and their livelihoods.
+
+Speaker 1: That is an incredibly sobering thought, Maya. A world that is simply less alive. We've spent this time detailing an accelerating crisis with devastating impacts on our health, our economy, and the very biodiversity of the planet. It‚Äôs a stark picture. But the world has not been completely idle. The final section of the report assesses the global response. 
+
+Speaker 1: Carter, the central pillar of international climate policy over the past decade has been the Paris Agreement, adopted back in 2015. For listeners who may not remember the details, can you remind us what it set out to achieve?
+
+Speaker 2: Of course. The Paris Agreement was a genuine diplomatic breakthrough. For the first time, it brought all nations, both developed and developing, into a common framework to combat climate change. Its main goals are to hold the increase in the global average temperature to well below 2 degrees Celsius above pre-industrial levels, and to pursue efforts to limit that temperature increase even further to 1.5 degrees Celsius.
+
+Speaker 1: And how was it designed to achieve that? What's the actual mechanism?
+
+Speaker 2: The agreement operates on a five-year cycle of what's called ratcheting ambition. The idea is that countries are required to submit their own national climate action plans, which are known as Nationally Determined Contributions, or NDCs. Then, every five years, they are supposed to come back to the table with a new, stronger plan that is more ambitious than their last one.
+
+Speaker 3: Okay, hold on. Nationally Determined Contributions. That sounds like a lot of diplomatic jargon. If I'm hearing you right, does that just mean that every country gets to make up its own plan, and there's no real penalty or enforcement if they don't follow it or if their plan is too weak?
+
+Speaker 2: You're not wrong, Frank. It is not an international treaty with a heavy-handed enforcement mechanism in the traditional sense. It's a framework that is built more on transparency, reporting, and a kind of global peer pressure. The idea is that by having everyone's commitments out in the open, and by regularly taking stock of our collective progress, countries will be encouraged and expected to ramp up their efforts over time.
+
+Speaker 4: So it‚Äôs less of a strict global law and more of a collective promise. A set of promises, really. But based on everything we've talked about today, from the shattered temperature records to the accelerating ice melt, it seems like those promises are being broken.
+
+Speaker 1: Maya, that takes us directly to what the report calls the ambition gap. Carter, you explained the process. Now let's talk about the reality. How big is the shortfall between what countries have promised in their NDCs and what the science tells us we actually need to do?
+
+Speaker 2: The shortfall is massive. It's a chasm, really. The most recent analysis from the United Nations, which looked at the latest pledges from 195 countries, concluded that we are falling miles short of what's needed. If every country fully implemented its current pledges, we would see a global emission reduction of only about 5.9 percent by 2030 compared to 2019 levels.
+
+Speaker 4: Only six percent? That sounds tiny. How does that compare to the goal?
+
+Speaker 2: Well, the IPCC, the main scientific body, has found that to keep the 1.5 degree limit within reach, our emissions need to be slashed by at least 43 percent by 2030. So we are pledging for a six percent cut when we need a 43 percent cut. 
+
+Speaker 2: This gap means that the sum of all these national promises currently has the world on a trajectory toward a catastrophic level of warming somewhere between 2.5 and 2.9 degrees Celsius.
+
+Speaker 3: That's just astounding. It's not a gap, it‚Äôs a total disconnect from reality. So these huge annual conferences, the COPs we hear about on the news every year with all the world leaders, what are they actually achieving if the numbers are still this bad? Is it just a talking shop?
+
+Speaker 2: That's a criticism you hear a lot, and there is a great deal of frustration. These conferences are the primary venue for negotiating how to implement the Paris Agreement. They have produced some important outcomes. For instance, COP28 in Dubai produced the first ever global stocktake, which is essentially the world's climate report card. And it ended with a historic, first-ever call for countries to begin transitioning away from fossil fuels.
+
+Speaker 4: But Carter, the language there seems so important. I remember the debate was about a phase-out of fossil fuels, but the final agreement was to transition away from them. It feels like very carefully chosen, watered-down language. Does that kind of subtle change in wording actually lead to real-world action, or does it just give countries a loophole?
+
+Speaker 2: That is the heart of the debate. Many nations were deeply disappointed that the language wasn't stronger. The hope is that even that language signals a clear direction to the global economy. That same conference also established a global goal to triple renewable energy capacity and double the rate of energy efficiency improvements by 2030, which are very concrete targets.
+
+Speaker 1: And what about the most recent conference mentioned in the report, COP29?
+
+Speaker 2: That was dubbed the Finance COP. Its main job was to agree on a new climate finance goal to help developing nations. After very contentious negotiations, they agreed that developed countries should lead in mobilizing at least 300 billion dollars per year by 2035 for developing nations. But again, many of those nations expressed deep disappointment, stating that this number falls far, far short of their estimated needs, which are in the trillions.
+
+Speaker 1: This seems to be a recurring theme of falling short. Let's shift from the policy to the other major part of the response, which is technology. Here, the report does seem to highlight one area as a significant success story. And that is the renewables revolution.
+
+Speaker 2: Yes, this has been the brightest spot of the last decade without a doubt. We've seen an absolutely explosive growth of renewable energy technologies, especially solar panels and wind power. This was driven by incredible innovation and economies of scale, and it caused the costs of solar and wind to plummet. 
+
+Speaker 2: They are now the cheapest sources of new electricity generation in most of the world. To give you a sense of the scale, in 2023, the world added a record 473 gigawatts of new renewable capacity. The International Energy Agency even forecasts that this year, in 2025, renewables will overtake coal as the single largest source of global electricity.
+
+Speaker 3: That‚Äôs genuinely good news, and everyone loves seeing cheaper energy. But I noticed the report also says that we are still not on track to meet that COP28 goal of tripling renewable capacity by 2030. 
+
+Speaker 3: Why is that? If this technology is so cheap and effective, why aren't we just building it everywhere, all the time, as fast as we possibly can? What's the hold-up?
+
+Speaker 2: It's a great question, Frank. The momentum is incredible, but the scale of the challenge is even bigger. To achieve that tripling goal, we would need to be adding, on average, around 1,050 gigawatts of new capacity every single year for the rest of the decade. 
+
+Speaker 2: That's more than double the record we just set in 2023. The barriers are no longer primarily about cost; they are about things like modernizing our electrical grids to handle this new type of energy, overcoming supply chain bottlenecks for components, and streamlining the permitting processes to get projects built faster. So even in this huge success story, there is a major gap between our current progress and the required pace of change.
+
+Speaker 1: So, Carter, even our biggest technological success story, renewable energy, is facing a challenge of sheer scale and speed. The report points to another critical tool in the toolbox, something often called the first fuel, which is energy efficiency.
+
+Speaker 3: Now this is something that just seems like pure common sense to me. Using less energy to get the same result, whether it's an efficient appliance or an insulated home. It saves people money on their bills, it reduces strain on the power grid, and it cuts emissions. It seems like the absolute lowest-hanging fruit. Why aren't we talking about this constantly?
+
+Speaker 2: You are absolutely right, Frank. Improving energy efficiency is the cheapest and cleanest way to address our energy needs, which is why the COP28 goal to double the global average annual rate of energy efficiency improvements by 2030 is so critical. But the reality, as the report lays out, has been deeply disappointing.
+
+Speaker 1: How so? What does the data show?
+
+Speaker 2: After a brief speed-up in 2022, which was mostly in response to the global energy crisis, the rate of global energy intensity improvement slowed way down to just one percent in both 2023 and 2024. To be on a pathway to net-zero emissions, we need that rate to be averaging around four percent per year. So we are falling far short. The report effectively calls it a major and concerning policy failure on a global scale.
+
+Speaker 1: So if we're failing on the common-sense goal of efficiency, what about the more high-tech solutions that promise to clean up our existing emissions? Carter, the report spends some time on Carbon Capture, Utilisation, and Storage, or CCUS.
+
+Speaker 3: Again, on the surface, this sounds like a pragmatic solution. For those really difficult industries that are hard to electrify, like making cement or steel, why not just build a system to capture the carbon dioxide before it ever gets into the atmosphere? It seems like a logical way to solve the problem without having to completely shut down these essential industries overnight.
+
+Speaker 2: And that is exactly how it is often presented, Frank, as a necessary solution for these hard-to-abate sectors. And there is a lot of momentum in terms of announcements. The report notes there are over 700 projects in various stages of development. However, it also points to a massive gap between those announcements and the operational reality.
+
+Speaker 4: What do you mean by that? A gap between announcements and reality?
+
+Speaker 2: As of early 2024, the total global operational capacity for capturing CO2 was just over 50 million tonnes per year. That is a tiny fraction of what has been announced or proposed for 2030. And critically, only 20 percent of that announced capacity had actually reached a final investment decision. 
+
+Speaker 2: This indicates that most of these projects are still just on the drawing board, they are not yet real. So deployment has consistently and significantly lagged behind the expectations and the promises.
+
+Speaker 4: You know, I have to wonder if there's a risk here that this technology just becomes an excuse. A way for fossil fuel companies and heavy industries to continue polluting under the promise that someday, in the future, they'll be able to clean it all up. It feels like it could be a dangerous distraction from the real work of actually cutting emissions at the source.
+
+Speaker 1: Speaking of potentially dangerous and controversial ideas, the report mentions that as the world falls further behind on emissions reductions, there is a growing, albeit highly contentious, interest in something called solar geoengineering. Carter, can you even begin to explain what that is?
+
+Speaker 2: I can try. It's also sometimes called solar radiation modification. This refers to a set of hypothetical technologies that are designed to cool the planet by reflecting a small fraction of incoming sunlight back out to space. The most commonly discussed method is called stratospheric aerosol injection, which would involve spraying reflective particles, like sulfur dioxide, into the upper atmosphere to mimic the cooling effect of a large volcanic eruption.
+
+Speaker 4: That sounds absolutely terrifying. I mean, the idea of us deliberately conducting a planetary-scale experiment with our only atmosphere, when we can't possibly predict all the consequences‚Ä¶ it just feels like the height of human arrogance. We've already made one huge mess by pumping carbon dioxide into the air; this sounds like a way to make another, potentially even worse, mess.
+
+Speaker 2: Your reaction, Maya, captures the essence of the controversy. The scientific community is extremely cautious. The report emphasizes that geoengineering is not a substitute for cutting emissions. It does not address the root cause of the problem, which is the greenhouse gas blanket, and it carries immense and poorly understood risks. 
+
+Speaker 2: It could potentially disrupt regional weather patterns, harm the ozone layer, and it creates a moral hazard by possibly reducing the incentive for us to do the hard work of decarbonizing our economies.
+
+Speaker 1: So it's seen as a last-ditch, break-glass-in-case-of-emergency option with huge potential side effects. Maya, your point about the arrogance of these high-tech ideas is well taken. And while we're discussing these futuristic and risky technologies, the report highlights a profound failure in a much more basic and immediate area: finance and justice for the people already suffering the consequences. Carter, can you explain what the report calls the adaptation finance gap?
+
+Speaker 2: This is one of the most sobering findings in the entire report. While much of the focus is on mitigation, which is cutting emissions, adaptation, which is preparing for the impacts of climate change, is equally critical, especially for the world's most vulnerable nations. The UNEP Adaptation Gap Report revealed a staggering shortfall in funding.
+
+Speaker 1: How big is the shortfall?
+
+Speaker 2: The report estimates that the annual adaptation finance needs of developing countries are somewhere between 215 billion and 387 billion dollars. In stark contrast, the total international public finance that flowed to these countries for adaptation in 2021 was just 21 billion dollars, which was actually a 15 percent decline from the year before. 
+
+Speaker 2: This means the actual needs are 10 to 18 times greater than the funds that are actually being provided, leaving the most vulnerable communities dangerously exposed and underprepared.
+
+Speaker 3: I understand the need is great, but why is this framed as a justice issue? Isn't every country ultimately responsible for protecting its own citizens and adapting to its own challenges?
+
+Speaker 2: That question gets to the very core of the UN climate negotiations. The entire process is built upon a foundational principle known as common but differentiated responsibilities and respective capabilities. It's a bit of a mouthful, but the concept is straightforward. 
+
+Speaker 2: It acknowledges that while all nations share a common responsibility to protect the global climate, the developed countries, which have been industrializing for over a century, bear a much greater historical responsibility for causing the problem in the first place. They also possess far greater financial and technological capabilities to address it.
+
+Speaker 4: So it‚Äôs the idea that the polluter should pay. The ones who created the mess have a greater obligation to help clean it up, and to help protect those who are most harmed by it.
+
+Speaker 2: Exactly. Climate justice frameworks articulate this through the concept of a double inequality. The very people and nations who have contributed the least to the emissions that cause climate change are the ones who are suffering the earliest and most severe consequences. 
+
+Speaker 2: Therefore, a just global response requires that the developed nations lead the way in making the deepest emissions cuts, and that they provide substantial financial and technological support to help developing nations adapt to the impacts they did little to cause.
+
+Speaker 1: Carter, you were just explaining this core principle of climate justice, that the nations with the greatest historical responsibility for emissions also have the greatest capacity to help solve the problem.
+
+Speaker 2: Yes, and it builds on what Maya was saying. It‚Äôs about recognizing the profound unfairness, the, uh, double inequality that lies at the heart of the climate crisis. The people who are most harmed are the ones who did the least to cause the problem. Think about it, uh, a farmer in the Sahel whose land is turning to desert, or a family in a low-lying island nation whose home is threatened by sea level rise‚Ä¶ their contribution to historical emissions is practically zero.
+
+Speaker 4: So what you're saying is, that farmer, whose crops are failing from a drought they had no part in creating, is right now paying a much, much higher price than someone in a wealthy country who has, you know, benefited from a century of industrial development powered by fossil fuels.
+
+Speaker 2: That is the injustice in a nutshell. And so, the framework for a just response is built on that understanding. It means developed nations have a moral and ethical obligation to lead with deep, rapid emissions cuts. And, crucially, it means they have an obligation to provide significant financial and technological support to help developing nations build clean economies and adapt to the impacts they are already facing.
+
+Speaker 3: I understand the moral argument. I do. But from a purely practical standpoint, it seems incredibly complicated. I mean, how far back do you go to assign this historical responsibility? Are you trying to calculate the emissions of the United Kingdom from the 1880s? It feels like an impossibly complex way to assign blame.
+
+Speaker 2: That's a fair point, Frank, and you know, it‚Äôs less about calculating precise historical blame and more about acknowledging the reality of the present day. The framework is not about punishing past generations. It's about recognizing which nations today have the accumulated wealth, the technology, and the stable institutions‚Äîmany of which were built on that history of fossil-fueled development‚Äîto lead the global response. It‚Äôs about capability and responsibility in the here and now.
+
+Speaker 1: This whole conversation about justice, responsibility, and the immense shortfall in support really underscores the urgency of the crisis. And perhaps nothing in this entire report highlights that urgency more than the growing scientific understanding of a concept known as climate tipping points. Carter, for our listeners, what exactly is a tipping point?
+
+Speaker 2: It is probably the most sobering concept in all of climate science. The IPCC defines a tipping point as a critical threshold in the Earth's system. Once that threshold is crossed, a part of the system could trigger an abrupt, cascading, and potentially irreversible change.
+
+Speaker 1: Abrupt and irreversible. Those are two very powerful words. What does irreversible mean in this context?
+
+Speaker 2: It means that even if we managed to cool the planet back down later, the system might not flip back. The change could be locked in for centuries, or even millennia. We could pass a point of no return.
+
+Speaker 4: That is‚Ä¶ a terrifying thought. So what are these systems? What parts of the planet are we talking about?
+
+Speaker 2: Scientists have identified several large-scale components of the Earth system that may have these tipping points. The most commonly discussed are the great ice sheets. We‚Äôre talking about the irreversible collapse of the Greenland and the West Antarctic ice sheets.
+
+Speaker 1: And what would be the consequence of something like that?
+
+Speaker 2: Well, uh, together, those two ice sheets hold enough frozen water to raise the global mean sea level by over 10 meters. That's about 33 feet.
+
+Speaker 4: Ten meters‚Ä¶ I‚Ä¶ I can‚Äôt even comprehend that. That's not just flooding. That is wiping entire cities, entire island nations, completely off the map for good.
+
+Speaker 2: Yes, the consequences would be civilization-altering. And another major tipping element is in the oceans themselves. A major slowdown or even a shutdown of the Atlantic Meridional Overturning Circulation, often called the AMOC.
+
+Speaker 3: The AMOC. I've heard of that, but it sounds like something out of a disaster movie. What does this current actually do for us?
+
+Speaker 2: It's a massive system of ocean currents that acts like a conveyor belt, transporting warm water from the tropics up to the North Atlantic. It plays a huge role in regulating weather patterns, especially in the Northern Hemisphere. 
+
+Speaker 2: A collapse of this system would drastically alter weather across North America and Europe, causing, you know, extreme cooling in some places, changing rainfall patterns, and disrupting monsoons that billions of people depend on for their food.
+
+Speaker 1: So we have the ice and the oceans. What else?
+
+Speaker 2: Then we have the biosphere systems. There are two major ones scientists are deeply concerned about. The first is the potential dieback of the Amazon rainforest.
+
+Speaker 1: So the Amazon could go from being this vital carbon sink that helps us, to becoming a major carbon source that actually hurts us?
+
+Speaker 2: Precisely. Large parts of the forest could transition into a drier, savanna-like ecosystem. And in doing so, it would release the vast quantities of carbon stored in its trees and soil, which would create a powerful feedback loop that accelerates even more global warming.
+
+Speaker 4: And the other one? You hear people talk about a ticking carbon bomb in the arctic. Is that what you mean?
+
+Speaker 2: That's the one. The abrupt, widespread thawing of permafrost. This is the permanently frozen ground in the arctic regions, and it contains enormous amounts of organic carbon that has been locked away for thousands of years. As it thaws, microbes decompose that organic matter and release it into the atmosphere as carbon dioxide and, even more potently, methane. This is another one of those dangerous feedback loops.
+
+Speaker 1: So Carter, we have these massive, continent-scale systems that could fundamentally break. I think for a long time, many of us thought of these tipping points as very distant risks. You know, things that might happen if warming got really, really bad, say, at five or six degrees Celsius. What does the latest science in the report say about that?
+
+Speaker 2: This, Alice, is perhaps the single most concerning finding to emerge in the last few years of research. The scientific consensus has shifted. Those early estimates that suggested these were high-warming risks have been revised. The latest research, which is cited in the IPCC reports, indicates that the temperature thresholds for triggering some of these tipping points may be much, much lower than we previously thought.
+
+Speaker 3: How much lower are we talking about?
+
+Speaker 2: The latest studies indicate that several of these major tipping points, including the collapse of the Greenland and West Antarctic ice sheets, the shutdown of the AMOC, and widespread permafrost thaw, could potentially be triggered at warming levels between 1.5 and 2.0 degrees Celsius.
+
+Speaker 4: But wait a minute. Carter, you said at the very, very beginning of our conversation that the world already temporarily breached 1.5 degrees of warming in 2024. If the trigger point is 1.5 degrees, what does that mean for us right now?
+
+Speaker 2: It means‚Ä¶ well, it means that the risk is no longer a distant, abstract threat for future generations. It places the possibility of crossing these irreversible thresholds squarely within the realm of possibility this century. It moves the conversation from the future into the immediate present. 
+
+Speaker 2: And, you know, it adds a profound, almost existential urgency to the need for immediate, deep, and drastic emissions reductions. The window of opportunity to steer away from these points is closing, and it is closing very, very rapidly.
+
+Speaker 1: That is a deeply unsettling reality to confront, Carter. And Maya, I see you reacting to that. When you hear that the 1.5 degree line, which we‚Äôve talked about for so long as this future guardrail, is not only something we've touched but is also the potential trigger for these irreversible changes‚Ä¶ what does that feel like?
+
+Speaker 4: You know, it‚Ä¶ it almost takes your breath away. It feels like we've been driving towards a cliff in the fog, arguing about how fast we should be going. And Carter is saying the fog has just cleared, and we're right at the edge. We‚Äôre there. That's a very, very hard thing to fully process.
+
+Speaker 3: It is. And it brings up a really difficult, practical question for me. If we're already on the verge of crossing these irreversible thresholds, what is the point of all this? I mean, does a 43 percent emissions cut by 2030, which already seems impossible, even matter anymore if the fuse has already been lit on something like the Greenland ice sheet? Have we‚Ä¶ have we already lost the game?
+
+Speaker 2: Frank, that is the most important question anyone can ask right now. And the conclusion of the report, uh, argues that this is precisely why our actions now matter more than they ever have before. The first major conclusion is that the defining characteristic of the last decade is non-linear acceleration.
+
+Speaker 1: Okay, non-linear acceleration. Break that down for us.
+
+Speaker 2: Think of it like a car that's rolling down a hill. But the hill isn't a steady slope; it's a curve that gets steeper and steeper as you go. So for every foot you travel, your speed increases more than it did in the previous foot. You are accelerating exponentially, not in a straight line, not arithmetically. That‚Äôs what‚Äôs happening to our planetary systems. The risks are growing at an accelerating rate.
+
+Speaker 1: So every fraction of a degree of warming we can prevent now, every year we can act faster, has a much bigger impact in preventing that future acceleration than it would have twenty or thirty years ago.
+
+Speaker 2: Exactly. It‚Äôs what scientists call positive feedback loops becoming more potent. So, to answer Frank‚Äôs question, it‚Äôs the absolute opposite of the game being lost. It means the stakes of our actions in the next five to ten years are higher than they have ever been in human history. Every ton of carbon we keep out of the atmosphere now pays huge dividends in slowing down that terrifying acceleration toward those tipping points.
+
+Speaker 1: And the report also concludes that these are not isolated problems, correct? It talks about a cascade of interconnected crises.
+
+Speaker 2: Yes, that's the second key takeaway. We can no longer think of climate impacts as a series of separate events. A drought is not just a lack of water. It is a trigger. It triggers failures in the food system when crops fail. It triggers failures in the economic system when farmers lose their livelihoods. 
+
+Speaker 2: It triggers, you know, public health crises from malnutrition and water-borne diseases. It can even culminate in social instability and displacement. Climate change is a threat multiplier that makes all our existing vulnerabilities worse.
+
+Speaker 4: You can really see that in real life, can‚Äôt you? I mean, a wildfire isn't just a fire anymore. It becomes a public health crisis for millions of people breathing in the smoke. It's an economic crisis for the entire region. It becomes a water crisis months later when the first heavy rains wash toxic ash and debris into the reservoirs. You realize that one event pulls on all the other threads that hold our society together. Everything is connected.
+
+Speaker 2: That‚Äôs a perfect way to put it, Maya. And because everything is connected, the report concludes that our response has to be holistic. We can‚Äôt have siloed policies that address energy, or agriculture, or public health in isolation. They are all part of the same interconnected challenge.
+
+Speaker 1: This brings us to the third, and perhaps the toughest, conclusion from the report. Which is that our global response, as it stands today, is being dangerously outpaced by the physical reality of climate change.
+
+Speaker 2: That's the hard truth of the last decade. Despite all the meetings and the progress on renewables, the response remains critically insufficient. The report concludes that this failure is defined by three persistent and widening gaps. First is the ambition gap we already discussed, the gap between the weak climate pledges from countries and what science clearly shows is necessary.
+
+Speaker 1: And the second?
+
+Speaker 2: The second is the adaptation finance gap, which we just covered. The massive shortfall in funding that leaves the world‚Äôs most vulnerable populations essentially undefended against the coming storms and droughts. And the third is the justice gap, which undermines the trust and cooperation that are absolutely essential for any kind of effective global solution.
+
+Speaker 3: So if I'm hearing this correctly, the report‚Äôs ultimate conclusion is that our primary problem is no longer a technological one. We have the solar panels, we have the wind turbines, we have the efficiency solutions. The report is saying that the biggest barriers now are political, financial, and social. It's about a lack of political will, a failure to mobilize the necessary funds, and a failure to address the core injustices of the crisis.
+
+Speaker 2: That is the absolute crux of the conclusion. Technology is a vital tool, an essential tool, but it is not a silver bullet. The primary obstacles are now in our halls of government, in our financial institutions, and, uh, in our collective willingness to face this reality and act at the scale it requires.
+
+Speaker 1: So after this incredibly detailed and, frankly, alarming look back at the last decade, where does this leave us? We have a planet in a state of acceleration. We've temporarily breached the 1.5 degree threshold. And the risk of irreversible tipping points is no longer a future problem, but a present-day danger. Maya, I want to start with you. What‚Äôs your final takeaway?
+
+Speaker 4: It leaves me feeling that the time for simply being worried, or for abstract hope, is over. The only appropriate response to this level of evidence is determined action. This report is a story written in data, and it's telling us we have to transform this stark awareness into real, tangible work in our communities and demand it from our leaders. There‚Äôs no time for anything else.
+
+Speaker 1: Frank?
+
+Speaker 3: It leaves me thinking that we need to have a much more honest and pragmatic conversation about the real-world costs and trade-offs. We‚Äôve talked about technology and policy, but this report makes it clear that the real fight is over politics and economics. And until we tackle that head-on, with honesty, we'll keep falling short.
+
+Speaker 1: And Carter, a final thought from you.
+
+Speaker 2: The science has been clear for a long time, but the evidence from this past decade is definitive. You know, this period from 2015 to 2025 will be remembered as the decade the consequences of our inaction became undeniable. That temporary breach of 1.5 degrees served as a final, unambiguous warning. The scientific challenge now is to monitor these accelerating changes. But the human challenge is to finally close those gaps between promises and performance, before those tipping points are crossed for good.
+
+Speaker 1: Carter, that is a powerful and frankly stark place to end, on the precipice of these tipping points with the clock running out. But... you know, before we wrap up completely, I want to hold on that last thought. The human challenge. I feel we can't end just with the warning. I want to pivot from the problems we've detailed so thoroughly to the specific pathways forward that are emerging. Beyond the high-level policy failures, where are the new fronts in this challenge?
+
+Speaker 2: That's a crucial pivot to make, Alice. Because, uh, despair is paralyzing. And despite the failures, there are new strategies and, you know, new arenas of action that are gaining momentum.
+
+Speaker 1: Let's talk about one of those. We've mentioned the justice gap and the economic challenges. What about the people, the workers and communities, whose entire livelihoods are tied to the fossil fuel industries we need to transition away from?
+
+Speaker 2: You're talking about the concept of a Just Transition. And you know, this has become a central part of the conversation because it's both morally right and politically essential. A Just Transition means ensuring that the shift to a green economy is fair and inclusive. It means we don't leave coal miners, oil rig workers, and entire communities that depend on these industries behind.
+
+Speaker 3: This is something I think is critical. You can't just tell millions of people that their jobs, their skills, their histories are obsolete without a concrete plan. You know, you'd have massive social and political unrest. For people to buy into this massive economic shift, they have to see a future for themselves in it. A real plan for retraining, for new jobs in clean energy manufacturing or grid modernization, that is absolutely essential.
+
+Speaker 4: And it's more than just jobs, isn't it? It's about identity and community. For generations, some towns have been defined by the local power plant or the mine. A just transition means investing in those places, helping them to diversify and build a new economic foundation that honors their heritage but, you know, allows them to thrive in a different kind of future. It's about respecting people while we make these big changes.
+
+Speaker 1: So ensuring the transition is fair is one emerging pathway. Maya, you just mentioned respecting people and their heritage. What about respecting nature itself? The report touched on biodiversity. Are we starting to see a move towards working with nature to solve this?
+
+Speaker 4: I hope so. Because for so long it feels like we've been trying to invent some new machine to fix the problems our last machine created. It just seems so obvious that we should be looking to nature, which has been regulating the climate for millions of years, for solutions.
+
+Speaker 2: And that intuition is now a major field of action called Nature-Based Solutions. The idea is to use the power of healthy ecosystems to help us. And, you know, the benefits are often twofold. For example, restoring coastal mangrove forests. Mangroves are incredible at absorbing carbon, but they also act as a natural sea wall, protecting coastal communities from storm surges far more effectively and cheaply than a concrete barrier.
+
+Speaker 1: So it helps with both mitigation, by absorbing carbon, and adaptation, by providing protection.
+
+Speaker 2: Exactly. And there are many other examples. Reforestation and afforestation, uh, planting trees, to draw down carbon from the atmosphere. Regenerative agriculture, which involves farming practices that restore the health of the soil, turning it back into a powerful carbon sink. These solutions don't just fight climate change; they also restore biodiversity, they clean our water, and they can make our food systems more resilient.
+
+Speaker 1: So much of the report focused on the failures of national governments to act. But we know a lot of the real-world changes happen at a more local level. What about the role of cities and even large corporations? Are they stepping up to fill the leadership vacuum?
+
+Speaker 2: In many cases, yes. Cities are often more agile and pragmatic than national governments. Networks like the C40 Cities Climate Leadership Group are hubs of innovation. You know, cities are where you see real progress on electrifying public transport, creating greener buildings, and improving waste management, all of which have a huge impact on emissions.
+
+Speaker 3: That makes sense. But what about the private sector? We hear every major company in the world announcing some kind of a net-zero by 2050 target. How much of that is real, tangible action, and how much of it is just good public relations? You know, just greenwashing? Is anyone actually holding them accountable for these promises?
+
+Speaker 2: That is the billion-dollar question, Frank. And you're right to be skeptical. The last few years have seen a surge in these pledges, but there's also been a surge in scrutiny. There is a huge push now to move companies beyond vague promises towards transparent, science-based targets for the near term. We're seeing a real divide emerge between the companies that are genuinely transforming their business models and those that are, uh, frankly, just trying to improve their image. Accountability is still a massive work in progress.
+
+Speaker 1: So if governments are slow and corporations can't always be trusted, what other avenues for accountability are emerging? Where else are people pushing for change?
+
+Speaker 2: One of the most dynamic and, you know, potentially powerful new fronts is in the courtroom. We are seeing a huge increase in what is called climate litigation.
+
+Speaker 4: So, people are actually suing governments and companies over climate change?
+
+Speaker 2: Yes, all over the world. Citizens, activist groups, cities, and even states are taking national governments to court to force them to adopt stronger climate policies, arguing that inaction violates their fundamental human rights to a healthy environment. 
+
+Speaker 2: And, connecting back to our earlier conversation, they are also suing the major fossil fuel companies. They are using that attribution science we discussed to directly link the emissions from a company's products to the specific harms and financial damages their communities have suffered from floods, wildfires, and sea level rise. It's a new and rapidly evolving way to demand accountability.
+
+Speaker 1: And Carter, that‚Äôs a fascinating development. The idea that a courtroom could become a key battleground for climate action. Frank, you look skeptical.
+
+Speaker 3: Well, I am. I mean, it sounds good in a headline, "Activists Sue Oil Giant." But do these lawsuits actually work? It seems like they would get tied up in court for decades, with armies of corporate lawyers. Can a lawsuit really change the course of a multi-trillion-dollar global industry?
+
+Speaker 2: It's a valid skepticism, Frank. And you're right, it's not a quick fix. But, uh, the impact isn't just about winning a single huge payout. These cases create enormous pressure. They force companies to disclose internal documents, they generate negative publicity, and, you know, they establish a legal record of responsibility. It fundamentally changes the risk calculation for these industries and their investors.
+
+Speaker 4: And it changes the story, doesn't it? It reframes this from being a sort of blameless, collective problem to one of specific, attributable harm. It says, you knew about the damage your product would cause, and you did it anyway. That's a powerful narrative.
+
+Speaker 1: So beyond the courtroom, what other economic tools are being discussed to drive this transition? The report mentions things like carbon pricing. Carter, what does that actually mean?
+
+Speaker 2: Carbon pricing is a very direct economic strategy. It's about putting a price on carbon pollution to discourage its use. There are two main ways to do it. You can have a straightforward carbon tax, where the government sets a price per ton of carbon emitted. Or you can have a cap-and-trade system, where the government sets a limit, a cap, on total emissions, and then allows companies to buy and sell permits to emit.
+
+Speaker 3: Okay, but let's be honest about what that means. A carbon tax just gets passed on to the consumer, right? It means higher gas prices, higher heating bills. It seems like a policy that would disproportionately hurt lower-income families and working people, while the big corporations just factor it into the cost of doing business.
+
+Speaker 2: That is the single biggest and most important concern with carbon pricing, Frank. And if it's designed poorly, that's exactly what can happen. However, a well-designed system can actually be equitable. For example, some proposals are for a carbon fee and dividend system.
+
+Speaker 1: A dividend? So you get money back?
+
+Speaker 2: Exactly. The revenue collected from the carbon tax isn't just kept by the government. It's returned directly to citizens on an equal, per-person basis. In that system, most lower and middle-income families would actually come out ahead. They would get more back in the dividend than they pay in higher energy costs, because wealthier people tend to have a much larger carbon footprint.
+
+Speaker 4: You know, it's also about what costs we're already paying. We don't see a line item on our bills for it, but we are all paying the price for pollution right now. We pay it in healthcare costs from asthma and other respiratory diseases linked to burning fossil fuels. 
+
+Speaker 4: We pay it in disaster recovery funds when our taxes go to rebuilding a town after a flood. A carbon price isn't creating a new cost; it's just making a hidden cost visible.
+
+Speaker 1: This brings us to a question I think is on everyone's mind. We've talked about these huge, complex systems, from international law to national energy policy. It can all feel very distant. So what about us? What about individual action versus systemic change? Maya, does it really make a difference if I diligently sort my recycling or eat less meat when the scale of the problem is this vast?
+
+Speaker 4: That is the question, isn't it? And it's so easy to feel like your small actions are just a drop in an angry ocean. But I truly believe they matter, just maybe not in the way we think. You know, the direct impact of me not using a plastic straw isn't going to stop the West Antarctic ice sheet from collapsing. I get that. But that's not the only point.
+
+Speaker 1: So what is the point, from your perspective?
+
+Speaker 4: When we make these conscious choices, we're not just reducing our own tiny footprint. We are sending signals. We are sending a signal to the market that there's demand for sustainable products. We are sending a signal to our friends and neighbors that this is something we care about, which helps to normalize climate consciousness in our culture. 
+
+Speaker 4: And, you know, most importantly, we are sending a signal to politicians that we are a constituency that will support bold climate action. Our individual actions build the social and political momentum for the big systemic changes to happen.
+
+Speaker 2: I think Maya's point is absolutely crucial. The two are not in opposition; they reinforce each other. You need both. Individual action alone is not sufficient, that's clear. We cannot solve this crisis by changing lightbulbs and bringing reusable bags to the grocery store. We absolutely need the large-scale government policies and corporate transformations that will decarbonize our entire energy grid and industrial base.
+
+Speaker 3: Right. Because asking an individual to solve climate change is like asking a soldier to win a war by themselves. It's an unfair burden.
+
+Speaker 2: Exactly. But at the same time, systemic change is not something that just happens in a vacuum. It is the result of millions of people demanding it. So individual action is the necessary foundation. It's the engine of cultural change that makes the politics of systemic change possible. They are two sides of the very same coin. One cannot succeed without the other.
+
+Speaker 1: That‚Äôs a really helpful way to frame it, Carter. So our individual choices create the culture, and that culture creates the political will for systemic change. Let's look forward then. As we chart a course out of this crisis, what are some of the other major technological or social shifts we need to be thinking about? The report's appendix lists a hundred different topics, one of which is the future of food.
+
+Speaker 2: Yes, and this is absolutely critical because, as we discussed, agriculture is a major source of emissions. The future of food really involves a two-pronged approach. First, on the production side, it means scaling up what's often called sustainable or regenerative agriculture. These are farming practices that can reduce emissions, improve soil health so it absorbs more carbon, and use less water.
+
+Speaker 4: And what's the second part? It has to be about what we eat, right?
+
+Speaker 2: It is. It also involves changes in diet, particularly in wealthy nations. The science is quite clear that, uh, a diet lower in red meat consumption and higher in plant-based foods has a significantly smaller environmental footprint. This doesn't mean everyone has to become a vegetarian, but a societal shift in that direction would have a huge impact.
+
+Speaker 3: Now, this is where it gets tricky for me. You start talking about what people eat, and it feels like a massive overreach. People's diets are incredibly personal and cultural. Are we really going to tell people they can't have a burger? That feels like a political non-starter, and it plays right into the hands of those who say climate action is about sacrifice and a lower quality of life.
+
+Speaker 4: I hear that, Frank. I really do. But maybe the framing isn't about sacrifice. Maybe it's about health, and choice, and innovation. You know, the incredible boom in really high-quality, tasty plant-based alternatives is a market-driven solution. It's not about forcing anyone to do anything; it's about providing better options that are good for people and good for the planet. It‚Äôs a cultural shift, not a government mandate.
+
+Speaker 1: So food is one area. What about on the energy side? We've talked a lot about renewables. But there's another powerful, and often controversial, source of carbon-free electricity mentioned in the report: nuclear power. Carter, where does that fit into the picture?
+
+Speaker 2: Well, nuclear power is‚Ä¶ complicated. On the one hand, it is a proven, reliable, 24/7 source of zero-emission electricity. And from a purely climate perspective, many scientists and energy experts argue that it has to be part of the solution, especially for providing a stable baseload of power when the sun isn't shining or the wind isn't blowing.
+
+Speaker 3: It seems like a no-brainer to me. If the goal is to eliminate carbon emissions from electricity as fast as possible, why aren't we building advanced nuclear reactors everywhere? The safety concerns, from what I've read about the newer designs, are vastly different from the older plants people think of.
+
+Speaker 4: But the legacy is still there, isn‚Äôt it? For so many people, the word nuclear brings up images of Chernobyl or Fukushima. And even if the new plants are safer, you still have the problem of nuclear waste. What do we do with this material that remains dangerously radioactive for thousands of years? It feels like we're solving one problem for ourselves by creating a potentially massive one for countless generations to come.
+
+Speaker 2: And that, Maya, is the core of the dilemma. The issues of waste disposal, public perception, high upfront costs, and long construction times have made nuclear a very difficult path to pursue politically, even if the technology itself has advanced. It remains one of the most contentious and unresolved debates in the energy transition.
+
+Speaker 1: This debate over nuclear power really highlights that the energy transition isn't just a scientific or economic challenge. It‚Äôs also a geopolitical one. Carter, how is this massive global shift from fossil fuels to clean energy changing the relationships between countries?
+
+Speaker 2: It's changing everything. For a century, geopolitics has been shaped by who has the oil and gas. But in a world powered by renewables, the map of power changes. It shifts from countries with fossil fuel reserves to countries that lead in manufacturing solar panels, wind turbines, and batteries. It also shifts power to countries that have the critical mineral resources, like lithium, cobalt, and copper, that are essential for these technologies.
+
+Speaker 3: So we're just trading a dependency on oil from the Middle East for a dependency on batteries and minerals from other parts of the world? It sounds like we're just swapping one set of geopolitical problems for another.
+
+Speaker 2: That is a very real risk, and it‚Äôs a major concern. Creating more resilient and diversified supply chains for these technologies is a huge priority. But there's also an upside. The resources for renewable energy, you know, sunlight and wind, are far more democratically distributed around the globe than fossil fuels are. 
+
+Speaker 2: Almost every country has the potential to generate its own clean energy, which could lead to greater energy independence and a more stable world in the long run.
+
+Speaker 1: So after this incredibly comprehensive discussion, from the accelerating science to the cascading impacts and the immense challenges in our global response, I want to bring it back to a final thought from each of you. We're standing at the end of this decade of consequence. The report makes it clear the window is closing. Where do we go from here? Frank?
+
+Speaker 3: For me, it comes down to honesty. I think we need to be more honest about the scale of the challenge and the true costs and trade-offs of the transition. We can't pretend this will be easy or painless. But if we can have a pragmatic conversation that acknowledges the difficulties, I think we have a better chance of bringing everyone along and actually getting it done.
+
+Speaker 1: Maya, a final thought from you.
+
+Speaker 4: I keep coming back to that idea of connection. This report shows how everything is connected‚Äîthe ice melting in the Arctic is connected to the flood in your town, is connected to the food on your plate. And if the problem is one of broken connections, then the solution has to be about rebuilding them. 
+
+Speaker 4: Reconnecting with nature, reconnecting with our communities, and, you know, finding a shared sense of purpose to protect our common home. For me, the way forward has to be rooted in that sense of shared humanity.
+
+Speaker 1: Thank you, Frank and Maya. That's a powerful call for honesty and for rebuilding our connections. Carter, I want to give you the final word on this part of our discussion. After laying out all this evidence, what is the single most important message you think we should take away about the path forward?
+
+Speaker 2: I think, uh, the message is that the era of excuses is over. For decades, you could argue that we didn't fully understand, or that the technology wasn't ready, or that the impacts felt distant. This report from 2015 to 2025 slams the door on all of that. We know, with painful certainty, what is happening. 
+
+Speaker 2: We have the technological solutions, like solar and wind, that are not only ready but are now cheaper than the alternative. And the impacts are no longer distant; they are here, causing billions in damages and immense human suffering every single year.
+
+Speaker 1: So the barriers are no longer technical or scientific.
+
+Speaker 2: Not primarily. The report's inescapable conclusion is that the greatest barrier is a lack of political will, fueled by inertia and, you know, the vested interests of the fossil fuel industry. Overcoming that political barrier is now the central challenge. 
+
+Speaker 2: The road ahead, the road to the next major climate conference, COP30, and beyond, is not about inventing a new machine. It's about building a global consensus for action that is so powerful it becomes politically unstoppable.
+
+Speaker 3: Carter, you say that, building a global consensus. But you know, I look at the world, and our politics seem more fractured and nationalistic than ever. How on earth do we create this unstoppable global movement when major countries can barely agree on basic trade rules, let alone something that requires a complete re-engineering of our entire economy? It feels‚Ä¶ well, it feels naive.
+
+Speaker 2: It's not naive to see the immense difficulty, Frank. It is, uh, perhaps the hardest thing humanity has ever tried to do. But it's not without precedent. We have faced global threats before. You know, scientists in the 1980s discovered that certain chemicals were destroying the ozone layer. The world came together, listened to the science, and passed the Montreal Protocol to phase out those chemicals. And it worked. The ozone layer is healing.
+
+Speaker 4: But is that a fair comparison? Banning a few chemicals used in spray cans and refrigerators seems so much simpler than replacing the entire energy source that powers our civilization.
+
+Speaker 2: Oh, you are absolutely right, Maya. The climate challenge is orders of magnitude more complex and more difficult. But the principle is the same: science identified a threat, and international cooperation solved it. What's different now, and you know, what gives me a sliver of hope, is that the threat is no longer an invisible hole in the sky. 
+
+Speaker 2: The escalating costs of floods, droughts, and fires are becoming so painfully obvious that the political calculation for leaders is starting to change. Inaction is becoming more politically expensive than action.
+
+Speaker 4: And maybe the consensus doesn't just come from those leaders in a conference room. You know, I think about the youth climate movement. When millions of young people around the world take to the streets, inspired by activists like Greta Thunberg, that creates a different kind of pressure. 
+
+Speaker 4: It‚Äôs a moral pressure. It builds from the ground up and forces its way into the halls of power. It's a reminder that this isn't just about economics; it's about their future that's being negotiated away.
+
+Speaker 1: That‚Äôs a powerful point, Maya, the role of that moral pressure from the next generation. And it brings up the stark reality of what is truly at stake here. Carter, when we talk about these long-term consequences, like sea-level rise, the report makes it clear these are not temporary problems that will just go away if we fix our emissions. It talks about impacts being locked in for centuries. Can you explain that long-term legacy?
+
+Speaker 2: Yes, and this is a concept that is, uh, difficult to grasp but absolutely crucial. The Earth's climate system has enormous inertia. Think of the oceans like a giant flywheel. They have absorbed over 90 percent of the excess heat we've trapped, and it takes a very, very long time for that heat to dissipate. Likewise, carbon dioxide is a very long-lived gas. Much of what we emit today will still be in the atmosphere, warming the planet, hundreds of years from now.
+
+Speaker 3: So what does that mean in practical terms? Let‚Äôs say, hypothetically, we wave a magic wand and stop all greenhouse gas emissions tomorrow, globally. Zero emissions. Does the warming stop? Do sea levels stop rising?
+
+Speaker 2: No. And that is the hard reality. Even in that magical scenario, the planet would continue to warm for some time, and sea levels would continue to rise for centuries, possibly for millennia. The heat that is already stored in the deep ocean would continue to circulate and warm the surface. 
+
+Speaker 2: The existing greenhouse gases would continue to trap heat. The amount of warming and sea level rise we've already experienced is, in many ways, a done deal. That is the legacy we have already written.
+
+Speaker 4: So even in the best-case scenario, things will still get worse before they get better.
+
+Speaker 2: For a time, yes. But it's vital we don't interpret that as our efforts being futile. It is the absolute opposite. The actions we take in this decade will determine how much worse things get and for how long. 
+
+Speaker 2: We are at the controls, making a choice right now between a future where sea levels rise by, say, another meter, which is devastating but perhaps manageable, and a future where they rise by ten meters, which would be an unimaginable catastrophe.
+
+Speaker 2:  We are deciding today what percentage of the world‚Äôs species will go extinct. We are deciding how much of the planet will become uninhabitable for our own grandchildren. We are locking in that future with the choices we make today.
+
+Speaker 1: That is an incredibly powerful and sobering thought, Carter. The idea that we are, right now, writing the legacy for centuries to come. You know, it raises a profound psychological question. How do we live with that knowledge? How do we confront this reality of a locked-in future without falling into paralysis or, you know, just complete despair?
+
+Speaker 4: That‚Äôs the question I grapple with every day, Alice. And I know so many others do, too. There‚Äôs a real grief in realizing what we‚Äôve already lost, and a real fear for what‚Äôs to come. And some days, that can feel completely overwhelming. But, you know, what I've found, for myself, is that the only real antidote to that anxiety is action.
+
+Speaker 3: Action. That‚Äôs easy to say. But if the problem is this big, and some of the damage is already done, what does that action even look like? It can feel like‚Ä¶ I don‚Äôt know, bailing out a sinking ship with a teaspoon. It might make you feel better, but is it actually changing the outcome? I worry about climate fatigue. People just get so overwhelmed by the bad news that they tune it all out.
+
+Speaker 4: I see what you mean, Frank. I really do. But maybe the teaspoon isn't the point. Maybe the point is that when you start bailing, the person next to you sees you and picks up their own teaspoon. And then another person does. The action itself builds a sense of community and shared purpose. 
+
+Speaker 4: It‚Äôs about building what some people call "active hope." It's not a blind optimism that everything will be fine. It‚Äôs a belief that if we work together, we can still create a better outcome than the one we‚Äôre heading for. And that work, that action, gives us a sense of agency in a situation that can feel‚Ä¶ hopeless.
+
+Speaker 2: I think that‚Äôs a crucial insight, Maya. And Frank, to address your point about fatigue, part of the solution is to change the narrative from one of pure sacrifice to one of opportunity. And there‚Äôs real data to back this up. You know, the transition to a clean economy isn't just about shutting things down; it's about building new things. The International Energy Agency has reported that jobs in the clean energy sector are growing rapidly around the world, outpacing the fossil fuel industry.
+
+Speaker 1: So this connects back to what we discussed earlier, the idea of a Just Transition. It‚Äôs about creating tangible, positive, real-world opportunities for people.
+
+Speaker 2: Precisely. It's about showing people a vision of the future that is not just survivable, but actually better. A future with cleaner air, quieter cities, and new, well-paying jobs in industries like solar installation, battery manufacturing, and grid modernization. When people can see a concrete benefit for themselves and their communities, it‚Äôs a very powerful motivator. It helps to overcome that sense of fatigue and shifts the focus to building a future we actually want.
+
+Speaker 1: So, as we talk about building this new future, let‚Äôs dive into another one of the advanced technologies mentioned in the report's appendix. We hear a lot of buzz about it. Carter, can you tell us about Green Hydrogen? What is it, and what role is it supposed to play?
+
+Speaker 2: Of course. In simple terms, green hydrogen is a way to store clean energy. You take electricity from a renewable source, like a solar or wind farm, and you use it to power a machine called an electrolyzer. And this machine splits water‚Äîwhich is H2O‚Äîinto its basic components, hydrogen and oxygen. The hydrogen that you get from this process is a clean, carbon-free fuel.
+
+Speaker 3: Okay, so it's a clean fuel. But Carter, I've heard there are major problems with it. For one, it‚Äôs incredibly inefficient, isn't it? You use a huge amount of electricity to make the hydrogen, and then you lose more energy when you convert it back into power. And the cost‚Ä¶ it seems to be way more expensive than just using the electricity directly. It sounds like another one of those futuristic solutions that's always just over the horizon.
+
+Speaker 2: Uh, those are absolutely the key challenges, Frank. You are right. There are energy losses in the process, and right now, the cost of producing green hydrogen is still high compared to other options. However, the costs are falling rapidly as the technology scales up, much like we saw with solar panels a decade ago. And its real potential isn't necessarily for powering cars or homes, where batteries are often a better fit.
+
+Speaker 1: So where does it fit? What's the specific job for this tool?
+
+Speaker 2: Its promise is in those hard-to-abate sectors that we keep coming back to. Think about heavy industries like steel and cement manufacturing, which require incredibly high heat that's hard to achieve with just electricity. Or, uh, long-haul transportation, like container ships and airplanes. 
+
+Speaker 2: For these sectors, a clean-burning fuel like green hydrogen could be a genuine game-changer, a way to decarbonize parts of our economy that batteries can't easily reach.
+
+Speaker 4: You know, hearing this, it highlights something I think is really confusing for a lot of people. It feels like every year there‚Äôs a new savior technology. First, it was biofuels, then it was clean coal, now it's hydrogen. It‚Äôs hard to keep up, and it can start to feel like we're just hoping for some single magic bullet to come along and fix everything for us. Maybe that‚Äôs the wrong way to look at it?
+
+Speaker 2: Maya, that is an incredibly wise observation. And you are absolutely right. The search for a single magic bullet has been a distraction. The most useful analogy is to think of it as a toolbox. You would never try to build a house with only a hammer. You need a saw, a screwdriver, a wrench‚Ä¶ all for different tasks.
+
+Speaker 1: Oh, I see. So it's not about hydrogen versus batteries, or renewables versus nuclear.
+
+Speaker 2: Exactly. It's about having all of them in the toolbox. We need renewables to generate the clean electricity. We need batteries for short-term storage and for electric vehicles. We need green hydrogen for those specific industrial and transport applications. We need to massively ramp up energy efficiency to reduce overall demand. The goal isn't to find the one perfect solution; it's to build a resilient, robust, and flexible system using all the different tools that we have.
+
+Speaker 1: That‚Äôs a really helpful way to frame it, Carter. A whole toolbox, not a magic wand. But you know, when you talk about all these huge, complex systems‚Äîfrom green hydrogen infrastructure to nuclear power plants‚Äîit can all feel very distant and overwhelming for the average person. 
+
+Speaker 1: Which brings us to a question I think is on everyone's mind. What about us? What about individual action versus systemic change? Maya, does it really make a difference if I diligently sort my recycling or eat less meat when the scale of the problem is this vast?
+
+Speaker 4: That is the question, isn't it? And it's so easy to feel like your small actions are just a drop in an angry ocean. But I truly believe they matter, just maybe not in the way we usually think. You know, the direct carbon impact of me not using a plastic straw isn't going to stop the West Antarctic ice sheet from collapsing. I get that. But that's not the only point.
+
+Speaker 3: But isn't it the most important point? I mean, we can all feel good about our reusable coffee cups, but meanwhile, a single coal plant is wiping out all our collective efforts in a matter of minutes. It feels like a distraction. It shifts the burden of responsibility from the handful of massive corporations and governments causing the problem onto the shoulders of billions of individuals. It feels unfair.
+
+Speaker 4: I see that, Frank, and that's a real danger. But when we make these conscious choices, we're doing more than just reducing our own tiny footprint. We are sending signals. We send a signal to the market that there's demand for sustainable products. We send a signal to our friends and neighbors that this is something we care about, which, you know, helps to normalize climate consciousness in our culture. 
+
+Speaker 4: And most importantly, we send a signal to politicians that we are a constituency that will support bold climate action. Our individual actions build the social and political momentum for the big systemic changes to happen.
+
+Speaker 2: I think Maya's point is absolutely crucial. And Frank's concern is equally valid. The two ideas are not in opposition; they actually reinforce each other. You need both. Individual action alone is not sufficient, that's clear. We cannot solve this crisis by changing lightbulbs. We absolutely need the large-scale government policies and corporate transformations that will decarbonize our entire industrial base.
+
+Speaker 1: So it's not a choice between one or the other.
+
+Speaker 2: Not at all. But at the same time, that systemic change doesn't just happen in a vacuum. It is the result of millions of people demanding it. So individual action is the necessary foundation. It's the engine of cultural change that makes the politics of systemic change possible. They are two sides of the very same coin. One cannot succeed without the other.
+
+Speaker 1: That‚Äôs a great way to put it. So if individual action helps create the political will for systemic change, let's talk about one of the most powerful systemic tools that economists often discuss. Carter, the report mentions carbon pricing and emissions trading systems. Can you explain what that is?
+
+Speaker 2: Certainly. Carbon pricing is a very direct economic strategy. It's about putting a price on carbon pollution to discourage it. There are two main ways to do it. You can have a straightforward carbon tax, where the government sets a price per ton of carbon dioxide emitted. Or you can have what's called a cap-and-trade system.
+
+Speaker 1: And how does cap-and-trade work?
+
+Speaker 2: In that system, the government sets a limit, a cap, on the total amount of emissions allowed in a sector, say, the electricity sector. And that cap gets lower every year. Then, companies within that sector are given permits to pollute, or they have to buy them. If a company pollutes less than its permit allows, it can sell its leftover permits to a company that pollutes more. This creates a financial incentive to cut emissions as cheaply as possible.
+
+Speaker 3: Okay, but let's be honest about what a carbon tax really means for the average person. It just gets passed on to the consumer, right? It means higher prices at the gas pump, higher home heating bills. It seems like a policy that would disproportionately hurt lower-income families and working people, who spend a much bigger chunk of their income on those essentials. It sounds deeply unfair.
+
+Speaker 2: That is the single biggest and most important concern with carbon pricing, Frank. And if it's designed poorly, that's exactly what can happen. It can be regressive. However, a well-designed system can actually address this and be equitable. For example, some of the most popular proposals are for a carbon fee and dividend system.
+
+Speaker 1: A dividend? So you're saying people would get money back?
+
+Speaker 2: Exactly. The revenue collected from the carbon tax isn't just kept by the government to spend on other things. It's returned directly to every citizen on an equal, per-person basis, like a check in the mail or a direct deposit. 
+
+Speaker 2: In that system, most lower and middle-income families would actually come out ahead. They would get more back in the dividend than they pay in higher energy costs, simply because wealthier people tend to travel more, have larger homes, and have a much larger carbon footprint.
+
+Speaker 4: You know, it's also about what costs we're already paying. We don't see a line item on our bills for it, but we are all paying the price for pollution right now. We pay it in healthcare costs from asthma and other respiratory diseases linked to burning fossil fuels. 
+
+Speaker 4: We pay it in our insurance premiums, which go up after every climate-fueled disaster. We pay it in our taxes, which go to rebuilding a town after a flood. A carbon price isn't creating a new cost; it's just making a hidden cost visible and putting it on the people who are creating the pollution.
+
+Speaker 1: That‚Äôs a powerful reframe, Maya. Shifting our perspective from a new tax to making a hidden cost visible. This conversation about fairness and who pays the cost brings us to another critical justice issue the report touches on: the impact on the workers and communities whose entire economies are built on the old system. Carter, can you talk about the concept of a Just Transition?
+
+Speaker 2: Yes, and you know, this has moved from the fringes of the discussion to the absolute center, because it's both morally right and, frankly, politically essential. A Just Transition means ensuring that the massive shift to a green economy is fair and inclusive. It means we don't leave coal miners, oil rig workers, and entire communities that depend on these industries behind.
+
+Speaker 3: This is something I think is absolutely critical, and it's often glossed over. You can't just tell millions of people that their jobs, their skills, their entire community's history is obsolete without a concrete, funded plan. If you do, you get massive social and political unrest. For people to buy into this huge economic shift, they have to see a future for themselves in it. A real plan for retraining, for new jobs in clean energy manufacturing or grid modernization, that is absolutely essential.
+
+Speaker 4: And it's more than just a paycheck, isn't it? It's about identity and community. For generations, some towns have been defined by the local power plant or the mine. That's a source of pride. A just transition means investing directly in those places, helping them to diversify their economies and build a new foundation that honors their heritage but, you know, allows them to thrive in a different kind of future. It's about respecting people while we make these big, necessary changes.
+
+Speaker 2: That's right. And it means ensuring that the new green jobs are good jobs, with fair wages, benefits, and the right to unionize. The goal isn't just to swap a fossil fuel job for any old job; it's to ensure the clean energy economy creates widespread prosperity and opportunity. If it doesn't, as Frank said, it will fail politically.
+
+Speaker 1: This focus on political stability is a crucial point. The report also talks about how climate change is a threat multiplier, not just for economies, but for global peace and security. Carter, can you explain how climate change can lead to conflict?
+
+Speaker 2: Well, the mechanism, according to defense and intelligence analysts, is that climate change exacerbates existing tensions and vulnerabilities. It's rarely the single, direct cause of a war, but it's like pouring gasoline on a fire that's already smoldering.
+
+Speaker 1: Can you give us an example?
+
+Speaker 2: Take a region that already has a history of ethnic tension and a fragile government. Now, add a multi-year, climate-driven drought. The water sources dry up. The pastures for livestock wither away. Crops fail. This leads to massive food and water scarcity, which in turn can drive resource competition between different groups. 
+
+Speaker 2: It can cause governments to collapse, create mass displacement, and open up a power vacuum that can be exploited by extremist groups. The climate stress is the catalyst that pushes a fragile situation into a full-blown crisis.
+
+Speaker 3: But hang on a minute. It seems to me that people have been fighting over land and water for thousands of years. How can we be so sure that this isn't just old conflicts playing out, and that we're just slapping a new climate change label on them? Is the link really that direct?
+
+Speaker 2: That's a fair question, Frank. And you're right, these are often old tensions. But what the science and the data show is a clear intensification. The droughts are more severe and longer-lasting than before. The floods are more extreme. The report notes that the IPCC states with high confidence that climate extremes are increasingly driving displacement, and that displacement itself is a major source of instability. So it‚Äôs not creating conflicts out of thin air; it‚Äôs making existing ones far more frequent and far more deadly.
+
+Speaker 4: You know, when I hear this, I just think about the human cost. We see these headlines about instability in a faraway region, but we forget that these are families being forced to flee their homes because the land they have farmed for generations can no longer support them. They are not leaving because they want to; they are leaving because they have no choice. It connects directly back to that horrifying statistic you mentioned earlier, Carter, about the death rate from these disasters being 15 times higher in vulnerable regions.
+
+Speaker 1: It truly underscores the profound inequity of this crisis. And this idea of instability leads me to another geopolitical question. We've talked about how the energy transition changes the map of power from oil states to countries with critical minerals. Carter, how is this massive global shift changing the relationships between major world powers?
+
+Speaker 2: It's reshaping geopolitics in a fundamental way. For a century, international relations have been shaped by who has the oil and the gas. But in a world powered by renewables, the sources of power change. It shifts from countries with fossil fuel reserves to countries that lead in manufacturing the key technologies, so that‚Äôs solar panels, wind turbines, and batteries.
+
+Speaker 3: So we're just trading a dependency on oil from the Middle East for a dependency on batteries and solar panels from, say, China? It sounds like we're just swapping one set of geopolitical problems for another. We're still vulnerable, just in a different way.
+
+Speaker 2: That is a very real risk, Frank, and it‚Äôs a major strategic concern for governments in Europe and North America. Creating more resilient, secure, and diversified supply chains for these clean energy technologies is a huge global priority right now. But there's also a fundamental upside to this new map.
+
+Speaker 1: And what‚Äôs that?
+
+Speaker 2: The resources for renewable energy, you know, sunlight and wind, are far more democratically distributed around the globe than fossil fuel reserves are. Almost every single country has the potential to generate its own clean energy for its own people. Over the long run, this could lead to greater energy independence for many nations, reducing the number of global choke points and potentially leading to a more stable and equitable world.
+
+Speaker 4: That‚Äôs a really hopeful thought. The idea that this transition, if we manage it right, could actually make the world a more peaceful place by giving more countries control over their own energy future.
+
+Speaker 1: It is. We‚Äôve spent a lot of time talking about the failures of national governments and these huge geopolitical shifts. But we know a lot of the real-world changes are happening at a more local level. Carter, what does the report say about the role of cities in leading climate action?
+
+Speaker 2: In many cases, cities are where the action is. They are often more agile, more pragmatic, and less tied up in partisan gridlock than national governments. And they have to be, because they are on the front lines of the impacts, from heat waves to flooding. So you have these incredible networks, like the C40 Cities Climate Leadership Group, which are basically hubs of innovation.
+
+Speaker 3: What kind of innovation are we talking about? What are cities actually doing on the ground that makes a difference?
+
+Speaker 2: They are doing a lot. They are electrifying their public transport fleets, from buses to garbage trucks. They are creating greener building codes that mandate higher energy efficiency. They are investing in massive tree-planting campaigns and creating more parks to combat the urban heat island effect. 
+
+Speaker 2: They are redesigning streets to be more friendly for pedestrians and cyclists, and less dominated by cars. All of these actions, when added up across hundreds of cities, have a huge impact on both emissions and the quality of life for residents.
+
+Speaker 4: And you can really feel that difference. You know, when your city invests in a new, reliable bus line or a safe, protected bike lane, your life gets better. Your commute is less stressful. The air feels cleaner. It‚Äôs another one of those examples where the climate solution is also just a better way of living. It's not about sacrifice; it's about building cities that are more pleasant and more livable for everyone.
+
+Speaker 1: It truly seems a recurring theme is that a more sustainable world is also a healthier and more equitable one. We have covered so much ground today, from the accelerating science of a planet in crisis, to the cascading impacts on our health, economy, and security, and to the immense challenges and emerging pathways in our global response. 
+
+Speaker 1: As we draw this conversation to a close, I want to come back to a final, forward-looking thought from each of you. We are standing at the end of this decade of consequence. The report makes it clear the window for action is closing with terrifying speed. Where do we go from here? Frank, what is your final takeaway?
+
+Speaker 3: For me, it has to be about getting real. The scale of this report shows that we are past the point of easy, feel-good solutions. The transformation that is required is going to be hard, and it's going to be expensive. We need to stop pretending otherwise. 
+
+Speaker 3: The path forward has to be built on honesty about the costs, on ensuring the transition is fair to working people, and on deploying every single pragmatic tool we have, from renewables to nuclear to carbon capture, without letting ideology get in the way. It‚Äôs an all-hands-on-deck emergency, and we need to start acting like it.
+
+Speaker 1: Thank you, Frank. A powerful call for pragmatic, honest, all-of-the-above action. 
+
+Speaker 4: I keep coming back to that idea of the story we tell ourselves. For so long, the climate story has been framed by fear, by what we have to give up. And that fear is real, the grief for what we're losing is valid. But a story of fear alone can lead to paralysis. I believe we have to start telling a new story, a story of what we stand to gain. 
+
+Speaker 4: We gain a chance to build a world that is healthier, more just, and more connected to nature and to each other. That's the vision we have to hold on to. The way forward has to be rooted not just in fear of the future we want to avoid, but in a compelling, active hope for the future we want to create.
+
+Speaker 1: Thank you, Maya. A beautiful and necessary call for a new, more hopeful narrative. And Carter, I‚Äôll give you the final word. After laying out all this sobering science and the stark conclusions of this report, what is the ultimate message you want to leave our listeners 
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/4p_climate_45min.txt b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/4p_climate_45min.txt
new file mode 100644
index 0000000..cccf953
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/demo/text_examples/4p_climate_45min.txt
@@ -0,0 +1,421 @@
+Speaker 1: Hello and welcome to Planet in Peril. I'm your host, Alice. We're here today to discuss a really sobering new report that looks back at the last ten years of climate change, from 2015 to 2025. It paints a picture not just of steady warming, but of a dangerous acceleration. And to help us unpack this, I'm joined by our expert panel. Welcome Carter, Frank, and Maya.
+
+Speaker 2: Hi Alice, it's great to be here. I'm Carter.
+
+Speaker 3: Hello, uh, I'm Frank. Good to be on.
+
+Speaker 4: And I'm Maya. Thanks for having me.
+
+Speaker 1: So, let's dive right in. Carter, this report, titled Decade of Consequence, uses some very strong language right from the start. Can you set the scene for us? What makes this last decade so... pivotal and alarming?
+
+Speaker 2: Well Alice, the key takeaway is that word you used: acceleration. We're no longer on a gentle, predictable upward slope. The data, and this is coming from the big global bodies like the IPCC and the World Meteorological Organization, shows that every key indicator of the planet's health sped up in the last ten years. We've essentially pushed the global system into a new, more volatile state.
+
+Speaker 4: You know, that really resonates. It feels that way, doesn't it? I mean, just thinking about my own garden, the seasons feel less predictable. The summer heat seems to arrive earlier and hit harder every year. It feels less stable.
+
+Speaker 1: That‚Äôs a great point, Maya. It's moved from an abstract concept to a lived experience for so many. Carter, let's talk about the most direct indicator, temperature. The report says records haven't just been broken, they have been shattered.
+
+Speaker 2: That's right. The ten-year period from 2015 to 2024 is, without a doubt, the warmest decade since we started keeping records in 1850. And it's not a fluke... every single year within that decade is among the ten warmest years ever recorded.
+
+Speaker 3: Okay, Carter, but we always hear about record-breaking years. Every year seems to be the hottest ever. How is this different? Is it just a continuation of a trend?
+
+Speaker 2: It is, but the trend itself is speeding up. And this decade saw something truly significant. The year 2024 became the first full calendar year where the global average temperature went past the 1.5 degree Celsius threshold from the Paris Agreement. Specifically, it hit about 1.55 degrees above the pre-industrial average.
+
+Speaker 4: Wow. One point five degrees. We‚Äôve been talking about that number as a future goal, a line we must not cross. And we're already there, even temporarily? That's... unsettling.
+
+Speaker 3: But Carter used the word temporarily. So does that mean the Paris Agreement goal is already lost? And you know, 2024 had a strong El Ni√±o event, which is a natural warming cycle. How much of this is just nature doing its thing?
+
+Speaker 2: That's an excellent and crucial question, Frank. No, a single year's breach doesn't mean the goal is permanently lost, as that refers to a long-term average. But it serves as a massive warning shot. It shows that the climate system is capable of reaching these dangerous levels now. And while El Ni√±o played a role, it was riding on top of this powerful, long-term warming trend. The key isn't just one record year; it‚Äôs the accelerating rate of warming.
+
+Speaker 1: Can you elaborate on that? The accelerating rate?
+
+Speaker 2: Of course. Data from NOAA, the US National Oceanic and Atmospheric Administration, shows that since 1982, the world has been warming at a rate of zero point two degrees Celsius per decade. Now, that might not sound like much, but it‚Äôs more than three times faster than the average rate since 1850. So, to answer your question, Frank, this isn't a natural blip. The engine is revving faster and faster.
+
+Speaker 1: So let's talk about that engine. What's driving this acceleration? The report links it directly to greenhouse gases in the atmosphere.
+
+Speaker 2: Exactly. The physics are very direct. And in the last decade, the concentrations of these gases have soared to levels that are, frankly, unprecedented in human history. The IPCC's latest major report states with high confidence that atmospheric carbon dioxide levels are now higher than at any time in at least two million years.
+
+Speaker 4: Two million years. I... I can't even process that number. It feels like we're running a massive, uncontrolled experiment on our only home.
+
+Speaker 2: That‚Äôs a good way to put it, Maya. To give you some concrete numbers, in 2024, the average concentration of carbon dioxide hit 422.7 parts per million. That's a full 50 percent higher than before the industrial age began. And just like with temperature, the rate of increase is accelerating. In the 1960s, it grew by about zero point eight parts per million per year. In the last ten years? It's averaged 2.6 parts per million per year. The year 2024 saw the largest single-year jump ever recorded.
+
+Speaker 1: So the warming is accelerating, and the concentration of the gas causing the warming is also accelerating. This brings us to the core question, which is addressed in the second section of the report. The science of attribution. Carter, how certain are scientists that this is... us?
+
+Speaker 2: The scientific community is as certain as it is about the theory of gravity. The IPCC uses the strongest possible language. The report states unequivocally that human influence has warmed the atmosphere, ocean and land. There's no ambiguity left.
+
+Speaker 3: Unequivocal. That is a strong word. But what does that mean in practice? I mean, a lot of people hear this and think, okay, but how do they know it's not the sun, or volcanoes, or some other natural cycle?
+
+Speaker 2: It's a fair question. Scientists know because they use incredibly sophisticated climate models. They run simulations of the last 150 years with only natural factors, like solar cycles and volcanic eruptions. And when they do that, the models completely fail to replicate the warming we've actually observed. They just can't get the temperature to rise. It's only when they add in the human-caused greenhouse gas emissions that the models accurately match the real-world temperature record.
+
+Speaker 4: Oh, I see. So it‚Äôs like trying to solve a mystery. You test out all the natural suspects, and none of them can be the culprit. But when you add in the human suspect, the story suddenly makes perfect sense.
+
+Speaker 2: That's a perfect analogy. The IPCC even quantifies it. The best estimate is that humans have caused about one point zero seven degrees Celsius of warming since the late 1800s. The total observed warming over that same period? About one point one degrees Celsius. So, we account for... basically all of it.
+
+Speaker 3: Right. So if it's unequivocally us, what specific human activities are we talking about? When people say we need to cut emissions, what are we actually supposed to be cutting?
+
+Speaker 1: That‚Äôs a perfect question, Frank. Carter, the report gets right into this. Can you break down the main sources for us?
+
+Speaker 2: Absolutely. The picture is actually very clear. The primary driver, by a huge margin, is the burning of fossil fuels, so that‚Äôs coal, oil, and natural gas. In 2019, about 79 percent of all global greenhouse gas emissions came from using fossil fuels across four main areas: energy production for electricity and heat, industry, transportation, and buildings.
+
+Speaker 3: So it really isn't just about driving cars. I mean, that's what you always hear. But this is about how we power our homes, how we make things, our entire economic structure.
+
+Speaker 2: Precisely. The power sector alone, which generates electricity and heat, is the single biggest contributor. And what's concerning is that even with the amazing growth of renewable energy, the International Energy Agency has pointed out that demand for oil and gas has stayed stubbornly high. We're still investing in new fossil fuel infrastructure, which creates a real risk of locking in these emissions for decades to come.
+
+Speaker 4: You know, it's so easy to picture smokestacks and the tailpipes of cars when we talk about this. But the report mentions another big piece of the puzzle, right? Something about our land, about forests and farming?
+
+Speaker 2: Yes, and it's a critical piece, Maya. The remaining 21 to 22 percent of emissions come from what scientists call AFOLU. That stands for Agriculture, Forestry, and Other Land Use. This includes methane emissions from livestock, nitrous oxide from fertilizers, and, crucially, deforestation.
+
+Speaker 1: And why is deforestation such a major factor?
+
+Speaker 2: It delivers a devastating one-two punch. First, when we clear forests, primarily for agriculture, we release the massive amounts of carbon that were stored in those trees and soils directly into the atmosphere. Between 2015 and 2020, the world continued to lose an estimated 10 million hectares of forest every single year. Second, by destroying the forest, we're eliminating a vital natural carbon sink that would otherwise be absorbing CO2 from the air. So it adds carbon while also reducing the planet's ability to clean it up.
+
+Speaker 1: So we have a very clear picture of the sources. This leads to the obvious question of what we are doing about it. The report talks about a persistent and vast emissions gap. Carter, what is that?
+
+Speaker 2: The emissions gap is the difference between what countries have pledged to do and what the science says is actually required to meet the goals of the Paris Agreement. The United Nations Environment Programme releases a report on this every year, and the findings are stark. The 2023 report found that with the policies we have right now, the world is on a trajectory for a temperature rise of nearly 3 degrees Celsius by the end of the century.
+
+Speaker 4: Three degrees... Carter, we were just talking about how damaging it is to even temporarily hit 1.5 degrees. Three sounds... catastrophic.
+
+Speaker 2: It would be. To align with the 1.5 degree pathway, the report states that predicted global emissions in 2030 need to be cut by a staggering 42 percent from where they're heading now.
+
+Speaker 3: Hold on a minute. A 42 percent cut by 2030? Carter, that's just a handful of years away. Is that even realistic? Are countries just not trying, or is the goal itself simply impossible for our modern world to achieve?
+
+Speaker 2: It's an immense challenge, Frank, there's no question. The report does note that there has been some progress since the Paris Agreement was signed. Projected emissions for 2030 are lower now than they were expected to be a decade ago. However, this improvement is nowhere near the scale or speed that is required. So this gap... it really represents the collective failure of the world to turn political commitments into sufficient real-world action.
+
+Speaker 4: And while governments and experts are debating these huge numbers and percentages, people on the ground are already feeling the effects. It feels like the consequences are here now, but the solutions are still stuck in negotiations.
+
+Speaker 1: Maya, that is such a powerful point, and it leads us directly to one of the most significant scientific advancements of the past decade, which is the ability to link specific weather events directly to climate change. Carter, tell us about the science of attribution.
+
+Speaker 2: This has been a game-changer. For a long time, we could only say that climate change makes certain types of events, like heatwaves, more likely in general. But now, attribution science allows scientists to provide robust, quantitative assessments of the role human-caused warming played in a specific, individual event.
+
+Speaker 1: So how does that work, in simple terms?
+
+Speaker 2: They use multiple climate models to compare the probability of a specific extreme event happening in the world as it is today, with all our emissions, to its probability in a counterfactual world, a simulated world without human-caused greenhouse gases. This allows them to say, with a calculated degree of confidence, how much more likely or how much more intense an event was made because of climate change.
+
+Speaker 3: So you‚Äôre saying that scientists can now point to a specific flood, or a specific wildfire, and actually put a number on it? They can say this was 50 percent worse, or ten times more likely, because of our emissions?
+
+Speaker 2: Yes, exactly. The science has matured to that point. For example, studies have found that some recent heatwaves, like the one in the Pacific Northwest in 2021, would have been virtually impossible without human-induced climate change. This ability to quantify the human fingerprint on disasters is profound. It transforms climate change from a distant, future threat into a direct and measurable cause of the harm and damage people are experiencing today.
+
+Speaker 1: And this science has profound implications, doesn't it, Carter? It means the conversation shifts from future projections to present-day accountability. So let's talk about those cascading consequences the report details. It frames extreme weather as the new normal. What does that actually look like?
+
+Speaker 2: It looks like a world where the weather has fundamentally shifted gears. The science of attribution has now firmly linked the dramatic rise in the frequency and intensity of extreme events to human-caused warming. So what used to be a rare event is now becoming a regular occurrence. In 2024 alone, for example, there were over 600 reported extreme weather events.
+
+Speaker 4: It really does feel that way. I mean, the summer heat seems to build earlier and last longer, and it feels more oppressive, more dangerous than I ever remember. And then, when the rain finally comes, it's not a gentle shower. It's a deluge that overwhelms everything.
+
+Speaker 2: You've just described the mechanics of it perfectly, Maya. Extreme heat events have become more frequent and more severe. Temperatures hitting over 40 degrees Celsius, which is 104 degrees Fahrenheit, used to be a rarity in many places. Now, it's becoming common. And that heat leads to the paradox of the water cycle.
+
+Speaker 3: A paradox? How so? It seems to me we're either in a drought or a flood. How can both be happening more often? It feels contradictory.
+
+Speaker 2: It does, but they are two sides of the same coin. A warmer atmosphere holds more moisture, about 7 percent more for every single degree Celsius of warming. So when it does rain, the downpours are far heavier, which dramatically increases flood risk. In fact, since the year 2000, flood-related disasters have risen by 134 percent compared to the two decades before.
+
+Speaker 1: But what about the drought side of that coin?
+
+Speaker 2: At the same time, those higher temperatures bake the land. They increase evaporation from soil, from rivers, from reservoirs, leading to more rapid and severe droughts in many regions. This has given rise to a phenomenon that scientists are now calling climate whiplash, where a region can swing violently between a devastating drought one year and catastrophic floods the next. It just overwhelms our infrastructure and our ecosystems.
+
+Speaker 1: And this combination of prolonged heat and severe drought creates a perfect storm for another disaster we see constantly on the news: wildfires.
+
+Speaker 2: Exactly. Wildfire seasons have become longer and more intense in many parts of the world. Scientific analysis estimates that human-caused climate change has already doubled the area of forest burned in the Western United States in recent decades. And this creates a terrifying feedback loop. These megafires don't just destroy communities, they release enormous amounts of stored carbon back into the atmosphere, which in turn causes more warming, which then leads to more fires.
+
+Speaker 4: I live in California, and that feedback loop is something you can feel in your bones. The fear during fire season is palpable. And even if you're not near the flames, the smoke can choke the sky for weeks. It's a constant, unhealthy reminder of what's happening.
+
+Speaker 1: Maya, you've taken us right to the next critical point. These disasters are not just statistics. They have a direct and severe impact on our health. The report goes so far as to call climate change the greatest global health threat of the 21st century. Carter?
+
+Speaker 2: It is, without a doubt. The impacts are extensive. Let's start with the most direct one: the heat itself. Extreme heat is one of the deadliest weather phenomena. The IPCC confirms with very high confidence that the increase in extreme heat has resulted in human mortality and morbidity in every region of the world.
+
+Speaker 3: We hear about vulnerable people being at risk during heatwaves, which makes sense. But does it have a broader impact on the general population, on the economy?
+
+Speaker 2: A massive one. The Lancet Countdown on Health and Climate Change, which is a major annual report, documented these record-breaking health threats. They estimated that in 2023, 3.4 billion potential labor hours were lost globally just due to people being exposed to extreme heat. That‚Äôs an increase of 69 percent compared to the average in the 1990s. So yes, it has huge economic and productivity impacts.
+
+Speaker 1: And those are just the direct impacts of the heat itself. What about the less obvious health threats?
+
+Speaker 2: They are just as concerning. A warmer world is a more hospitable world for the vectors that carry diseases. Rising temperatures and changing rainfall patterns are expanding the geographic range for diseases like malaria, dengue, West Nile virus, and Lyme disease. We're seeing them appear in places they've never been before.
+
+Speaker 4: And it must affect our food and water, the very foundations of our health.
+
+Speaker 2: Absolutely. Climate change directly undermines both. The report notes that climate change has slowed the growth of agricultural productivity over the past 50 years. It's a key driver of the global food insecurity that affected, by some estimates, over 750 million people in 2023. At the same time, about half the world's population, that's four billion people, now experiences severe water scarcity for at least one month of the year, a situation made much worse by melting glaciers and prolonged droughts.
+
+Speaker 4: And beyond all the physical ailments, there has to be a psychological toll. The stress of living with this uncertainty, the trauma of surviving a disaster, the anxiety about what the future holds for your children. The report touches on mental health, doesn't it?
+
+Speaker 2: It does. This is a growing and critical area of concern. The IPCC has now clearly associated increasing temperatures and the trauma from extreme events with significant challenges to mental health. This includes post-traumatic stress disorder after a disaster, anxiety and depression when people lose their homes or livelihoods, and a broader condition people are calling eco-anxiety, especially among young people, about the future of the planet.
+
+Speaker 1: And this idea of a psychological toll, this eco-anxiety, leads to another form of stress: financial. The report makes it clear that the economic consequences of climate change have become impossible to ignore over the last decade. Carter, can you start by outlining the scale of these costs?
+
+Speaker 2: The scale is immense, and it's escalating rapidly. The most direct measure we have comes from the global reinsurance industry, the companies that insure the insurance companies. Data from the Swiss Re Institute shows that for five consecutive years, from 2020 through 2024, the global insured losses from natural catastrophes have surpassed 100 billion US dollars.
+
+Speaker 3: Okay, 100 billion is a massive number. But you have to wonder, isn't some of that just due to inflation, or the simple fact that we've built more expensive homes and cities in high-risk areas like coastlines? Are the storms themselves really causing more financial damage, or do we just have more valuable things in their way?
+
+Speaker 2: That's a very important point, Frank. And yes, growing asset values in vulnerable areas, what they call exposure, is definitely a part of the story. However, the data clearly shows that the primary driver of the upward trend is the increased frequency and intensity of the severe weather events themselves. For example, in 2024, the total economic losses from natural disasters hit an estimated 318 billion dollars. The insured portion was 137 billion. The rest was uninsured.
+
+Speaker 1: So more than half of all the losses were not covered by insurance. What does the report say about that?
+
+Speaker 2: It refers to this as the protection gap, and this gap is widening. In 2024, 57 percent of all global economic losses from these catastrophes were uninsured. This is a huge problem, especially in developing countries where very few people have insurance. For these communities, a single disaster can wipe out years of economic development and trap them in a cycle of poverty and recovery.
+
+Speaker 4: And this isn't just an abstract global statistic. I mean, we see it in our own communities. We hear stories of insurance premiums skyrocketing to the point where they are unaffordable. Or worse, insurance companies simply pulling out of entire states like Florida or California because the risk of wildfire or flooding has become too high. This creates this incredible financial stress for families who are just trying to protect their homes.
+
+Speaker 1: And it's not just private homes and property. Our shared public infrastructure is also facing enormous risks.
+
+Speaker 2: That's right. Our entire modern society, the energy grids, transportation networks, water treatment plants, they were all designed and built for a climate that no longer exists. 
+
+Speaker 2: Sea level rise directly threatens ports and coastal cities, extreme heat puts an incredible strain on power grids, and intense flooding can destroy roads and bridges. The World Bank has warned that the cost of inaction, particularly in terms of damage to infrastructure, could run into the trillions of dollars.
+
+Speaker 3: Trillions in damage. But fixing it would also cost trillions. I mean, upgrading a nation's entire power grid or rebuilding its coastal defenses requires a colossal upfront investment. Where is that money supposed to come from, especially for countries that are already struggling?
+
+Speaker 2: It's a major challenge, but the analysis shows that inaction is far more expensive. The World Bank estimates that for every one dollar invested in making infrastructure more climate-resilient now, we could see a benefit of four dollars in avoided damages and disruptions down the road. It‚Äôs a classic case of an ounce of prevention being worth a pound of cure.
+
+Speaker 1: When homes are destroyed, infrastructure fails, and livelihoods are lost, people are inevitably forced to move. The report identifies climate change as a powerful driver of human displacement.
+
+Speaker 2: Yes, it acts as a threat multiplier. The number of forcibly displaced people worldwide has nearly doubled in the last ten years, reaching an estimated 123.2 million by the end of 2024. 
+
+Speaker 2: And while conflict is still a primary driver, the IPCC states with high confidence that climate and weather extremes are increasingly forcing people from their homes on every single continent. In fact, 2024 saw the highest number of new displacements from extreme weather in 16 years.
+
+Speaker 3: I understand the numbers, but I think it's tricky to label someone a climate refugee. People move for all sorts of reasons, for better jobs, to escape poverty, for family. How can you really untangle all those factors and say with certainty that someone was displaced specifically by climate change?
+
+Speaker 2: You've hit on the core of the issue. It's rarely a single cause, which is why the term threat multiplier is so accurate. A drought, for example, can kill crops, which leads to economic collapse, which can then lead to resource conflicts, and all of those factors together push people to move. 
+
+Speaker 2: Climate change is the spark that ignites these other pre-existing vulnerabilities. And the report highlights a chilling statistic on this point: between 2010 and 2020, the death rate from floods, droughts, and storms was 15 times higher in highly vulnerable regions compared to the most secure ones.
+
+Speaker 4: And it's not just people who are being displaced and harmed. It's... it's everything else. The entire web of life that supports us.
+
+Speaker 1: That‚Äôs a vital point, Maya. The report draws a direct line between the climate crisis and the broader biodiversity crisis that's happening all around us. Carter?
+
+Speaker 2: Yes, the two are deeply intertwined. Climate change is a primary driver of what many scientists now refer to as the Earth's sixth mass extinction. A landmark global assessment from the IPBES warned that an estimated one million animal and plant species are now threatened with extinction, many within decades. 
+
+Speaker 2: While land use change is currently the biggest driver, climate change is projected to become as, or even more, important in the coming decades.
+
+Speaker 1: Can you give us a concrete example of this happening right now?
+
+Speaker 2: The most potent symbol is the fate of the world's coral reefs. The last decade has been catastrophic for them. The Great Barrier Reef, for instance, has suffered six mass coral bleaching events just since 2015. 
+
+Speaker 2: These are caused by prolonged marine heatwaves that literally cook the coral, causing them to expel their symbiotic algae and turn white. The increasing frequency of these heatwaves leaves no time for the reefs to recover.
+
+Speaker 4: It‚Äôs so hard to hear that. Losing the coral reefs‚Ä¶ it's like imagining a world without the Amazon rainforest. It's a loss so profound you can't even begin to calculate the cost. A world that's just‚Ä¶ less alive.
+
+Speaker 2: And the science is very clear on this. Scientists warn that if global warming exceeds the 1.5 degree target, over 90 percent of the world's tropical coral reefs could be lost by the middle of this century. It's a devastating blow to marine biodiversity and to the millions of people who depend on those reefs for their food and their livelihoods.
+
+Speaker 1: That is an incredibly sobering thought, Maya. A world that is simply less alive. We've spent this time detailing an accelerating crisis with devastating impacts on our health, our economy, and the very biodiversity of the planet. It‚Äôs a stark picture. But the world has not been completely idle. The final section of the report assesses the global response. 
+
+Speaker 1: Carter, the central pillar of international climate policy over the past decade has been the Paris Agreement, adopted back in 2015. For listeners who may not remember the details, can you remind us what it set out to achieve?
+
+Speaker 2: Of course. The Paris Agreement was a genuine diplomatic breakthrough. For the first time, it brought all nations, both developed and developing, into a common framework to combat climate change. Its main goals are to hold the increase in the global average temperature to well below 2 degrees Celsius above pre-industrial levels, and to pursue efforts to limit that temperature increase even further to 1.5 degrees Celsius.
+
+Speaker 1: And how was it designed to achieve that? What's the actual mechanism?
+
+Speaker 2: The agreement operates on a five-year cycle of what's called ratcheting ambition. The idea is that countries are required to submit their own national climate action plans, which are known as Nationally Determined Contributions, or NDCs. Then, every five years, they are supposed to come back to the table with a new, stronger plan that is more ambitious than their last one.
+
+Speaker 3: Okay, hold on. Nationally Determined Contributions. That sounds like a lot of diplomatic jargon. If I'm hearing you right, does that just mean that every country gets to make up its own plan, and there's no real penalty or enforcement if they don't follow it or if their plan is too weak?
+
+Speaker 2: You're not wrong, Frank. It is not an international treaty with a heavy-handed enforcement mechanism in the traditional sense. It's a framework that is built more on transparency, reporting, and a kind of global peer pressure. The idea is that by having everyone's commitments out in the open, and by regularly taking stock of our collective progress, countries will be encouraged and expected to ramp up their efforts over time.
+
+Speaker 4: So it‚Äôs less of a strict global law and more of a collective promise. A set of promises, really. But based on everything we've talked about today, from the shattered temperature records to the accelerating ice melt, it seems like those promises are being broken.
+
+Speaker 1: Maya, that takes us directly to what the report calls the ambition gap. Carter, you explained the process. Now let's talk about the reality. How big is the shortfall between what countries have promised in their NDCs and what the science tells us we actually need to do?
+
+Speaker 2: The shortfall is massive. It's a chasm, really. The most recent analysis from the United Nations, which looked at the latest pledges from 195 countries, concluded that we are falling miles short of what's needed. If every country fully implemented its current pledges, we would see a global emission reduction of only about 5.9 percent by 2030 compared to 2019 levels.
+
+Speaker 4: Only six percent? That sounds tiny. How does that compare to the goal?
+
+Speaker 2: Well, the IPCC, the main scientific body, has found that to keep the 1.5 degree limit within reach, our emissions need to be slashed by at least 43 percent by 2030. So we are pledging for a six percent cut when we need a 43 percent cut. 
+
+Speaker 2: This gap means that the sum of all these national promises currently has the world on a trajectory toward a catastrophic level of warming somewhere between 2.5 and 2.9 degrees Celsius.
+
+Speaker 3: That's just astounding. It's not a gap, it‚Äôs a total disconnect from reality. So these huge annual conferences, the COPs we hear about on the news every year with all the world leaders, what are they actually achieving if the numbers are still this bad? Is it just a talking shop?
+
+Speaker 2: That's a criticism you hear a lot, and there is a great deal of frustration. These conferences are the primary venue for negotiating how to implement the Paris Agreement. They have produced some important outcomes. For instance, COP28 in Dubai produced the first ever global stocktake, which is essentially the world's climate report card. And it ended with a historic, first-ever call for countries to begin transitioning away from fossil fuels.
+
+Speaker 4: But Carter, the language there seems so important. I remember the debate was about a phase-out of fossil fuels, but the final agreement was to transition away from them. It feels like very carefully chosen, watered-down language. Does that kind of subtle change in wording actually lead to real-world action, or does it just give countries a loophole?
+
+Speaker 2: That is the heart of the debate. Many nations were deeply disappointed that the language wasn't stronger. The hope is that even that language signals a clear direction to the global economy. That same conference also established a global goal to triple renewable energy capacity and double the rate of energy efficiency improvements by 2030, which are very concrete targets.
+
+Speaker 1: And what about the most recent conference mentioned in the report, COP29?
+
+Speaker 2: That was dubbed the Finance COP. Its main job was to agree on a new climate finance goal to help developing nations. After very contentious negotiations, they agreed that developed countries should lead in mobilizing at least 300 billion dollars per year by 2035 for developing nations. But again, many of those nations expressed deep disappointment, stating that this number falls far, far short of their estimated needs, which are in the trillions.
+
+Speaker 1: This seems to be a recurring theme of falling short. Let's shift from the policy to the other major part of the response, which is technology. Here, the report does seem to highlight one area as a significant success story. And that is the renewables revolution.
+
+Speaker 2: Yes, this has been the brightest spot of the last decade without a doubt. We've seen an absolutely explosive growth of renewable energy technologies, especially solar panels and wind power. This was driven by incredible innovation and economies of scale, and it caused the costs of solar and wind to plummet. 
+
+Speaker 2: They are now the cheapest sources of new electricity generation in most of the world. To give you a sense of the scale, in 2023, the world added a record 473 gigawatts of new renewable capacity. The International Energy Agency even forecasts that this year, in 2025, renewables will overtake coal as the single largest source of global electricity.
+
+Speaker 3: That‚Äôs genuinely good news, and everyone loves seeing cheaper energy. But I noticed the report also says that we are still not on track to meet that COP28 goal of tripling renewable capacity by 2030. 
+
+Speaker 3: Why is that? If this technology is so cheap and effective, why aren't we just building it everywhere, all the time, as fast as we possibly can? What's the hold-up?
+
+Speaker 2: It's a great question, Frank. The momentum is incredible, but the scale of the challenge is even bigger. To achieve that tripling goal, we would need to be adding, on average, around 1,050 gigawatts of new capacity every single year for the rest of the decade. 
+
+Speaker 2: That's more than double the record we just set in 2023. The barriers are no longer primarily about cost; they are about things like modernizing our electrical grids to handle this new type of energy, overcoming supply chain bottlenecks for components, and streamlining the permitting processes to get projects built faster. So even in this huge success story, there is a major gap between our current progress and the required pace of change.
+
+Speaker 1: So, Carter, even our biggest technological success story, renewable energy, is facing a challenge of sheer scale and speed. The report points to another critical tool in the toolbox, something often called the first fuel, which is energy efficiency.
+
+Speaker 3: Now this is something that just seems like pure common sense to me. Using less energy to get the same result, whether it's an efficient appliance or an insulated home. It saves people money on their bills, it reduces strain on the power grid, and it cuts emissions. It seems like the absolute lowest-hanging fruit. Why aren't we talking about this constantly?
+
+Speaker 2: You are absolutely right, Frank. Improving energy efficiency is the cheapest and cleanest way to address our energy needs, which is why the COP28 goal to double the global average annual rate of energy efficiency improvements by 2030 is so critical. But the reality, as the report lays out, has been deeply disappointing.
+
+Speaker 1: How so? What does the data show?
+
+Speaker 2: After a brief speed-up in 2022, which was mostly in response to the global energy crisis, the rate of global energy intensity improvement slowed way down to just one percent in both 2023 and 2024. To be on a pathway to net-zero emissions, we need that rate to be averaging around four percent per year. So we are falling far short. The report effectively calls it a major and concerning policy failure on a global scale.
+
+Speaker 1: So if we're failing on the common-sense goal of efficiency, what about the more high-tech solutions that promise to clean up our existing emissions? Carter, the report spends some time on Carbon Capture, Utilisation, and Storage, or CCUS.
+
+Speaker 3: Again, on the surface, this sounds like a pragmatic solution. For those really difficult industries that are hard to electrify, like making cement or steel, why not just build a system to capture the carbon dioxide before it ever gets into the atmosphere? It seems like a logical way to solve the problem without having to completely shut down these essential industries overnight.
+
+Speaker 2: And that is exactly how it is often presented, Frank, as a necessary solution for these hard-to-abate sectors. And there is a lot of momentum in terms of announcements. The report notes there are over 700 projects in various stages of development. However, it also points to a massive gap between those announcements and the operational reality.
+
+Speaker 4: What do you mean by that? A gap between announcements and reality?
+
+Speaker 2: As of early 2024, the total global operational capacity for capturing CO2 was just over 50 million tonnes per year. That is a tiny fraction of what has been announced or proposed for 2030. And critically, only 20 percent of that announced capacity had actually reached a final investment decision. 
+
+Speaker 2: This indicates that most of these projects are still just on the drawing board, they are not yet real. So deployment has consistently and significantly lagged behind the expectations and the promises.
+
+Speaker 4: You know, I have to wonder if there's a risk here that this technology just becomes an excuse. A way for fossil fuel companies and heavy industries to continue polluting under the promise that someday, in the future, they'll be able to clean it all up. It feels like it could be a dangerous distraction from the real work of actually cutting emissions at the source.
+
+Speaker 1: Speaking of potentially dangerous and controversial ideas, the report mentions that as the world falls further behind on emissions reductions, there is a growing, albeit highly contentious, interest in something called solar geoengineering. Carter, can you even begin to explain what that is?
+
+Speaker 2: I can try. It's also sometimes called solar radiation modification. This refers to a set of hypothetical technologies that are designed to cool the planet by reflecting a small fraction of incoming sunlight back out to space. The most commonly discussed method is called stratospheric aerosol injection, which would involve spraying reflective particles, like sulfur dioxide, into the upper atmosphere to mimic the cooling effect of a large volcanic eruption.
+
+Speaker 4: That sounds absolutely terrifying. I mean, the idea of us deliberately conducting a planetary-scale experiment with our only atmosphere, when we can't possibly predict all the consequences‚Ä¶ it just feels like the height of human arrogance. We've already made one huge mess by pumping carbon dioxide into the air; this sounds like a way to make another, potentially even worse, mess.
+
+Speaker 2: Your reaction, Maya, captures the essence of the controversy. The scientific community is extremely cautious. The report emphasizes that geoengineering is not a substitute for cutting emissions. It does not address the root cause of the problem, which is the greenhouse gas blanket, and it carries immense and poorly understood risks. 
+
+Speaker 2: It could potentially disrupt regional weather patterns, harm the ozone layer, and it creates a moral hazard by possibly reducing the incentive for us to do the hard work of decarbonizing our economies.
+
+Speaker 1: So it's seen as a last-ditch, break-glass-in-case-of-emergency option with huge potential side effects. Maya, your point about the arrogance of these high-tech ideas is well taken. And while we're discussing these futuristic and risky technologies, the report highlights a profound failure in a much more basic and immediate area: finance and justice for the people already suffering the consequences. Carter, can you explain what the report calls the adaptation finance gap?
+
+Speaker 2: This is one of the most sobering findings in the entire report. While much of the focus is on mitigation, which is cutting emissions, adaptation, which is preparing for the impacts of climate change, is equally critical, especially for the world's most vulnerable nations. The UNEP Adaptation Gap Report revealed a staggering shortfall in funding.
+
+Speaker 1: How big is the shortfall?
+
+Speaker 2: The report estimates that the annual adaptation finance needs of developing countries are somewhere between 215 billion and 387 billion dollars. In stark contrast, the total international public finance that flowed to these countries for adaptation in 2021 was just 21 billion dollars, which was actually a 15 percent decline from the year before. 
+
+Speaker 2: This means the actual needs are 10 to 18 times greater than the funds that are actually being provided, leaving the most vulnerable communities dangerously exposed and underprepared.
+
+Speaker 3: I understand the need is great, but why is this framed as a justice issue? Isn't every country ultimately responsible for protecting its own citizens and adapting to its own challenges?
+
+Speaker 2: That question gets to the very core of the UN climate negotiations. The entire process is built upon a foundational principle known as common but differentiated responsibilities and respective capabilities. It's a bit of a mouthful, but the concept is straightforward. 
+
+Speaker 2: It acknowledges that while all nations share a common responsibility to protect the global climate, the developed countries, which have been industrializing for over a century, bear a much greater historical responsibility for causing the problem in the first place. They also possess far greater financial and technological capabilities to address it.
+
+Speaker 4: So it‚Äôs the idea that the polluter should pay. The ones who created the mess have a greater obligation to help clean it up, and to help protect those who are most harmed by it.
+
+Speaker 2: Exactly. Climate justice frameworks articulate this through the concept of a double inequality. The very people and nations who have contributed the least to the emissions that cause climate change are the ones who are suffering the earliest and most severe consequences. 
+
+Speaker 2: Therefore, a just global response requires that the developed nations lead the way in making the deepest emissions cuts, and that they provide substantial financial and technological support to help developing nations adapt to the impacts they did little to cause.
+
+Speaker 1: Carter, you were just explaining this core principle of climate justice, that the nations with the greatest historical responsibility for emissions also have the greatest capacity to help solve the problem.
+
+Speaker 2: Yes, and it builds on what Maya was saying. It‚Äôs about recognizing the profound unfairness, the, uh, double inequality that lies at the heart of the climate crisis. The people who are most harmed are the ones who did the least to cause the problem. Think about it, uh, a farmer in the Sahel whose land is turning to desert, or a family in a low-lying island nation whose home is threatened by sea level rise‚Ä¶ their contribution to historical emissions is practically zero.
+
+Speaker 4: So what you're saying is, that farmer, whose crops are failing from a drought they had no part in creating, is right now paying a much, much higher price than someone in a wealthy country who has, you know, benefited from a century of industrial development powered by fossil fuels.
+
+Speaker 2: That is the injustice in a nutshell. And so, the framework for a just response is built on that understanding. It means developed nations have a moral and ethical obligation to lead with deep, rapid emissions cuts. And, crucially, it means they have an obligation to provide significant financial and technological support to help developing nations build clean economies and adapt to the impacts they are already facing.
+
+Speaker 3: I understand the moral argument. I do. But from a purely practical standpoint, it seems incredibly complicated. I mean, how far back do you go to assign this historical responsibility? Are you trying to calculate the emissions of the United Kingdom from the 1880s? It feels like an impossibly complex way to assign blame.
+
+Speaker 2: That's a fair point, Frank, and you know, it‚Äôs less about calculating precise historical blame and more about acknowledging the reality of the present day. The framework is not about punishing past generations. It's about recognizing which nations today have the accumulated wealth, the technology, and the stable institutions‚Äîmany of which were built on that history of fossil-fueled development‚Äîto lead the global response. It‚Äôs about capability and responsibility in the here and now.
+
+Speaker 1: This whole conversation about justice, responsibility, and the immense shortfall in support really underscores the urgency of the crisis. And perhaps nothing in this entire report highlights that urgency more than the growing scientific understanding of a concept known as climate tipping points. Carter, for our listeners, what exactly is a tipping point?
+
+Speaker 2: It is probably the most sobering concept in all of climate science. The IPCC defines a tipping point as a critical threshold in the Earth's system. Once that threshold is crossed, a part of the system could trigger an abrupt, cascading, and potentially irreversible change.
+
+Speaker 1: Abrupt and irreversible. Those are two very powerful words. What does irreversible mean in this context?
+
+Speaker 2: It means that even if we managed to cool the planet back down later, the system might not flip back. The change could be locked in for centuries, or even millennia. We could pass a point of no return.
+
+Speaker 4: That is‚Ä¶ a terrifying thought. So what are these systems? What parts of the planet are we talking about?
+
+Speaker 2: Scientists have identified several large-scale components of the Earth system that may have these tipping points. The most commonly discussed are the great ice sheets. We‚Äôre talking about the irreversible collapse of the Greenland and the West Antarctic ice sheets.
+
+Speaker 1: And what would be the consequence of something like that?
+
+Speaker 2: Well, uh, together, those two ice sheets hold enough frozen water to raise the global mean sea level by over 10 meters. That's about 33 feet.
+
+Speaker 4: Ten meters‚Ä¶ I‚Ä¶ I can‚Äôt even comprehend that. That's not just flooding. That is wiping entire cities, entire island nations, completely off the map for good.
+
+Speaker 2: Yes, the consequences would be civilization-altering. And another major tipping element is in the oceans themselves. A major slowdown or even a shutdown of the Atlantic Meridional Overturning Circulation, often called the AMOC.
+
+Speaker 3: The AMOC. I've heard of that, but it sounds like something out of a disaster movie. What does this current actually do for us?
+
+Speaker 2: It's a massive system of ocean currents that acts like a conveyor belt, transporting warm water from the tropics up to the North Atlantic. It plays a huge role in regulating weather patterns, especially in the Northern Hemisphere. 
+
+Speaker 2: A collapse of this system would drastically alter weather across North America and Europe, causing, you know, extreme cooling in some places, changing rainfall patterns, and disrupting monsoons that billions of people depend on for their food.
+
+Speaker 1: So we have the ice and the oceans. What else?
+
+Speaker 2: Then we have the biosphere systems. There are two major ones scientists are deeply concerned about. The first is the potential dieback of the Amazon rainforest.
+
+Speaker 1: So the Amazon could go from being this vital carbon sink that helps us, to becoming a major carbon source that actually hurts us?
+
+Speaker 2: Precisely. Large parts of the forest could transition into a drier, savanna-like ecosystem. And in doing so, it would release the vast quantities of carbon stored in its trees and soil, which would create a powerful feedback loop that accelerates even more global warming.
+
+Speaker 4: And the other one? You hear people talk about a ticking carbon bomb in the arctic. Is that what you mean?
+
+Speaker 2: That's the one. The abrupt, widespread thawing of permafrost. This is the permanently frozen ground in the arctic regions, and it contains enormous amounts of organic carbon that has been locked away for thousands of years. As it thaws, microbes decompose that organic matter and release it into the atmosphere as carbon dioxide and, even more potently, methane. This is another one of those dangerous feedback loops.
+
+Speaker 1: So Carter, we have these massive, continent-scale systems that could fundamentally break. I think for a long time, many of us thought of these tipping points as very distant risks. You know, things that might happen if warming got really, really bad, say, at five or six degrees Celsius. What does the latest science in the report say about that?
+
+Speaker 2: This, Alice, is perhaps the single most concerning finding to emerge in the last few years of research. The scientific consensus has shifted. Those early estimates that suggested these were high-warming risks have been revised. The latest research, which is cited in the IPCC reports, indicates that the temperature thresholds for triggering some of these tipping points may be much, much lower than we previously thought.
+
+Speaker 3: How much lower are we talking about?
+
+Speaker 2: The latest studies indicate that several of these major tipping points, including the collapse of the Greenland and West Antarctic ice sheets, the shutdown of the AMOC, and widespread permafrost thaw, could potentially be triggered at warming levels between 1.5 and 2.0 degrees Celsius.
+
+Speaker 4: But wait a minute. Carter, you said at the very, very beginning of our conversation that the world already temporarily breached 1.5 degrees of warming in 2024. If the trigger point is 1.5 degrees, what does that mean for us right now?
+
+Speaker 2: It means‚Ä¶ well, it means that the risk is no longer a distant, abstract threat for future generations. It places the possibility of crossing these irreversible thresholds squarely within the realm of possibility this century. It moves the conversation from the future into the immediate present. 
+
+Speaker 2: And, you know, it adds a profound, almost existential urgency to the need for immediate, deep, and drastic emissions reductions. The window of opportunity to steer away from these points is closing, and it is closing very, very rapidly.
+
+Speaker 1: That is a deeply unsettling reality to confront, Carter. And Maya, I see you reacting to that. When you hear that the 1.5 degree line, which we‚Äôve talked about for so long as this future guardrail, is not only something we've touched but is also the potential trigger for these irreversible changes‚Ä¶ what does that feel like?
+
+Speaker 4: You know, it‚Ä¶ it almost takes your breath away. It feels like we've been driving towards a cliff in the fog, arguing about how fast we should be going. And Carter is saying the fog has just cleared, and we're right at the edge. We‚Äôre there. That's a very, very hard thing to fully process.
+
+Speaker 3: It is. And it brings up a really difficult, practical question for me. If we're already on the verge of crossing these irreversible thresholds, what is the point of all this? I mean, does a 43 percent emissions cut by 2030, which already seems impossible, even matter anymore if the fuse has already been lit on something like the Greenland ice sheet? Have we‚Ä¶ have we already lost the game?
+
+Speaker 2: Frank, that is the most important question anyone can ask right now. And the conclusion of the report, uh, argues that this is precisely why our actions now matter more than they ever have before. The first major conclusion is that the defining characteristic of the last decade is non-linear acceleration.
+
+Speaker 1: Okay, non-linear acceleration. Break that down for us.
+
+Speaker 2: Think of it like a car that's rolling down a hill. But the hill isn't a steady slope; it's a curve that gets steeper and steeper as you go. So for every foot you travel, your speed increases more than it did in the previous foot. You are accelerating exponentially, not in a straight line, not arithmetically. That‚Äôs what‚Äôs happening to our planetary systems. The risks are growing at an accelerating rate.
+
+Speaker 1: So every fraction of a degree of warming we can prevent now, every year we can act faster, has a much bigger impact in preventing that future acceleration than it would have twenty or thirty years ago.
+
+Speaker 2: Exactly. It‚Äôs what scientists call positive feedback loops becoming more potent. So, to answer Frank‚Äôs question, it‚Äôs the absolute opposite of the game being lost. It means the stakes of our actions in the next five to ten years are higher than they have ever been in human history. Every ton of carbon we keep out of the atmosphere now pays huge dividends in slowing down that terrifying acceleration toward those tipping points.
+
+Speaker 1: And the report also concludes that these are not isolated problems, correct? It talks about a cascade of interconnected crises.
+
+Speaker 2: Yes, that's the second key takeaway. We can no longer think of climate impacts as a series of separate events. A drought is not just a lack of water. It is a trigger. It triggers failures in the food system when crops fail. It triggers failures in the economic system when farmers lose their livelihoods. 
+
+Speaker 2: It triggers, you know, public health crises from malnutrition and water-borne diseases. It can even culminate in social instability and displacement. Climate change is a threat multiplier that makes all our existing vulnerabilities worse.
+
+Speaker 4: You can really see that in real life, can‚Äôt you? I mean, a wildfire isn't just a fire anymore. It becomes a public health crisis for millions of people breathing in the smoke. It's an economic crisis for the entire region. It becomes a water crisis months later when the first heavy rains wash toxic ash and debris into the reservoirs. You realize that one event pulls on all the other threads that hold our society together. Everything is connected.
+
+Speaker 2: That‚Äôs a perfect way to put it, Maya. And because everything is connected, the report concludes that our response has to be holistic. We can‚Äôt have siloed policies that address energy, or agriculture, or public health in isolation. They are all part of the same interconnected challenge.
+
+Speaker 1: This brings us to the third, and perhaps the toughest, conclusion from the report. Which is that our global response, as it stands today, is being dangerously outpaced by the physical reality of climate change.
+
+Speaker 2: That's the hard truth of the last decade. Despite all the meetings and the progress on renewables, the response remains critically insufficient. The report concludes that this failure is defined by three persistent and widening gaps. First is the ambition gap we already discussed, the gap between the weak climate pledges from countries and what science clearly shows is necessary.
+
+Speaker 1: And the second?
+
+Speaker 2: The second is the adaptation finance gap, which we just covered. The massive shortfall in funding that leaves the world‚Äôs most vulnerable populations essentially undefended against the coming storms and droughts. And the third is the justice gap, which undermines the trust and cooperation that are absolutely essential for any kind of effective global solution.
+
+Speaker 3: So if I'm hearing this correctly, the report‚Äôs ultimate conclusion is that our primary problem is no longer a technological one. We have the solar panels, we have the wind turbines, we have the efficiency solutions. The report is saying that the biggest barriers now are political, financial, and social. It's about a lack of political will, a failure to mobilize the necessary funds, and a failure to address the core injustices of the crisis.
+
+Speaker 2: That is the absolute crux of the conclusion. Technology is a vital tool, an essential tool, but it is not a silver bullet. The primary obstacles are now in our halls of government, in our financial institutions, and, uh, in our collective willingness to face this reality and act at the scale it requires.
+
+Speaker 1: So after this incredibly detailed and, frankly, alarming look back at the last decade, where does this leave us? We have a planet in a state of acceleration. We've temporarily breached the 1.5 degree threshold. And the risk of irreversible tipping points is no longer a future problem, but a present-day danger. Maya, I want to start with you. What‚Äôs your final takeaway?
+
+Speaker 4: It leaves me feeling that the time for simply being worried, or for abstract hope, is over. The only appropriate response to this level of evidence is determined action. This report is a story written in data, and it's telling us we have to transform this stark awareness into real, tangible work in our communities and demand it from our leaders. There‚Äôs no time for anything else.
+
+Speaker 1: Frank?
+
+Speaker 3: It leaves me thinking that we need to have a much more honest and pragmatic conversation about the real-world costs and trade-offs. We‚Äôve talked about technology and policy, but this report makes it clear that the real fight is over politics and economics. And until we tackle that head-on, with honesty, we'll keep falling short.
+
+Speaker 1: And Carter, a final thought from you.
+
+Speaker 2: The science has been clear for a long time, but the evidence from this past decade is definitive. You know, this period from 2015 to 2025 will be remembered as the decade the consequences of our inaction became undeniable. That temporary breach of 1.5 degrees served as a final, unambiguous warning. The scientific challenge now is to monitor these accelerating changes. But the human challenge is to finally close those gaps between promises and performance, before those tipping points are crossed for good.
+
+Speaker 1: Carter, that is a powerful and frankly stark place to end, on the precipice of these tipping points with the clock running out. But... you know, before we wrap up completely, I want to hold on that last thought. The human challenge. I feel we can't end just with the warning. I want to pivot from the problems we've detailed so thoroughly to the specific pathways forward that are emerging. Beyond the high-level policy failures, where are the new fronts in this challenge?
+
+Speaker 2: That's a crucial pivot to make, Alice. Because, uh, despair is paralyzing. And despite the failures, there are new strategies and, you know, new arenas of action that are gaining momentum.
+
+Speaker 1: Let's talk about one of those. We've mentioned the justice gap and the economic challenges. What about the people, the workers and communities, whose entire livelihoods are tied to the fossil fuel industries we need to transition away from?
+
+Speaker 2: You're talking about the concept of a Just Transition. And you know, this has become a central part of the conversation because it's both morally right and politically essential. A Just Transition means ensuring that the shift to a green economy is fair and inclusive. It means we don't leave coal miners, oil rig workers, and entire communities that depend on these industries behind.
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Alice_woman.wav b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Alice_woman.wav
new file mode 100644
index 0000000..459c39d
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Alice_woman.wav differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Carter_man.wav b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Carter_man.wav
new file mode 100644
index 0000000..e9d6500
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Carter_man.wav differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Frank_man.wav b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Frank_man.wav
new file mode 100644
index 0000000..d89eb4f
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Frank_man.wav differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Mary_woman_bgm.wav b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Mary_woman_bgm.wav
new file mode 100644
index 0000000..6b3fc92
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Mary_woman_bgm.wav differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Maya_woman.wav b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Maya_woman.wav
new file mode 100644
index 0000000..85abeba
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/en-Maya_woman.wav differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/in-Samuel_man.wav b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/in-Samuel_man.wav
new file mode 100644
index 0000000..07606b6
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/in-Samuel_man.wav differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Carter_man.pt b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Carter_man.pt
new file mode 100644
index 0000000..1d795ef
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Carter_man.pt differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Davis_man.pt b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Davis_man.pt
new file mode 100644
index 0000000..a259516
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Davis_man.pt differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Emma_woman.pt b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Emma_woman.pt
new file mode 100644
index 0000000..9876ca3
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Emma_woman.pt differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Frank_man.pt b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Frank_man.pt
new file mode 100644
index 0000000..2c37e82
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Frank_man.pt differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Grace_woman.pt b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Grace_woman.pt
new file mode 100644
index 0000000..366bda8
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Grace_woman.pt differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Mike_man.pt b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Mike_man.pt
new file mode 100644
index 0000000..f02ef03
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/en-Mike_man.pt differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/in-Samuel_man.pt b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/in-Samuel_man.pt
new file mode 100644
index 0000000..9338cb6
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/streaming_model/in-Samuel_man.pt differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/zh-Anchen_man_bgm.wav b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/zh-Anchen_man_bgm.wav
new file mode 100644
index 0000000..b6bba02
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/zh-Anchen_man_bgm.wav differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/zh-Bowen_man.wav b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/zh-Bowen_man.wav
new file mode 100644
index 0000000..dd2a0be
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/zh-Bowen_man.wav differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/zh-Xinran_woman.wav b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/zh-Xinran_woman.wav
new file mode 100644
index 0000000..e619d1d
Binary files /dev/null and b/packages/official-plugins/vibevoice/src/vibevoice/demo/voices/zh-Xinran_woman.wav differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__pycache__/__init__.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 99fb241..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__pycache__/data_vibevoice.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__pycache__/data_vibevoice.cpython-310.pyc
deleted file mode 100644
index ce954dd..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__pycache__/data_vibevoice.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__pycache__/train_vibevoice.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__pycache__/train_vibevoice.cpython-310.pyc
deleted file mode 100644
index 0bad194..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__pycache__/train_vibevoice.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/__init__.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 5beedff..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/__init__.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/__init__.cpython-311.pyc
deleted file mode 100644
index dbf8e31..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/__init__.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice.cpython-310.pyc
deleted file mode 100644
index 8859d81..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice.cpython-311.pyc
deleted file mode 100644
index a47a8fa..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice_streaming.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice_streaming.cpython-310.pyc
deleted file mode 100644
index 97c1ce1..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice_streaming.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice_streaming.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice_streaming.cpython-311.pyc
deleted file mode 100644
index 27c02c7..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/configuration_vibevoice_streaming.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/lora_loading.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/lora_loading.cpython-310.pyc
deleted file mode 100644
index c9ff40c..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/lora_loading.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/lora_loading.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/lora_loading.cpython-311.pyc
deleted file mode 100644
index 46950dc..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/lora_loading.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice.cpython-310.pyc
deleted file mode 100644
index 8450006..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice.cpython-311.pyc
deleted file mode 100644
index 9a8dfad..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_inference.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_inference.cpython-310.pyc
deleted file mode 100644
index c893512..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_inference.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_inference.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_inference.cpython-311.pyc
deleted file mode 100644
index 399211f..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_inference.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming.cpython-310.pyc
deleted file mode 100644
index 3299e6f..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming.cpython-311.pyc
deleted file mode 100644
index cf882aa..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming_inference.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming_inference.cpython-310.pyc
deleted file mode 100644
index 881bf49..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming_inference.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming_inference.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming_inference.cpython-311.pyc
deleted file mode 100644
index 59a6610..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modeling_vibevoice_streaming_inference.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_diffusion_head.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_diffusion_head.cpython-310.pyc
deleted file mode 100644
index dd36888..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_diffusion_head.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_diffusion_head.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_diffusion_head.cpython-311.pyc
deleted file mode 100644
index 56e03dd..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_diffusion_head.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_text_tokenizer.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_text_tokenizer.cpython-310.pyc
deleted file mode 100644
index 0d72fb1..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_text_tokenizer.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_text_tokenizer.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_text_tokenizer.cpython-311.pyc
deleted file mode 100644
index 01f4e53..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_text_tokenizer.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_tokenizer.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_tokenizer.cpython-310.pyc
deleted file mode 100644
index 3bfea41..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_tokenizer.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_tokenizer.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_tokenizer.cpython-311.pyc
deleted file mode 100644
index b9ded42..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/modular_vibevoice_tokenizer.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/streamer.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/streamer.cpython-310.pyc
deleted file mode 100644
index 4ac0d5c..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/streamer.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/streamer.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/streamer.cpython-311.pyc
deleted file mode 100644
index 3801656..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__pycache__/streamer.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/__init__.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 8263532..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/__init__.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/__init__.cpython-311.pyc
deleted file mode 100644
index d8c4cdb..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/__init__.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_processor.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_processor.cpython-310.pyc
deleted file mode 100644
index 893d2d3..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_processor.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_processor.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_processor.cpython-311.pyc
deleted file mode 100644
index 6c4f778..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_processor.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_streaming_processor.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_streaming_processor.cpython-310.pyc
deleted file mode 100644
index 2957727..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_streaming_processor.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_streaming_processor.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_streaming_processor.cpython-311.pyc
deleted file mode 100644
index 2e947d0..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_streaming_processor.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_tokenizer_processor.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_tokenizer_processor.cpython-310.pyc
deleted file mode 100644
index 4d192d7..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_tokenizer_processor.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_tokenizer_processor.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_tokenizer_processor.cpython-311.pyc
deleted file mode 100644
index 41af294..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__pycache__/vibevoice_tokenizer_processor.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/vibevoice/pyproject.toml b/packages/official-plugins/vibevoice/src/vibevoice/pyproject.toml
new file mode 100644
index 0000000..a7d2998
--- /dev/null
+++ b/packages/official-plugins/vibevoice/src/vibevoice/pyproject.toml
@@ -0,0 +1,51 @@
+[build-system]
+requires = ["setuptools>=61.0"]
+build-backend = "setuptools.build_meta"
+
+[project]
+name = "vibevoice"
+version = "0.1.0"
+authors = [
+  { name="vibevoice team", email="vibepod@microsoft.com" },
+]
+description = "A model for speech generation with an AR + diffusion architecture."
+readme = "README.md"
+requires-python = ">=3.9"
+classifiers = [
+    "Programming Language :: Python :: 3",
+    # "License :: OSI Approved :: MIT License",
+    "Operating System :: OS Independent",
+]
+dependencies = [
+    "torch",
+    "accelerate==1.6.0",
+    "transformers==4.51.3", # we develop this project on transformers==4.51.3, later version may not be compatible
+    "datasets==3.5.0", # avoid issues with torchcodec
+    "peft", # for finetuning
+    "llvmlite>=0.40.0",
+    "numba>=0.57.0",
+    "diffusers",
+    "tqdm",
+    "numpy",
+    "scipy",
+    "librosa",
+    "ml-collections",
+    "absl-py",
+    "gradio==5.50.0",
+    "av",
+    "aiortc"
+]
+
+[project.optional-dependencies]
+# Optional dependencies for streaming model web service
+streaming-web = [
+    "fastapi",
+    "uvicorn[standard]",
+]
+
+[project.urls]
+"Homepage" = "https://github.com/microsoft/VibeVoice"
+"Bug Tracker" = "https://github.com/microsoft/VibeVoice/issues"
+
+[tool.setuptools.packages.find]
+where = ["."]
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/__init__.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 48bacb5..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/__init__.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/__init__.cpython-311.pyc
deleted file mode 100644
index 2be90c6..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/__init__.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/dpm_solver.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/dpm_solver.cpython-310.pyc
deleted file mode 100644
index e579296..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/dpm_solver.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/dpm_solver.cpython-311.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/dpm_solver.cpython-311.pyc
deleted file mode 100644
index 9a73416..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/dpm_solver.cpython-311.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/timestep_sampler.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/timestep_sampler.cpython-310.pyc
deleted file mode 100644
index 6ab5959..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__pycache__/timestep_sampler.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__pycache__/__init__.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 58c0e57..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__pycache__/convert_nnscaler_checkpoint_to_transformers.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__pycache__/convert_nnscaler_checkpoint_to_transformers.cpython-310.pyc
deleted file mode 100644
index 82be138..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__pycache__/convert_nnscaler_checkpoint_to_transformers.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__pycache__/merge_vibevoice_models.cpython-310.pyc b/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__pycache__/merge_vibevoice_models.cpython-310.pyc
deleted file mode 100644
index 20c7c89..0000000
Binary files a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__pycache__/merge_vibevoice_models.cpython-310.pyc and /dev/null differ
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/__init__.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/__init__.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/__init__.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/__init__.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/configs/qwen2.5_1.5b_64k.json b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/configs/qwen2.5_1.5b_64k.json
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/configs/qwen2.5_1.5b_64k.json
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/configs/qwen2.5_1.5b_64k.json
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/configs/qwen2.5_7b_32k.json b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/configs/qwen2.5_7b_32k.json
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/configs/qwen2.5_7b_32k.json
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/configs/qwen2.5_7b_32k.json
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__init__.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/finetune/__init__.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/__init__.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/finetune/__init__.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/data_vibevoice.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/finetune/data_vibevoice.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/data_vibevoice.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/finetune/data_vibevoice.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/train_vibevoice.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/finetune/train_vibevoice.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/finetune/train_vibevoice.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/finetune/train_vibevoice.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__init__.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/__init__.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/__init__.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/__init__.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/configuration_vibevoice.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/configuration_vibevoice.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/configuration_vibevoice.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/configuration_vibevoice.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/configuration_vibevoice_streaming.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/configuration_vibevoice_streaming.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/configuration_vibevoice_streaming.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/configuration_vibevoice_streaming.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/lora_loading.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/lora_loading.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/lora_loading.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/lora_loading.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice_inference.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice_inference.py
similarity index 99%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice_inference.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice_inference.py
index 9ef7694..a47b273 100644
--- a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice_inference.py
+++ b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice_inference.py
@@ -696,20 +696,17 @@ class VibeVoiceForConditionalGenerationInference(VibeVoicePreTrainedModel, Gener
     
     @torch.no_grad()
     def sample_speech_tokens(self, condition, neg_condition, cfg_scale=3.0):
-        import copy
-
-        noise_scheduler = copy.deepcopy(self.model.noise_scheduler)
-        noise_scheduler.set_timesteps(self.ddpm_inference_steps)
+        self.model.noise_scheduler.set_timesteps(self.ddpm_inference_steps)
         condition = torch.cat([condition, neg_condition], dim=0).to(self.model.prediction_head.device)
         speech = torch.randn(condition.shape[0], self.config.acoustic_vae_dim).to(condition)
-        for t in noise_scheduler.timesteps:
+        for t in self.model.noise_scheduler.timesteps:
             half = speech[: len(speech) // 2]
             combined = torch.cat([half, half], dim=0)
             eps = self.model.prediction_head(combined, t.repeat(combined.shape[0]).to(combined), condition=condition)
             cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)
             half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)
             eps = torch.cat([half_eps, half_eps], dim=0)
-            speech = noise_scheduler.step(eps, t, speech).prev_sample
+            speech = self.model.noise_scheduler.step(eps, t, speech).prev_sample
         return speech[: len(speech) // 2]
     
 
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice_streaming.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice_streaming.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice_streaming.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice_streaming.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice_streaming_inference.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice_streaming_inference.py
similarity index 99%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice_streaming_inference.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice_streaming_inference.py
index 597d5b6..14520f3 100644
--- a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modeling_vibevoice_streaming_inference.py
+++ b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modeling_vibevoice_streaming_inference.py
@@ -737,20 +737,17 @@ class VibeVoiceStreamingForConditionalGenerationInference(VibeVoiceStreamingPreT
         Returns:
             Generated speech latents
         """
-        import copy
-
-        noise_scheduler = copy.deepcopy(self.model.noise_scheduler)
-        noise_scheduler.set_timesteps(self.ddpm_inference_steps)
+        self.model.noise_scheduler.set_timesteps(self.ddpm_inference_steps)
         condition = torch.cat([condition, neg_condition], dim=0).to(self.model.prediction_head.device)
         speech = torch.randn(condition.shape[0], self.config.acoustic_vae_dim).to(condition)
-        for t in noise_scheduler.timesteps:
+        for t in self.model.noise_scheduler.timesteps:
             half = speech[: len(speech) // 2]
             combined = torch.cat([half, half], dim=0)
             eps = self.model.prediction_head(combined, t.repeat(combined.shape[0]).to(combined), condition=condition)
             cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)
             half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)
             eps = torch.cat([half_eps, half_eps], dim=0)
-            speech = noise_scheduler.step(eps, t, speech).prev_sample
+            speech = self.model.noise_scheduler.step(eps, t, speech).prev_sample
         return speech[: len(speech) // 2]
 
 
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modular_vibevoice_diffusion_head.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modular_vibevoice_diffusion_head.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modular_vibevoice_diffusion_head.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modular_vibevoice_diffusion_head.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modular_vibevoice_text_tokenizer.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modular_vibevoice_text_tokenizer.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modular_vibevoice_text_tokenizer.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modular_vibevoice_text_tokenizer.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modular_vibevoice_tokenizer.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modular_vibevoice_tokenizer.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/modular_vibevoice_tokenizer.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/modular_vibevoice_tokenizer.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/streamer.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/streamer.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/modular/streamer.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/modular/streamer.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__init__.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/__init__.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/__init__.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/__init__.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/vibevoice_processor.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/vibevoice_processor.py
similarity index 97%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/vibevoice_processor.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/vibevoice_processor.py
index 9fc912a..f67d0ce 100644
--- a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/vibevoice_processor.py
+++ b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/vibevoice_processor.py
@@ -93,25 +93,14 @@ class VibeVoiceProcessor:
         speech_tok_compress_ratio = config.get("speech_tok_compress_ratio", 3200)
         db_normalize = config.get("db_normalize", True)
         
-        # Load tokenizer - try from model path first, then fallback to Qwen
-        default_tokenizer_name = kwargs.pop("language_model_pretrained_name", "Qwen/Qwen2.5-1.5B")
-        language_model_pretrained_name = config.get("language_model_pretrained_name", None) or default_tokenizer_name
+        # Load tokenizer - try from model path first, then fallback to Qwen        
+        language_model_pretrained_name = config.get("language_model_pretrained_name", None) or kwargs.pop("language_model_pretrained_name", "Qwen/Qwen2.5-1.5B")
         logger.info(f"Loading tokenizer from {language_model_pretrained_name}")
         if 'qwen' in language_model_pretrained_name.lower():
-            tokenizer_kwargs = dict(kwargs)
-            if os.path.isdir(str(language_model_pretrained_name)):
-                tokenizer_kwargs.setdefault("local_files_only", True)
-            try:
-                tokenizer = VibeVoiceTextTokenizerFast.from_pretrained(
-                    language_model_pretrained_name,
-                    **tokenizer_kwargs
-                )
-            except Exception:
-                tokenizer_kwargs = dict(kwargs)
-                tokenizer = VibeVoiceTextTokenizerFast.from_pretrained(
-                    default_tokenizer_name,
-                    **tokenizer_kwargs
-                )
+            tokenizer = VibeVoiceTextTokenizerFast.from_pretrained(
+                language_model_pretrained_name,
+                **kwargs
+            )
         else:
             raise ValueError(f"Unsupported tokenizer type for {language_model_pretrained_name}. Supported types: Qwen, Llama, Gemma.")
         
@@ -708,4 +697,4 @@ class VibeVoiceProcessor:
     
 __all__ = [
     "VibeVoiceProcessor",
-]
+]
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/vibevoice_streaming_processor.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/vibevoice_streaming_processor.py
similarity index 95%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/vibevoice_streaming_processor.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/vibevoice_streaming_processor.py
index 24fbbf2..044d6df 100644
--- a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/vibevoice_streaming_processor.py
+++ b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/vibevoice_streaming_processor.py
@@ -100,24 +100,13 @@ class VibeVoiceStreamingProcessor:
         db_normalize = config.get("db_normalize", True)
 
         # Load tokenizer - try from model path first, then fallback to Qwen
-        default_tokenizer_name = kwargs.pop("language_model_pretrained_name", "Qwen/Qwen2.5-0.5B")
-        language_model_pretrained_name = config.get("language_model_pretrained_name", None) or default_tokenizer_name
+        language_model_pretrained_name = config.get("language_model_pretrained_name", None) or kwargs.pop("language_model_pretrained_name", "Qwen/Qwen2.5-0.5B")
         logger.info(f"Loading tokenizer from {language_model_pretrained_name}")
         if 'qwen' in language_model_pretrained_name.lower():
-            tokenizer_kwargs = dict(kwargs)
-            if os.path.isdir(str(language_model_pretrained_name)):
-                tokenizer_kwargs.setdefault("local_files_only", True)
-            try:
-                tokenizer = VibeVoiceTextTokenizerFast.from_pretrained(
-                    language_model_pretrained_name,
-                    **tokenizer_kwargs
-                )
-            except Exception:
-                tokenizer_kwargs = dict(kwargs)
-                tokenizer = VibeVoiceTextTokenizerFast.from_pretrained(
-                    default_tokenizer_name,
-                    **tokenizer_kwargs
-                )
+            tokenizer = VibeVoiceTextTokenizerFast.from_pretrained(
+                language_model_pretrained_name,
+                **kwargs
+            )
         else:
             raise ValueError(f"Unsupported tokenizer type for {language_model_pretrained_name}. Supported types: Qwen.")
 
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/vibevoice_tokenizer_processor.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/vibevoice_tokenizer_processor.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/processor/vibevoice_tokenizer_processor.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/processor/vibevoice_tokenizer_processor.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__init__.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/schedule/__init__.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/__init__.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/schedule/__init__.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/dpm_solver.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/schedule/dpm_solver.py
similarity index 99%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/dpm_solver.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/schedule/dpm_solver.py
index 2a9ca96..806241f 100644
--- a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/dpm_solver.py
+++ b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/schedule/dpm_solver.py
@@ -909,8 +909,12 @@ class DPMSolverMultistepScheduler(SchedulerMixin, ConfigMixin):
 
         if len(index_candidates) == 0:
             step_index = len(self.timesteps) - 1
+        # The sigma index that is taken for the **very** first `step`
+        # is always the second index (or the last index if there is only 1)
+        # This way we can ensure we don't accidentally skip a sigma in
+        # case we start in the middle of the denoising schedule (e.g. for image-to-image)
         elif len(index_candidates) > 1:
-            step_index = index_candidates[0].item()
+            step_index = index_candidates[1].item()
         else:
             step_index = index_candidates[0].item()
 
@@ -970,11 +974,6 @@ class DPMSolverMultistepScheduler(SchedulerMixin, ConfigMixin):
         if self.step_index is None:
             self._init_step_index(timestep)
 
-        if self.step_index >= len(self.sigmas) - 1:
-            if not return_dict:
-                return (sample,)
-            return SchedulerOutput(prev_sample=sample)
-
         # Improve numerical stability for small number of steps
         lower_order_final = (self.step_index == len(self.timesteps) - 1) and (
             self.config.euler_at_final
@@ -1063,4 +1062,4 @@ class DPMSolverMultistepScheduler(SchedulerMixin, ConfigMixin):
         return velocity
 
     def __len__(self):
-        return self.config.num_train_timesteps
+        return self.config.num_train_timesteps
\ No newline at end of file
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/timestep_sampler.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/schedule/timestep_sampler.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/schedule/timestep_sampler.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/schedule/timestep_sampler.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__init__.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/scripts/__init__.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/__init__.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/scripts/__init__.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/convert_nnscaler_checkpoint_to_transformers.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/scripts/convert_nnscaler_checkpoint_to_transformers.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/convert_nnscaler_checkpoint_to_transformers.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/scripts/convert_nnscaler_checkpoint_to_transformers.py
diff --git a/packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/merge_vibevoice_models.py b/packages/official-plugins/vibevoice/src/vibevoice/vibevoice/scripts/merge_vibevoice_models.py
similarity index 100%
rename from packages/official-plugins/vibevoice/src/local_overrides/vibevoice/scripts/merge_vibevoice_models.py
rename to packages/official-plugins/vibevoice/src/vibevoice/vibevoice/scripts/merge_vibevoice_models.py
